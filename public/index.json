[{"content":"The VGG (Visual Geometry Group) model is a type of convolutional neural network (CNN) outlined in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition. It\u0026rsquo;s known for its use of small convolution filters and deep layers, which helped it achieve top-notch performance in tasks like image classification. By stacking multiple layers with small kernel sizes, VGG can capture a wide range of features from input images. Plus, adding more rectification layers makes its decision-making process sharper and more accurate. The paper also introduced 1x1 convolutional layers to enhance nonlinearity without affecting the receptive view. For training, VGG follows the traditional supervised learning approach where input images and ground truth labels are provided.\nVGG\u0026rsquo;s architecture has significantly shaped the field of neural networks, serving as a foundation and benchmark for many subsequent models in computer vision.\nIn this blog post, we\u0026rsquo;ll guide you through implementing and training the VGG architecture using PyTorch, step by step. You can find the complete code for defining and training the VGG model on my GitHub repository (URL: https://github.com/JianZhongDev/VGGPyTorch).\nVGG architecture and implementation As you can see in the cover image of this post, the VGG model is made up of multiple layers of convolution followed by max-pooling, and it ends with a few fully connected layers. The output from these layers is then fed into a softmax layer to give a normalized confidence score for each image category.\nThe key features of the VGG network are these stacked convolutional layers and fully connected layers. We will start with these stacked layers in our implementation.\nStacked convolutional layers To start, we\u0026rsquo;ll create the stacked convolutional layer as PyTorch nn.Module, like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # stacked 2D convolutional layer class VGGStacked2DConv(nn.Module): def __init__( self, layer_descriptors = [], ): assert(isinstance(layer_descriptors, list)) super().__init__() self.network = nn.Identity() # create list of stacked layers stacked_layers = [] # iterater through each descriptor for the layers and create corresponding layers prev_out_channels = 1 for i_descrip in range(len(layer_descriptors)): cur_descriptor = layer_descriptors[i_descrip] # the descriptor needs to be dict if not isinstance(cur_descriptor, dict): continue # get input or default values nof_layers = cur_descriptor.get(\u0026#34;nof_layers\u0026#34;, 1) in_channels = cur_descriptor.get(\u0026#34;in_channels\u0026#34;, prev_out_channels) out_channels = cur_descriptor.get(\u0026#34;out_channels\u0026#34;, 1) kernel_size = cur_descriptor.get(\u0026#34;kernel_size\u0026#34;, 3) stride = cur_descriptor.get(\u0026#34;stride\u0026#34;, 1) padding = cur_descriptor.get(\u0026#34;padding\u0026#34;, 1) bias = cur_descriptor.get(\u0026#34;bias\u0026#34;, True) padding_mode = cur_descriptor.get(\u0026#34;padding_mode\u0026#34;, \u0026#34;zeros\u0026#34;) activation = cur_descriptor.get(\u0026#34;activation\u0026#34;, nn.ReLU) # create layers cur_in_channels = in_channels for _ in range(nof_layers): stacked_layers.append( nn.Conv2d( in_channels = cur_in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding, bias = bias, padding_mode = padding_mode, ) ) stacked_layers.append( activation() ) cur_in_channels = out_channels prev_out_channels = out_channels # convert list of layers to sequential layers if len(stacked_layers) \u0026gt; 0: self.network = nn.Sequential(*stacked_layers) def forward(self, x): y = self.network(x) return y The stacked convolutional layer takes in a list of descriptor dictionaries, each detailing the setup for a repeated convolutional layer followed by an activation. It reads these configurations and builds the stacked convolutional layers accordingly. If certain configuration parameters are not specified, the code fills in default values.\nStacked fully-connected and dropout layers VGG uses dropout regularizations in their fully connected layers. Adding the dropout regularization within PyTorch is straightforward: we just need to insert dropout layers after each hidden layer inside the stacked fully connected layer. (NOTE: Section 4.2 of the AlexNet paper provides valuable insights into dropout layers. It\u0026rsquo;s definitely worth a read.)\nWe can define the stacked fully connected layer in a similar manner as the stacked convolutional layers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # stacked linear layers class VGGStackedLinear(nn.Module): def __init__( self, layer_descriptors = [], ): assert(isinstance(layer_descriptors, list)) super().__init__() self.network = nn.Identity() # create list of stacked layers stacked_layers = [] # iterater through each descriptor for the layers and create corresponding layers prev_out_features = 1 for i_descrip in range(len(layer_descriptors)): cur_descriptor = layer_descriptors[i_descrip] # the descriptor needs to be dict if not isinstance(cur_descriptor, dict): continue nof_layers = cur_descriptor.get(\u0026#34;nof_layers\u0026#34;, 1) in_features = cur_descriptor.get(\u0026#34;in_features\u0026#34;, prev_out_features) out_features = cur_descriptor.get(\u0026#34;out_features\u0026#34;, 1) bias = cur_descriptor.get(\u0026#34;bias\u0026#34;, True) activation = cur_descriptor.get(\u0026#34;activation\u0026#34;, nn.ReLU) dropout_p = cur_descriptor.get(\u0026#34;dropout_p\u0026#34;, None) # create layers cur_in_features = in_features for _ in range(nof_layers): stacked_layers.append( nn.Linear( in_features = cur_in_features, out_features = out_features, bias = bias, ) ) if activation is not None: stacked_layers.append( activation() ) if dropout_p is not None: stacked_layers.append( nn.Dropout(p = dropout_p) ) cur_in_features = out_features prev_out_features = out_features # convert list of layers to sequential layers if len(stacked_layers) \u0026gt; 0: self.network = nn.Sequential(*stacked_layers) def forward(self, x): y = self.network(x) return y VGG model Now that we\u0026rsquo;ve defined the stacked convolutional and fully-connected layers, we can construct the VGG model as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # VGG model definition class VGG(nn.Module): def __init__( self, stacked_conv_descriptors, stacked_linear_descriptor, ): assert(isinstance(stacked_conv_descriptors, list)) assert(isinstance(stacked_linear_descriptor, list)) super().__init__() self.network = nn.Identity() stacked_layers = [] # add stacked convolutional layers and max pooling layers for i_stackconv_descrip in range(len(stacked_conv_descriptors)): cur_stacked_conv_descriptor = stacked_conv_descriptors[i_stackconv_descrip] if not isinstance(cur_stacked_conv_descriptor, list): continue stacked_layers.append( StackedLayers.VGGStacked2DConv( cur_stacked_conv_descriptor ) ) # add max pooling layer after stacked convolutional layer stacked_layers.append( nn.MaxPool2d( kernel_size = 2, stride = 2, ) ) # flatten convolutional layers stacked_layers.append( nn.Flatten() ) # add stacked linear layers stacked_layers.append( StackedLayers.VGGStackedLinear( stacked_linear_descriptor ) ) # add softmax layer at the very end stacked_layers.append( nn.Softmax(dim = -1) ) # convert list of layers to Sequantial network if len(stacked_layers) \u0026gt; 0: self.network = nn.Sequential(*stacked_layers) def forward(self, x): y = self.network(x) return y The VGG model takes in a stacked convolutional layer descriptor list, and a fully connected layer descriptor. First, it goes through the convolutional layer descriptors, creating stacked convolutional layers for each descriptor and adding a max pooling layer after each set of stacked convolutional layers. Then, it flattens the output from all the convolutional layers and constructs stacked fully connected layers based on the linear layer descriptor. Finally, a Softmax layer is appended at the end of the network.\nModel generation Using the model definition provided above, we can create a VGG model by specifying a few layer descriptors. For instance, we can replicate the VGG16 model described in the VGG paper as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## Demo creating 16-layer VGG model input_image_width = 224 input_image_height = 224 model_stacked_conv_list = [ [ {\u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_channels\u0026#34;: 3, \u0026#34;out_channels\u0026#34;: 64,}, ], [ {\u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_ckjhannels\u0026#34;: 64, \u0026#34;out_channels\u0026#34;: 128,}, ], [ {\u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_channels\u0026#34;: 128, \u0026#34;out_channels\u0026#34;: 256, }, {\u0026#34;nof_layers\u0026#34;: 1, \u0026#34;out_channels\u0026#34;: 256, \u0026#34;kernel_size\u0026#34;: 1, \u0026#34;padding\u0026#34;: 0}, ], [ {\u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_ckjhannels\u0026#34;: 256, \u0026#34;out_channels\u0026#34;: 512,}, {\u0026#34;nof_layers\u0026#34;: 1, \u0026#34;out_channels\u0026#34;: 512, \u0026#34;kernel_size\u0026#34;: 1, \u0026#34;padding\u0026#34;: 0}, ], [ {\u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_ckjhannels\u0026#34;: 512, \u0026#34;out_channels\u0026#34;: 512,}, {\u0026#34;nof_layers\u0026#34;: 1, \u0026#34;out_channels\u0026#34;: 512, \u0026#34;kernel_size\u0026#34;: 1, \u0026#34;padding\u0026#34;: 0}, ], ] conv_image_reduce_ratio = 2**len(model_stacked_conv_list) conv_final_image_width = input_image_width//conv_image_reduce_ratio conv_final_image_height = input_image_height//conv_image_reduce_ratio model_stacked_linear = [ { \u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_features\u0026#34;: conv_final_image_width * conv_final_image_height * 512, \u0026#34;out_features\u0026#34;: 4096, \u0026#34;dropout_p\u0026#34;: 0.5 }, { \u0026#34;nof_layers\u0026#34;: 1, \u0026#34;out_features\u0026#34;: 1000, \u0026#34;activation\u0026#34;: None } ] model = VGG.VGG( stacked_conv_descriptors = model_stacked_conv_list, stacked_linear_descriptor = model_stacked_linear, enable_debug = False, ) print(model) Here\u0026rsquo;s what the printout of the VGG16 model looks like:\nclick to expand 16-layer VGG model printout\rVGG( (network): Sequential( (0): VGGStacked2DConv( (network): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() ) ) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() ) ) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (4): VGGStacked2DConv( (network): Sequential( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) ) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) ) (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (8): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) ) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Flatten(start_dim=1, end_dim=-1) (11): VGGStackedLinear( (network): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU() (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU() (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) (12): Softmax(dim=-1) ) ) Data processing In the VGG paper, the only data processing done on the input data is subtracting the RGB value calculated from the training set. To apply this processing, we start by going through the entire training dataset and computing the mean value for each color channel.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## calculated the averaged channel values across the entire data set # train_dataloader is the dataloader iterating through the entire training data set input_dataloader = train_dataloader nof_batchs = len(input_dataloader) avg_ch_vals = [None for _ in range(nof_batchs)] for i_batch, data in enumerate(input_dataloader): inputs, labels = data cur_avg_ch = torch.mean(inputs, dim = (-1,-2), keepdim = True) avg_ch_vals[i_batch] = cur_avg_ch avg_ch_vals = torch.cat(avg_ch_vals, dim = 0) avg_ch_val = torch.mean(avg_ch_vals, dim = 0, keepdim = False) print(\u0026#34;result size = \u0026#34;) print(avg_ch_val.size()) print(\u0026#34;result val = \u0026#34;) print(repr(avg_ch_val)) Using the mean channel value, we can perform the mentioned data processing by defining a background subtraction function and using the Lambda() transform provided by torchvision like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import functools from torchvision.transforms import v2 # subtract constant value from the image def subtract_const(src_image, const_val): return src_image - const_val ## subtract global mean channel background train_data_ch_avg = torch.tensor([[[0.4914]],[[0.4822]],[[0.4465]]]) # NOTE: train_data_ch_avg is obtained from the channel mean value calculation code print(train_data_ch_avg.size()) subtract_ch_avg = functools.partial(subtract_const, const_val = train_data_ch_avg) subtract_channel_mean_transform = v2.Lambda(subtract_ch_avg) Data Augmentation The VGG paper also employed various data augmentation techniques to prevent overfitting. Here\u0026rsquo;s how we implement them:\nRandom horizontal flip torchvision already includes a built-in transformation for randomly flipping images horizontally. Therefore, we can simply utilize this built-in transformation for horizontal flips.\n1 2 3 from torchvision.transforms import v2 rand_hflip_transform = v2.RandomHorizontalFlip(0.5) In the VGG paper, they utilized both the original image and its horizontally flipped counterpart to predict classification results. They then averaged these results to obtain the final classification. Consequently, we can implement the validation process as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # validate model in one epoch and return the top k-th result def validate_one_epoch_topk_aug( model, validate_loader, loss_func, transforms, device, top_k = 1 ): tot_loss = 0.0 avg_loss = 0.0 tot_nof_batch = len(validate_loader) correct_samples = 0 tot_samples = len(validate_loader.dataset) nof_transforms = len(transforms) model.eval() with torch.no_grad(): for i_batch, data in enumerate(validate_loader): inputs, labels = data inputs = inputs.to(device) labels = labels.to(device) group_outputs = [None for _ in range(nof_transforms)] group_loss = [None for _ in range(nof_transforms)] for i_trans in range(nof_transforms): cur_transform = transforms[i_trans] cur_input = inputs if cur_transform is not None: cur_input = cur_transform(inputs) cur_output = model(cur_input) cur_loss = loss_func(cur_output, labels) group_outputs[i_trans] = cur_output group_loss[i_trans] = cur_loss outputs = torch.mean(torch.stack(group_outputs, dim = 0), dim = 0) loss = torch.mean(torch.stack(group_loss, dim = 0), dim = 0) tot_loss += loss.item() # NOTE: we will define batch_in_top_k() later # NOTE: batch_in_top_k() return a mask array indicate if label in the top k result correct_samples += (batch_in_top_k(outputs, labels, top_k)).type(torch.float).sum().item() avg_loss = tot_loss/tot_nof_batch correct_rate = correct_samples/tot_samples print(f\u0026#34;Validate: top{top_k} Accuracy: {(100*correct_rate):\u0026gt;0.2f}%, Avg loss: {avg_loss:\u0026gt;8f}\u0026#34;) return (avg_loss, correct_rate) ## demo training validation loop validate_transforms = [None, torchvision.transforms.functional.hflip] for i_epoch in range(nof_epochs): print(f\u0026#34; ------ Epoch {i_epoch} ------ \u0026#34;) train_one_epoch(model, train_dataloader, loss_func, optimizer, device) validate_one_epoch_topk_aug(model, validate_dataloader, loss_func, validate_transforms, device, top_k) Random color shift In VGG, another augmentation technique involved adjusting the RGB values of training images by a random combination of the principal component analysis (PCA) eigenvectors derived from the RGB values across all pixels of all images in the training set. For a detailed explanation, refer to section 4.1 Data Augmentation in the AlexNet paper.\nHere\u0026rsquo;s how the random color shift is implemented:\nBefore training begins, we go through the entire training set and gather all RGB values from each image. This data is used to create an \\(m \\times n\\) data matrix, where \\(n\\) represents the number of channels (3 for RGB images) and \\(m\\) represents the total number of pixels across all images in the training set \\(m = \\text{number of images} \\times \\text{image height} \\times \\text{image width}\\). We then calculate the covariance matrix of this data matrix. Next, we conduct principal component analysis (PCA) on the covariance matrix using singular value decomposition (SVD). The resulting \\(U\\) matrix contains columns representing the PCA eigenvectors, and the \\(S\\) matrix contains the corresponding eigenvalues. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ## PCA for covariance matrix of image channels across all the pixels # train_dataloader is the dataloader iterating through the entire training data set input_dataloader = train_dataloader nof_batchs = len(input_dataloader) ch_vecs = [None for _ in range(nof_batchs)] for i_batch, data in enumerate(input_dataloader): inputs, labels = data # swap channel and batch axis and flatten the dimension of (batch, image height, image width) ch_vecs[i_batch] = torch.flatten(torch.swapaxes(inputs, 0, 1), start_dim = 1, end_dim = -1) ch_vecs = torch.cat(ch_vecs, dim = -1) ch_cov = torch.cov(ch_vecs) ch_vecs = None U, S, Vh = torch.linalg.svd(ch_cov, full_matrices = True) ## Each column of U is a channel PCA eigenvector ## S contains the corresponding to eigenvectors print(\u0026#34;U:\u0026#34;) print(repr(U)) print(\u0026#34;S:\u0026#34;) print(S) print(\u0026#34;Vh:\u0026#34;) print(Vh) Note: In this implementation, all pixels are loaded into computer memory at the same time. For larger datasets, the code for calculating the covariance matrix may need enhancements to compute it without simultaneously loading all data into memory.\nDuring training, we create a randomized linear combination of PCA eigenvectors by adding up the product of each eigenvector with a randomized amplitude. This amplitude is computed by multiplying the corresponding eigenvalue by a random value drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 0.1.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import functools from torchvision.transforms import v2 # image channel radom PCA eigenvec addition agumentation def random_ch_shift_pca(src_image, pca_eigenvecs, pca_eigenvals, random_paras = None): norm_meam = 0 norm_std = 0.1 if isinstance(random_paras, dict): norm_meam = random_paras.get(\u0026#34;mean\u0026#34;, norm_meam) norm_std = random_paras.get(\u0026#34;std\u0026#34;, norm_std) nof_dims = len(src_image.size()) nof_channels = src_image.size(0) assert(pca_eigenvecs.size(0) == nof_channels) assert(len(pca_eigenvals.size()) == 1) assert(pca_eigenvals.size(0) == pca_eigenvecs.size(1)) norm_means = 0 * torch.ones(pca_eigenvals.size()) norm_stds = 0.1 * torch.ones(pca_eigenvals.size()) alphas = torch.normal(norm_means, norm_stds) scale_factors = (alphas * pca_eigenvals).view((-1,1)) ch_offset = torch.matmul(pca_eigenvecs, scale_factors) ch_offset = ch_offset.view((nof_channels,) + (1,) * (nof_dims - 1)) dst_image = src_image + ch_offset return dst_image ## random channel shifts # NOTE: trainset_pca_eigenvecs and trainset_pca_eigenvals are the U and S matrix obtained from above mentioned PCA analysis trainset_pca_eigenvecs = torch.tensor([[-0.5580, 0.7063, 0.4356], [-0.5775, 0.0464, -0.8151], [-0.5960, -0.7063, 0.3820]]) print(trainset_pca_eigenvecs.size()) trainset_pca_eigenvals = torch.tensor([0.1719, 0.0139, 0.0029]) print(trainset_pca_eigenvals.size()) random_ch_shift = functools.partial(random_ch_shift_pca, pca_eigenvecs = trainset_pca_eigenvecs, pca_eigenvals = trainset_pca_eigenvals, random_paras = {\u0026#34;mean\u0026#34;: 0, \u0026#34;std\u0026#34;: 0.1}, ) random_ch_shift_transform = v2.Lambda(random_ch_shift) Other data augmentations The VGG paper also employed additional augmentation techniques like random translations and random crops. However, since the CIFAR dataset\u0026rsquo;s image size is much smaller (32x32) compared to the ImageNet dataset (256x256), there isn\u0026rsquo;t much flexibility to utilize these techniques effectively.\nSummary of data transformations In summary, the data transformations for the training set, including preprocessing and all data augmentation techniques, can be implemented as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 from torchvision.transforms import v2 # NOTE: subtract_ch_avg() is defined in the Data processing section # NOTE: random_ch_shift() is defined in the Random color shift section ## data transform for training train_data_transforms = v2.Compose([ v2.ToImage(), v2.ToDtype(torch.float32,scale = True), v2.Lambda(subtract_ch_avg), v2.RandomHorizontalFlip(0.5), v2.Lambda(random_ch_shift), ]) For the test/validation set, all we need to do is include the preprocessing step in the data transformations.\n1 2 3 4 5 6 7 8 9 10 from torchvision.transforms import v2 # NOTE: subtract_ch_avg() is defined in the Data processing section ## data transform for validation validate_data_transforms = v2.Compose([ v2.ToImage(), v2.ToDtype(torch.float32,scale = True), v2.Lambda(subtract_ch_avg), ]) Training and validation Top k accuracy (or error) In the VGG paper, the main way they measured performance was using the top k error. In my version, I focused on calculating the top k accuracy instead. Top k accuracy shows how often the actual label is among the top k predictions made by the model with the highest confidence. On the other hand, top k error tells us how often the actual label is not included in the top k predictions.\nThe relationship between top k error and top k accuracy is simply connected by the following formula:\n$$ \\text{top k error} = 1 − \\text{top k accuracy} $$\nA higher top k accuracy and lower top k error indicate better model performance.\nDuring the validation (or test) process, the top k-th accuracy can be estimated by dividing the total number of valiation (or test) samples by the number of samples where the label is in the top k predictions.\n$$ \\text{top k accuracy} = \\frac{\\text{number of samples (label is in top k predictions)}}{\\text{total number of samples}} $$\nTherefore, top k accuracy can be calculated using the following code:\n1 2 3 4 5 6 7 # Evaluate if label is within top k prediction result for one batch of data def batch_in_top_k(outputs, labels, top_k = 1): sorted_outputs, sorted_idxs = torch.sort(outputs, dim = -1, descending = True) in_top_k = torch.full_like(labels, False) for cur_idx in range(top_k): in_top_k = torch.logical_or(sorted_idxs[:,cur_idx] == labels, in_top_k) return in_top_k In each batch, we organize the softmax layer results, which represent the confidences for each predicted category, in descending order. Then, we check if the ground truth label is among the top k predictions. This check result is stored in a boolean mask array, where \u0026rsquo;true\u0026rsquo; indicates the label is in the top k predictions, and \u0026lsquo;false\u0026rsquo; indicates it\u0026rsquo;s not. This boolean mask array holds the results for all samples within the batch. To find the total number of samples where the label is among the top k predictions, we simply sum the mask arrays from all batches.\nLoss function, regularization, and optimizer VGG employs multinomial logistic regression as its loss function. For optimization, it utilizes mini-batch gradient descent with momentum and weight decay. In PyTorch, these can be implemented as follows:\n1 2 loss_func = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr = 1E-2, momentum = 0.9, weight_decay= 5E-4) Additionally, dropout regularization has been incorporated into the model as another form of regularization as mentioned earlier in this post.\nLearning rate adjustment In the VGG paper, the authors initially train with a learning rate of 1E-2. Then, they reduce the learning rate by a factor of 10 when the validation set accuracy plateaus. This can be implemented using the ReduceLROnPlateau() function provided by PyTorch, like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer = optimizer, mode = \u0026#34;max\u0026#34;, factor = 0.1, patience = 10, threshold = 1E-3, min_lr = 0, ) # Demo training and validation loop for i_epoch in range(nof_epochs): print(f\u0026#34; ------ Epoch {i_epoch} ------ \u0026#34;) cur_lr = optimizer.param_groups[0][\u0026#39;lr\u0026#39;]; print(f\u0026#34;current lr = {cur_lr}\u0026#34;) cur_train_loss = train_one_epoch(model, train_dataloader, loss_func, optimizer, device) cur_validate_loss, cur_validate_accuracy = validate_one_epoch_topk_aug(model, validate_dataloader, loss_func, validate_transforms, device, top_k) scheduler.step(cur_validate_accuracy) print(\u0026#34;\\n\u0026#34;) NOTE: The description of the ReduceLROnPlateau() function in the PyTorch documentation can be confusing. I found that reading the source code of the ReduceLROnPlateau() definition provides clearer understanding.\nTraining deep models Optimizing deep models from scratch with completely random initialization can be very challenging for the optimizer. It often leads to the learning process getting stuck for long periods.\nTo tackle this issue, the VGG authors first train a shallow model. Then, they use the learned parameters from this shallow model to initialize deeper ones.\nTransferring learned parameters between models in PyTorch is straightforward. It involves copying the weights and biases from corresponding layers between the two models. If you\u0026rsquo;re using the VGG model definition from this blog post, the example code looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## demo transfer model parameters input_image_width = 32 input_image_height = 32 # create model 1 model1_stacked_conv_list = [ [ {\u0026#34;nof_layers\u0026#34;: 1, \u0026#34;in_channels\u0026#34;: 3, \u0026#34;out_channels\u0026#34;: 64,}, ], [ {\u0026#34;nof_layers\u0026#34;: 1, \u0026#34;in_ckjhannels\u0026#34;: 64, \u0026#34;out_channels\u0026#34;: 128,}, ], ] conv_image_reduce_ratio = 2**len(model1_stacked_conv_list) conv_final_image_width = input_image_width//conv_image_reduce_ratio conv_final_image_height = input_image_height//conv_image_reduce_ratio model1_stacked_linear = [ { \u0026#34;nof_layers\u0026#34;: 1, \u0026#34;in_features\u0026#34;: conv_final_image_width * conv_final_image_height * 512, \u0026#34;out_features\u0026#34;: 512, \u0026#34;dropout_p\u0026#34;: 0.5 }, { \u0026#34;nof_layers\u0026#34;: 1, \u0026#34;out_features\u0026#34;: 10, \u0026#34;activation\u0026#34;: None } ] model1 = VGG.VGG( stacked_conv_descriptors = model1_stacked_conv_list, stacked_linear_descriptor = model1_stacked_linear, enable_debug = False, ) # create model 2 model2_stacked_conv_list = [ [ {\u0026#34;nof_layers\u0026#34;: 1, \u0026#34;in_channels\u0026#34;: 3, \u0026#34;out_channels\u0026#34;: 64,}, ], [ {\u0026#34;nof_layers\u0026#34;: 2, \u0026#34;in_ckjhannels\u0026#34;: 64, \u0026#34;out_channels\u0026#34;: 128,}, ], ] conv_image_reduce_ratio = 2**len(model2_stacked_conv_list) conv_final_image_width = input_image_width//conv_image_reduce_ratio conv_final_image_height = input_image_height//conv_image_reduce_ratio model2_stacked_linear = [ { \u0026#34;nof_layers\u0026#34;: 1, \u0026#34;in_features\u0026#34;: conv_final_image_width * conv_final_image_height * 512, \u0026#34;out_features\u0026#34;: 512, \u0026#34;dropout_p\u0026#34;: 0.5 }, { \u0026#34;nof_layers\u0026#34;: 1, \u0026#34;out_features\u0026#34;: 10, \u0026#34;activation\u0026#34;: None } ] model2 = VGG.VGG( stacked_conv_descriptors = model2_stacked_conv_list, stacked_linear_descriptor = model2_stacked_linear, enable_debug = False, ) # transfer parameter of 1st convoluation layer from model 1 to model 2 model2.network[0].network[0].weight = model1.network[0].network[0].weight model2.network[0].network[0].bias = model1.network[0].network[0].bias NOTE: We\u0026rsquo;ve organized the sequential layers of the VGG model and stacked them within the \u0026ldquo;network\u0026rdquo; attribute of the object. This means we can access each specific layer inside the network by indexing the \u0026ldquo;network\u0026rdquo; attribute.\nResults Given the large size of the ImageNet dataset and the extensive time required for training, we\u0026rsquo;ll opt for a smaller dataset, CIFAR10, to demonstrate training and validation more quickly.\nI\u0026rsquo;ve examined several models based on the VGG architecture, and I\u0026rsquo;ve listed some of them (model I, II, and III) below:\nModel Configuration I II III conv3-128 conv3-128 conv3-128 maxpool conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 maxpool conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 maxpool FC-1024 FC-1024 FC-1024 FC-1024 FC-1024 FC-10 FC-10 FC-10 soft-max After training these model variations, I computed the top 1 to top 5 accuracies using the CIFAR10 test dataset. Here\u0026rsquo;s a summary of the results:\nModel config. top-1 accuarcy(%) top-2 accuarcy(%) top-3 accuarcy(%) top-4 accuarcy(%) top-5 accuarcy(%) I 82.45 92.74 96.23 97.82 98.87 II 84.88 93.95 96.91 98.23 98.99 III 86.93 94.39 96.83 98.15 98.90 We can observe that the accuracy tends to improve as the depth of the models increases.\nConclusion In this blog post, we\u0026rsquo;ve covered the implementation, training, and evaluation of the VGG network in a step-by-step manner. The VGG model showcases the effectiveness of deep neural networks in tackling image classification tasks. Moreover, their methods for data augmentation, regularization, and training provide valuable insights and lessons for training deep neural networks.\nReference [1] Simonyan, K., \u0026amp; Zisserman, A. (2014). Very deep convolutional networks for Large-Scale image recognition. arXiv (Cornell University). https://doi.org/10.48550/arxiv.1409.1556\n[2] Krizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems, 25, 1097–1105. https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\n[3] Krizhevsky, A., Nair, V. and Hinton, G. (2014) The CIFAR-10 Dataset. https://www.cs.toronto.edu/~kriz/cifar.html\nCitation If you found this article helpful, please cite it as:\nZhong, Jian (May 2024). Building and Training VGG with PyTorch: A Step-by-Step Guide. Vision Tech Insights. https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_vgg_pytorch/.\nOr\n@article{zhong2024buildtrainVGGPyTorch, title = \u0026#34;Building and Training VGG with PyTorch: A Step-by-Step Guide\u0026#34;, author = \u0026#34;Zhong, Jian\u0026#34;, journal = \u0026#34;jianzhongdev.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_vgg_pytorch/\u0026#34; } ","permalink":"http://localhost:1313/posts/implement_train_vgg_pytorch/","summary":"The VGG (Visual Geometry Group) model is a type of convolutional neural network (CNN) outlined in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition. It\u0026rsquo;s known for its use of small convolution filters and deep layers, which helped it achieve top-notch performance in tasks like image classification. By stacking multiple layers with small kernel sizes, VGG can capture a wide range of features from input images. Plus, adding more rectification layers makes its decision-making process sharper and more accurate.","title":"Building and Training VGG with PyTorch: A Step-by-Step Guide"},{"content":"Configuration files are commonly used to adjust settings in computer programs. I\u0026rsquo;m presently developing a configuration file parser for my high-speed data acquisition system using C++. Along the way, I\u0026rsquo;ve discovered some useful techniques involving C++ generics and inheritance that streamline coding. Therefore, I decided to document these tricks in the hope that they\u0026rsquo;ll be beneficial to others. You can find the ready-to-use source code for this configuration file parser in my GitHub repository. (URL: https://github.com/JianZhongDev/CppConfigFile.)\nConfiguration Files According to Wikipedia, configuration files are files used to set up the parameters and initial settings for computer programs. A configuration file parser is a piece of program that allows saving program settings to and loading them from configuration files. Configuration files are very handy when users need to provide certain configuration settings to the program before starting it. In my research, I\u0026rsquo;ve also discovered the convenience of having a configuration file parser to record the configuration settings of my experiments. This enables me to quickly switch between different software settings for various applications.\nRequirement Analysis For a data acquisition program, users often need to fine-tune settings to optimize performance for their specific needs. This includes things like timing delays, filtering coefficients, and switching between different data processing methods. These settings are stored as variables with various data types (like numbers, strings, and arrays) in the software. So, the configuration file parser has to handle a wide range of data types.\nWe also want the configuration file to be easily readable and editable by users, so it needs to be in a human-readable text format. This means the parser should be able to convert variables to strings and back again.\nPlus, it\u0026rsquo;d be great if users could add comments to the configuration file to keep track of changes.\nIn summary, here\u0026rsquo;s what the configuration file parser needs to do:\nStore multiple setting variables and their values. Handle values with different data types. Convert variables to strings, and update values from strings. Save settings to a text file that\u0026rsquo;s easy for humans to read. Update variable values from the text file. Process configuration files with comments. Data Structure and Algorithm Design Once we\u0026rsquo;ve nailed down the requirements, we can begin designing the data structures to meet them.\nGeneric Entry To handle the task of storing variables with different data types (requirement 2), we can develop our own custom class called GenericEntry. This class will enable us to access the data within it using the set() and get() methods for writing and reading data, respectively. Since different data types require different methods for reading and writing, we make these set() and get() methods virtual and require subclasses to implement them. The GenericEntry class also includes a type_name member and a get_typename() method to record the data type and verify data types.\nFor converting data to and from strings (requirement 3), considering that various data types require different approaches for this conversion, the GenericEntry class provides write_val_string() and read_val_string() methods. These methods facilitate converting the data value to a string and vice versa.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // Base type of generic entry class GenericEntry { protected: std::string type_name; public: //TODO: could potentially change the return type from void to int and return error flag //TODO: could potentially use type_index instead of hardcoded string as the type identifier GenericEntry() { // set up the type_name in the constructor this-\u0026gt;type_name = \u0026#34;generic_entry\u0026#34;; } // Set value of the entry virtual void set() { //Override this method in subclass } // Get value of the entry virtual void get() { //Override this method in subclass } // Return string of the type name void get_typename(std::string* dst_string) { *dst_string = this-\u0026gt;type_name; } // Write the value of the entry into string virtual void write_val_string(std::string* dst_string) { //Override this method in subclass } // Read value of the entry from string virtual void read_val_string(const std::string\u0026amp; src_string) { //Override this method in subclass } }; Once we\u0026rsquo;ve set up the most basic entry type, we create a more specialized subclass named TypedEntry. Leveraging the generics template feature, we implement the set() and get() functions. However, because custom classes, iterable types, and primitive data types (like int and unsigned int) require unique approaches for converting their data to strings, we leave the write_val_string()and read_val_string() methods for future implementation in more specialized subclasses.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // Entry of generic type definition template\u0026lt;typename data_t\u0026gt; class TypedEntry : public GenericEntry { protected: data_t data; public: //Constructor without initial value TypedEntry() { this-\u0026gt;type_name = \u0026#34;typed_entry\u0026#34;; } //Constructor with initial value TypedEntry(data_t data) { this-\u0026gt;TypedEntry(); this-\u0026gt;data = data_t(data); } // Implemented set entry value method template\u0026lt;typename data_t\u0026gt; void set(const data_t\u0026amp; data) { this-\u0026gt;data = data_t(data); } // Implemented get entry value method template\u0026lt;typename data_t\u0026gt; void get(data_t* data_p) { *data_p = data_t(this-\u0026gt;data); } virtual void write_val_string(std::string* dst_string) { //Override this method in subclass } virtual void read_val_string(const std::string\u0026amp; src_string) { //Override this method in subclass } }; Primitive types (like int and unsigned int) have straightforward methods for converting between data and strings. We can implement their entry classes like this: For each specific primitive type, we simply inherit from the primitive type entry and specify the type name in the constructors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // Entries with primitive type template\u0026lt;typename data_t\u0026gt; class PrimitiveTypeEntry : public TypedEntry\u0026lt;data_t\u0026gt; { // NOTE: Only need to define contructor giving type_name in the subclasses public: PrimitiveTypeEntry() { this-\u0026gt;type_name = \u0026#34;primitivetype_entry\u0026#34;; } PrimitiveTypeEntry(const data_t\u0026amp; data) { this-\u0026gt;PrimitiveTypeEntry(); this-\u0026gt;data = data_t(data); } virtual void write_val_string(std::string* dst_string) { *dst_string = std::to_string(this-\u0026gt;data); } virtual void read_val_string(const std::string\u0026amp; src_string) { if (std::is_fundamental\u0026lt;data_t\u0026gt;::value) { // validate the data type is primitive data type std::stringstream(src_string) \u0026gt;\u0026gt; this-\u0026gt;data; // use stringstream to convert value string to value } } }; // Entries with int type class IntEntry : public PrimitiveTypeEntry\u0026lt;int\u0026gt; { public: IntEntry(int data = 0) { this-\u0026gt;type_name = \u0026#34;int\u0026#34;; this-\u0026gt;data = data; } }; We can apply a similar approach to define types for vectors as well.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 // Vector class with primitive data type template\u0026lt;typename data_t\u0026gt; class VectorPrimitiveTypeEntry : public TypedEntry\u0026lt;std::vector\u0026lt;data_t\u0026gt;\u0026gt; { // NOTE: Only need to define contructor giving type_name in the subclasses protected: //NOTE: data string format: {val0, val1, val2} std::string str_dl = \u0026#34;,\u0026#34;; std::string str_enclosure[2] = { \u0026#34;{\u0026#34;, \u0026#34;}\u0026#34; }; public: VectorPrimitiveTypeEntry() { this-\u0026gt;type_name = \u0026#34;vector_primitivetype\u0026#34;; } VectorPrimitiveTypeEntry(const std::vector\u0026lt;data_t\u0026gt;\u0026amp; data) { this-\u0026gt;VectorPrimitiveTypeEntry(); this-\u0026gt;data = std::vector\u0026lt;data_t\u0026gt;(data); } virtual void write_val_string(std::string* dst_string) { std::stringstream result_strstream; unsigned data_len = this-\u0026gt;data.size(); unsigned count = 0; // iterate through data vector result_strstream \u0026lt;\u0026lt; this-\u0026gt;str_enclosure[0]; for (auto itr = this-\u0026gt;data.begin(); itr != this-\u0026gt;data.end(); ++itr) { result_strstream \u0026lt;\u0026lt; std::to_string(*itr); count++; if (count \u0026lt; data_len) result_strstream \u0026lt;\u0026lt; this-\u0026gt;str_dl; } result_strstream \u0026lt;\u0026lt; this-\u0026gt;str_enclosure[1]; *dst_string = result_strstream.str(); } virtual void read_val_string(const std::string\u0026amp; src_string) { // remove \u0026#39;{\u0026#39;, \u0026#39;}\u0026#39;, and \u0026#39;_\u0026#39; std::string tmp_str = helper_extract_string_between_enclosure(src_string, str_enclosure[0], str_enclosure[1]); tmp_str = helper_clean_tailheadchars_string(tmp_str, std::unordered_set\u0026lt;char\u0026gt;{\u0026#39; \u0026#39;}); // extract value string for each element std::vector\u0026lt;std::string\u0026gt; val_strs = helper_split_string_with_delimiter(tmp_str, this-\u0026gt;str_dl); if (std::is_fundamental\u0026lt;data_t\u0026gt;::value) { // validate data type // iterate through value strings for each element this-\u0026gt;data.clear(); for (auto itr = val_strs.begin(); itr != val_strs.end(); ++itr) { data_t tmp_val; std::stringstream(*itr) \u0026gt;\u0026gt; tmp_val; this-\u0026gt;data.push_back(tmp_val); } } } }; // Entry with float vector class VectorFloatEntry : public VectorPrimitiveTypeEntry\u0026lt;float\u0026gt; { public: VectorFloatEntry(const std::vector\u0026lt;float\u0026gt;\u0026amp; data = { 0.0 }) { this-\u0026gt;type_name = \u0026#34;vector_float\u0026#34;; this-\u0026gt;data = std::vector\u0026lt;float\u0026gt;(data); } }; Since a string is a more specialized class-based data type, we need to define its entry separately.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // Entries with string type class StringEntry : public TypedEntry\u0026lt;std::string\u0026gt; { protected: // NOTE: value string format: \u0026#34;value_string\u0026#34; std::string str_enclosure[2] = { \u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34; }; public: StringEntry(const std::string\u0026amp; data = \u0026#34;\u0026#34;) { this-\u0026gt;type_name = \u0026#34;string\u0026#34;; this-\u0026gt;data = data; } virtual void write_val_string(std::string* dst_string) { // Add \u0026#34; \u0026#34; to string *dst_string = str_enclosure[0] + std::string(this-\u0026gt;data) + str_enclosure[1]; } virtual void read_val_string(const std::string\u0026amp; src_string) { // Extract string between \u0026#34; \u0026#34; std::string tmp_str = helper_extract_string_between_enclosure(src_string, str_enclosure[0], str_enclosure[1]); this-\u0026gt;data = std::string(tmp_str); } }; Generic Hashmap Once we\u0026rsquo;ve got our generic entry class ready, we can tackle the task of storing data for multiple settings variables (requirement 1). We can achieve this by using a hash map (std::unordered_map), where we map the name of each setting variable to the entry storing its value. One important thing to remember is that when defining the hashmap, the value should be declared as a pointer to the base class. This prevents any issues where a subclass might get casted into the base class when adding it to the hashmap.\n1 2 typedef std::unordered_map\u0026lt;std::string, GenericEntry*\u0026gt; GenHashMap; GenHashMap test_genhashmap; With this generic hashmap setup, adding setting variables and entries is straightforward:\n1 2 // initialize generic hash map test_genhashmap[\u0026#34;int_val\u0026#34;] = new IntEntry(1); Converting the entry to and from a string is as simple as this:\n1 2 3 4 5 // update entry with string test_map[\u0026#34;int_val\u0026#34;]-\u0026gt;read_val_string(\u0026#34;-1\u0026#34;); // convert entry value into string std::string tmp_valstr; test_map[\u0026#34;int_val\u0026#34;]-\u0026gt;write_val_string(\u0026amp;tmp_valstr); After casting the entry to its subclass, we can easily set and retrieve values within the entry.\n1 2 3 4 5 // set value of entry ((IntEntry*)test_genhashmap[\u0026#34;int_val\u0026#34;])-\u0026gt;set(-1); // get value from entry int tmp_int; ((IntEntry*)test_genhashmap[\u0026#34;int_val\u0026#34;])-\u0026gt;get(\u0026amp;tmp_int); Furthermore, we can easily determine the type of the entry by calling the get_typename() method.\n1 2 3 // get type name string from entry std::string tmp_typename; test_map[\u0026#34;int_val\u0026#34;]-\u0026gt;get_typename(\u0026amp;tmp_typename); To simplify clearing the entire hashmap, I\u0026rsquo;ve created the clear_genhashmap() function, outlined below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 typedef int errflag_t; // delete all the elements in a generic hash map errflag_t clear_genhashmap( GenHashMap\u0026amp; gen_hashmap ) { // iterate through the hash map to release all the entries for (auto key_val_pair : gen_hashmap) { delete key_val_pair.second; } gen_hashmap.clear(); return 1; } Note: If maintaining the order of setting variables in the configuration file is crucial for your application, you can easily achieve this by switching the data type from std::unordered_map (hashmap) to std::ordered_map (tree-based map). Everything else in the code remains unchanged and can be used as is.\nSaving Configuration Files Since we\u0026rsquo;ve already implemented the string conversion function in the entries, saving the setting parameters to human-readable text files is straightforward. We simply need to iterate through the generic hashmap, saving the name (key), type, and value of each entry. Then, we add entry separators at the end of each entry and dump them into a text file. Additionally, I\u0026rsquo;ve included a header string to provide some helpful information in the configuration file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 // pack type name value string into one string std::string helper_pack_type_name_val_string( const std::string\u0026amp; type_string, const std::string\u0026amp; name_string, const std::string\u0026amp; val_string, const std::string\u0026amp; type_name_dl = \u0026#34; \u0026#34;, const std::string\u0026amp; name_val_dl = \u0026#34;=\u0026#34; ) { return type_string + type_name_dl + name_string + name_val_dl + val_string; } typedef int errflag_t; // save generic hash map entries to configuration text file errflag_t save_genhashmap_to_txt( const GenHashMap\u0026amp; gen_hashmap, const std::string\u0026amp; dst_file_path, std::ios_base::openmode dst_file_openmode = std::ios_base::out, const std::string\u0026amp; type_name_dl = \u0026#34; \u0026#34;, const std::string\u0026amp; name_val_dl = \u0026#34;=\u0026#34;, const std::string\u0026amp; entry_stop_str = \u0026#34;;\u0026#34;, const std::vector\u0026lt;std::string\u0026gt;\u0026amp; default_message_enclousre = {\u0026#34;/*\u0026#34;, \u0026#34;*/\u0026#34;}, const std::string\u0026amp; head_message = \u0026#34;\u0026#34; ) { errflag_t err_flag = 0; std::ofstream dst_file(dst_file_path, dst_file_openmode); if (dst_file.is_open()) { // save head message if given if (head_message.size() \u0026gt; 0) { dst_file \u0026lt;\u0026lt; default_message_enclousre[0] + head_message + default_message_enclousre[1] + \u0026#34;\\n\u0026#34;; } // iterate though hash map and save all entries for (const auto\u0026amp; key_val_pair : gen_hashmap) { std::string cur_name_str = key_val_pair.first; std::string cur_type_str; std::string cur_val_str; key_val_pair.second-\u0026gt;get_typename(\u0026amp;cur_type_str); key_val_pair.second-\u0026gt;write_val_string(\u0026amp;cur_val_str); // convert type name value to entry string std::string cur_entry_str = helper_pack_type_name_val_string( cur_type_str, cur_name_str, cur_val_str, type_name_dl, name_val_dl ); dst_file \u0026lt;\u0026lt; cur_entry_str + entry_stop_str + \u0026#34;\\n\u0026#34;; } dst_file.close(); // close file err_flag = 1; } else { //std::cout \u0026lt;\u0026lt; \u0026#34;ERR:\\t Unable to open file. File path = \u0026#34; + dst_file_path \u0026lt;\u0026lt; std::endl; std::string err_msg = \u0026#34;ERR:\\t Unable to open file. File path = \u0026#34; + dst_file_path + \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; err_msg; err_flag = -1; } return err_flag; } Loading Configuration Files Reading the setting information from the configuration file involves a bit more complexity. We need to handle comments in the file and avoid mistakenly reading separators within strings of entries with string type. These requirements are addressed by iterating through the entire configuration file string using two pointers. Here\u0026rsquo;s how it works:\nThe faster pointer moves ahead to mark the end of each candidate string while continuously checking the substring. The slower pointer sets the start position of each candidate string. Depending on the substring, the algorithm behaves as follows: If the substring matches the start separator of a string candidate to ignore, the faster pointer moves forward while ignoring all substrings until it finds the end separator of the ignore string candidate. If the substring matches the start separator of a comment string, the faster pointer continues moving forward while ignoring until it finds the end separator of the comment string candidate. The slower pointer ends up positioned after the end of the comment candidate string so that it\u0026rsquo;s not read in. If the substring matches the end separator of an entry string candidate, the substring between the slow and fast pointers is saved into the result vector. This indicates that we\u0026rsquo;ve found the string for the setting parameter entry. The entry string candidate undergoes some cleaning processes to remove any extra spaces and newline characters at both ends. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 // extract entry strings from complicated strings std::vector\u0026lt;std::string\u0026gt; helper_extract_entrystr( const std::string\u0026amp; src_string, const std::string\u0026amp; entry_stop_str = \u0026#34;;\u0026#34;, //string indicates the end of an entry string std::unordered_map\u0026lt;std::string, std::string\u0026gt; ignore_left_to_right_map = { {\u0026#34;//\u0026#34;, \u0026#34;\\n\u0026#34;}, {\u0026#34;/*\u0026#34;, \u0026#34;*/\u0026#34;} }, //string parts between \u0026#34;left\u0026#34; and \u0026#34;right\u0026#34; to ignore std::unordered_map\u0026lt;std::string, std::string\u0026gt; include_left_to_right_map = { {\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34;} } //string parts between \u0026#34;left\u0026#34; and \u0026#34;right\u0026#34; to include ) { std::size_t slow_idx = 0; std::size_t fast_idx = 0; std::size_t srcstr_len = src_string.size(); std::size_t entry_stop_str_len = entry_stop_str.size(); // count string lengths in the left_to_right map keys std::unordered_set\u0026lt;std::size_t\u0026gt; ignore_left_lens; for (const auto\u0026amp; itr : ignore_left_to_right_map) { ignore_left_lens.insert(itr.first.size()); } // count string lengths in the left_to_right map keys std::unordered_set\u0026lt;std::size_t\u0026gt; include_left_lens; for (const auto\u0026amp; itr : include_left_to_right_map) { include_left_lens.insert(itr.first.size()); } // itrate through src_string to find all entry strings std::vector\u0026lt;std::string\u0026gt; entry_strs; while (fast_idx \u0026lt; srcstr_len) { // check string between \u0026#34;left\u0026#34; and \u0026#34;right\u0026#34; to include for (auto cur_left_len : include_left_lens) { if (fast_idx + cur_left_len \u0026gt; srcstr_len) continue; std::string cur_left_str = src_string.substr(fast_idx, cur_left_len); if (include_left_to_right_map.find(cur_left_str) != include_left_to_right_map.end()) { std::string cur_right_str = include_left_to_right_map[cur_left_str]; std::size_t cur_right_len = cur_right_str.size(); for (fast_idx += cur_left_len; fast_idx + cur_right_len \u0026lt; srcstr_len; ++fast_idx) { if (src_string.substr(fast_idx, cur_right_len) == cur_right_str) break; } fast_idx += cur_right_len; } } // check string between \u0026#34;left\u0026#34; and \u0026#34;right\u0026#34; to exclude for (auto cur_left_len : ignore_left_lens) { if (fast_idx + cur_left_len \u0026gt; srcstr_len) continue; std::string cur_left_str = src_string.substr(fast_idx, cur_left_len); if (ignore_left_to_right_map.find(cur_left_str) != ignore_left_to_right_map.end()) { std::string cur_right_str = ignore_left_to_right_map[cur_left_str]; std::size_t cur_right_len = cur_right_str.size(); for (fast_idx += cur_left_len; fast_idx + cur_right_len \u0026lt; srcstr_len; ++fast_idx) { if (src_string.substr(fast_idx, cur_right_len) == cur_right_str) break; } fast_idx += cur_right_len; slow_idx = fast_idx; } } if (fast_idx + entry_stop_str_len \u0026gt; srcstr_len) break; // reach the end of src_string // found complete entry string if (src_string.substr(fast_idx, entry_stop_str_len) == entry_stop_str) { entry_strs.push_back(src_string.substr(slow_idx, fast_idx - slow_idx)); slow_idx = fast_idx + 1; } ++fast_idx; } return entry_strs; } Once we have the entry strings, we\u0026rsquo;ll break them down into type, name, and value fields. Each string for these fields will be cleaned up, removing any extra spaces and newline characters at both ends. Then, we\u0026rsquo;ll check if the hashmap has a key with the specified name, using it to identify the entries. We\u0026rsquo;ll then examine the value of the entry and update it with the corresponding value string if the type matches.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 typedef int errflag_t; // update generic hash map entries according to configuration text file errflag_t update_genhashmap_from_txt( GenHashMap\u0026amp; gen_hashmap, const std::string\u0026amp; src_file_path, std::ios_base::openmode src_file_openmode = std::ios_base::in, const std::string\u0026amp; type_name_dl = \u0026#34; \u0026#34;, const std::string\u0026amp; name_val_dl = \u0026#34;=\u0026#34;, const std::string\u0026amp; entry_stop_str = \u0026#34;;\u0026#34;, const std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026amp; ignore_left_to_right_map = {{\u0026#34;//\u0026#34;, \u0026#34;\\n\u0026#34;}, {\u0026#34;/*\u0026#34;, \u0026#34;*/\u0026#34;}}, const std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026amp; include_left_to_right_map = {{\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34;}}, const std::unordered_set\u0026lt;char\u0026gt;\u0026amp; rm_chars = {\u0026#39; \u0026#39;, \u0026#39;\\n\u0026#39;, \u0026#39;\\t\u0026#39;} ) { errflag_t err_flag = 0; std::ifstream src_file(src_file_path, src_file_openmode); if (src_file.is_open()) { std::string src_string( (std::istreambuf_iterator\u0026lt;char\u0026gt;(src_file)), std::istreambuf_iterator\u0026lt;char\u0026gt;() ); std::vector\u0026lt;std::string\u0026gt; entry_strings = helper_extract_entrystr( src_string, entry_stop_str, ignore_left_to_right_map, include_left_to_right_map ); for (auto\u0026amp; cur_entry_str : entry_strings) { std::string tmp_str; std::size_t tmp_str_len = 0; //clean up entry string tmp_str = helper_bothside_clean_chars( cur_entry_str, rm_chars ); // split entry string std::string type_string; std::string name_string; std::string value_string; helper_split_entrystr_into_type_name_val( tmp_str, type_name_dl, name_val_dl, \u0026amp;type_string, \u0026amp;name_string, \u0026amp;value_string ); // clean up name string name_string = helper_bothside_clean_chars( name_string, rm_chars ); // update entry if name exists if (gen_hashmap.find(name_string) != gen_hashmap.end()) { type_string = helper_bothside_clean_chars( type_string, rm_chars ); // update entry if type match std::string hp_typename; gen_hashmap[name_string]-\u0026gt;get_typename(\u0026amp;hp_typename); if (type_string == hp_typename) { value_string = helper_bothside_clean_chars( value_string, rm_chars ); gen_hashmap[name_string]-\u0026gt;read_val_string(value_string); } else { //std::cout \u0026lt;\u0026lt; \u0026#34;ERR:\\tType mismatch! \u0026#34; + type_string + \u0026#34; \u0026lt;--\u0026gt; \u0026#34; + hp_typename + \u0026#34;\\n\u0026#34;; std::string err_string = \u0026#34;ERR:\\tType mismatch! \u0026#34; + type_string + \u0026#34; \u0026lt;--\u0026gt; \u0026#34; + hp_typename + \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; err_string; } } else { //std::cout \u0026lt;\u0026lt; \u0026#34;ERR:\\tName not found! \u0026#34; + name_string + \u0026#34;\\n\u0026#34;; std::string err_string = \u0026#34;ERR:\\tName not found! \u0026#34; + name_string + \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; err_string; } } err_flag = 1; } else { //std::cout \u0026lt;\u0026lt; \u0026#34;ERR:\\t Unable to open file. File path = \u0026#34; + src_file_path \u0026lt;\u0026lt; std::endl; std::string err_string = \u0026#34;ERR:\\t Unable to open file. File path = \u0026#34; + src_file_path + \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; err_string; err_flag = -1; } return err_flag; } Since we typically use the same separators and comment notations when writing and reading the configuration file, it makes sense to create a class to store these separators and comment notations for the functions responsible for saving and loading the configuration file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 typedef int errflag_t; // class for text IO of generic hash map class GenHashMapIOTxt { public: std::string type_name_dl = \u0026#34; \u0026#34;; std::string name_val_dl = \u0026#34;=\u0026#34;; std::string entry_stop_str = \u0026#34;;\u0026#34;; std::vector\u0026lt;std::string\u0026gt; default_message_enclousre = { \u0026#34;/*\u0026#34;, \u0026#34;*/\u0026#34; }; std::unordered_map\u0026lt;std::string, std::string\u0026gt; ignore_left_to_right_map = { {\u0026#34;//\u0026#34;, \u0026#34;\\n\u0026#34;}, {\u0026#34;/*\u0026#34;, \u0026#34;*/\u0026#34;} }; std::unordered_map\u0026lt;std::string, std::string\u0026gt; include_left_to_right_map = { {\u0026#34;\\\u0026#34;\u0026#34;, \u0026#34;\\\u0026#34;\u0026#34;} }; std::unordered_set\u0026lt;char\u0026gt; rm_chars = { \u0026#39; \u0026#39;, \u0026#39;\\n\u0026#39;, \u0026#39;\\t\u0026#39; }; // save generic hash map to file errflag_t save_to_file( const GenHashMap\u0026amp; gen_hashmap, const std::string\u0026amp; dst_file_path, std::ios_base::openmode dst_file_openmode = std::ios_base::out, const std::string\u0026amp; head_message = \u0026#34;\u0026#34; ) { return save_genhashmap_to_txt( gen_hashmap, dst_file_path, dst_file_openmode, this-\u0026gt;type_name_dl, //this-\u0026gt;name_val_dl, \u0026#34; \u0026#34; + this-\u0026gt;name_val_dl + \u0026#34; \u0026#34;, this-\u0026gt;entry_stop_str, this-\u0026gt;default_message_enclousre, head_message ); } // load generic hash map errflag_t update_from_file( GenHashMap\u0026amp; gen_hashmap, const std::string\u0026amp; src_file_path, std::ios_base::openmode src_file_openmode = std::ios_base::in ) { return update_genhashmap_from_txt( gen_hashmap, src_file_path, src_file_openmode, this-\u0026gt;type_name_dl, this-\u0026gt;name_val_dl, this-\u0026gt;entry_stop_str, this-\u0026gt;ignore_left_to_right_map, this-\u0026gt;include_left_to_right_map, this-\u0026gt;rm_chars ); } }; Unit Test Finally, we can quickly test the configuration file parser. In the following code, I\u0026rsquo;ve used an integer entry, a vector of floats entry, and a string entry as examples.\nFirst, we create the generic hashmap and add initial entries to it. Then, we save the generic hashmap to a text file. Next, we modify the values in the generic map. Finally, we load the values from the configuration file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 void main() { GenHashMap test_genhashmap; GenHashMapIOTxt test_io_txt; // initialize generic hash map test_genhashmap[\u0026#34;int_val\u0026#34;] = new IntEntry(1); test_genhashmap[\u0026#34;string_val\u0026#34;] = new StringEntry(\u0026#34;Welcome to Vision Tech Insights!\u0026#34;); test_genhashmap[\u0026#34;vector_float_val\u0026#34;] = new VectorFloatEntry({ 0.1, 0.2, 0.3, 0.4, 0.5 }); std::cout \u0026lt;\u0026lt; \u0026#34;====== Initialize gen hashmap ======\u0026#34; \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; // print all the values in the generic hash map std::cout \u0026lt;\u0026lt; \u0026#34;--- genhashmap values START ---\u0026#34; \u0026lt;\u0026lt; std::endl; for (const auto\u0026amp; key_val_pair : test_genhashmap) { std::string cur_name_str = key_val_pair.first; std::string cur_type_str; std::string cur_val_str; key_val_pair.second-\u0026gt;get_typename(\u0026amp;cur_type_str); key_val_pair.second-\u0026gt;write_val_string(\u0026amp;cur_val_str); std::cout \u0026lt;\u0026lt; cur_type_str \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; cur_name_str \u0026lt;\u0026lt; \u0026#34; = \u0026#34; \u0026lt;\u0026lt; cur_val_str \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; \u0026#34;--- genhashmap values END ---\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; // save generic hash map into a text file std::cout \u0026lt;\u0026lt; \u0026#34;====== Save gen hashmap to text file ======\u0026#34; \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; // save to txt configuration file test_io_txt.save_to_file( test_genhashmap, \u0026#34;test_config_file.txt\u0026#34;, std::ios_base::out, \u0026#34;This is the test configuration file for Vision Tech Insights blog.\u0026#34; ); // save generic hash map into a text file std::cout \u0026lt;\u0026lt; \u0026#34;====== Set values in gen hashmap ======\u0026#34; \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; // try to change some of the values ((IntEntry*)test_genhashmap[\u0026#34;int_val\u0026#34;])-\u0026gt;set(-1); ((StringEntry*)test_genhashmap[\u0026#34;string_val\u0026#34;])-\u0026gt;set(std::string(\u0026#34;Hello world!\u0026#34;)); ((VectorFloatEntry*)test_genhashmap[\u0026#34;vector_float_val\u0026#34;])-\u0026gt;set(std::vector\u0026lt;float\u0026gt;{-0.1, -0.2, -0.3}); std::cout \u0026lt;\u0026lt; \u0026#34;--- genhashmap values START ---\u0026#34; \u0026lt;\u0026lt; std::endl; for (const auto\u0026amp; key_val_pair : test_genhashmap) { std::string cur_name_str = key_val_pair.first; std::string cur_type_str; std::string cur_val_str; key_val_pair.second-\u0026gt;get_typename(\u0026amp;cur_type_str); key_val_pair.second-\u0026gt;write_val_string(\u0026amp;cur_val_str); std::cout \u0026lt;\u0026lt; cur_type_str \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; cur_name_str \u0026lt;\u0026lt; \u0026#34; = \u0026#34; \u0026lt;\u0026lt; cur_val_str \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; \u0026#34;--- genhashmap values END ---\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; // try to get value from genhashmap std::cout \u0026lt;\u0026lt; \u0026#34;====== Get values from gen hashmap ======\u0026#34; \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; int tmp_int; ((IntEntry*)test_genhashmap[\u0026#34;int_val\u0026#34;])-\u0026gt;get(\u0026amp;tmp_int); std::string tmp_string; ((StringEntry*)test_genhashmap[\u0026#34;string_val\u0026#34;])-\u0026gt;get(\u0026amp;tmp_string); std::vector\u0026lt;float\u0026gt; tmp_float_vec; ((VectorFloatEntry*)test_genhashmap[\u0026#34;vector_float_val\u0026#34;])-\u0026gt;get(\u0026amp;tmp_float_vec); std::cout \u0026lt;\u0026lt; \u0026#34;--- get values START ---\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;tmp_int = \u0026#34; \u0026lt;\u0026lt; tmp_int \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;tmp_float_vec = {\u0026#34;; for (int i_val = 0; i_val \u0026lt; tmp_float_vec.size(); i_val++) { std::cout \u0026lt;\u0026lt; tmp_float_vec[i_val]; if (i_val \u0026lt; tmp_float_vec.size() - 1) { std::cout \u0026lt;\u0026lt; \u0026#34;, \u0026#34;; } } std::cout \u0026lt;\u0026lt; \u0026#34;}\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;tmp_string = \u0026#34; \u0026lt;\u0026lt; tmp_string \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;--- get values END ---\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; // load generic hash map from the text file std::cout \u0026lt;\u0026lt; \u0026#34;====== Load gen hashmap from text file ======\u0026#34; \u0026lt;\u0026lt; std::endl \u0026lt;\u0026lt; std::endl; // update values according to txt configuration file test_io_txt.update_from_file( test_genhashmap, \u0026#34;test_config_file.txt\u0026#34;, std::ios_base::in ); std::cout \u0026lt;\u0026lt; \u0026#34;--- genhashmap values START ---\u0026#34; \u0026lt;\u0026lt; std::endl; for (const auto\u0026amp; key_val_pair : test_genhashmap) { std::string cur_name_str = key_val_pair.first; std::string cur_type_str; std::string cur_val_str; key_val_pair.second-\u0026gt;get_typename(\u0026amp;cur_type_str); key_val_pair.second-\u0026gt;write_val_string(\u0026amp;cur_val_str); std::cout \u0026lt;\u0026lt; cur_type_str \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; cur_name_str \u0026lt;\u0026lt; \u0026#34; = \u0026#34; \u0026lt;\u0026lt; cur_val_str \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; \u0026#34;--- genhashmap values END ---\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; // release generic entries and clear gen hashmap clear_genhashmap(test_genhashmap); } The resulting configuration file (\u0026ldquo;test_config_file.txt\u0026rdquo;) looks like this:\n/*This is the test configuration file for Vision Tech Insights blog.*/\rint int_val = 1;\rstring string_val = \u0026#34;Welcome to Vision Tech Insights!\u0026#34;;\rvector_float vector_float_val = {0.100000,0.200000,0.300000,0.400000,0.500000}; The output of the execution is as follows:\n====== Initialize gen hashmap ======\r--- genhashmap values START ---\rint int_val = 1\rstring string_val = \u0026#34;Welcome to Vision Tech Insights!\u0026#34;\rvector_float vector_float_val = {0.100000,0.200000,0.300000,0.400000,0.500000}\r--- genhashmap values END ---\r====== Save gen hashmap to text file ======\r====== Set values in gen hashmap ======\r--- genhashmap values START ---\rint int_val = -1\rstring string_val = \u0026#34;Hello world!\u0026#34;\rvector_float vector_float_val = {-0.100000,-0.200000,-0.300000}\r--- genhashmap values END ---\r====== Get values from gen hashmap ======\r--- get values START ---\rtmp_int = -1\rtmp_float_vec = {-0.1, -0.2, -0.3}\rtmp_string = Hello world!\r--- get values END ---\r====== Load gen hashmap from text file ======\r--- genhashmap values START ---\rint int_val = 1\rstring string_val = \u0026#34;Welcome to Vision Tech Insights!\u0026#34;\rvector_float vector_float_val = {0.100000,0.200000,0.300000,0.400000,0.500000}\r--- genhashmap values END --- Conclusion This post offers an example implementation of a configuration file parser using C++. We\u0026rsquo;ve developed a generic entry class to handle various data types and convert them into strings. Leveraging inheritance and generics in C++ allows us to reuse code blocks for different data types. Storing entries and their corresponding variable names in a hashmap enables us to manage multiple variables simultaneously. With a classic two-pointer-based string processing algorithm, we can save the hashmap to or load it from the configuration file.\nCitation If you found this article helpful, please cite it as:\nZhong, Jian (Apr 2024). Building a Configuration File Parser with C++. Vision Tech Insights. https://jianzhongdev.github.io/VisionTechInsights/posts/building_a_configuration_file_parser_with_cpp/.\nOr\n@article{zhong2024configfileparsercpp, title = \u0026#34;Building a Configuration File Parser with C++\u0026#34;, author = \u0026#34;Zhong, Jian\u0026#34;, journal = \u0026#34;jianzhongdev.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://jianzhongdev.github.io/VisionTechInsights/posts/building_a_configuration_file_parser_with_cpp/.\u0026#34; } References [1] \u0026ldquo;Configuration file.\u0026rdquo; Wikipedia, The Free Encyclopedia. Wikimedia Foundation, Inc. Retrieved April 21, 2024, from https://en.wikipedia.org/wiki/Configuration_file.\n","permalink":"http://localhost:1313/posts/building_a_configuration_file_parser_with_cpp/","summary":"Configuration files are commonly used to adjust settings in computer programs. I\u0026rsquo;m presently developing a configuration file parser for my high-speed data acquisition system using C++. Along the way, I\u0026rsquo;ve discovered some useful techniques involving C++ generics and inheritance that streamline coding. Therefore, I decided to document these tricks in the hope that they\u0026rsquo;ll be beneficial to others. You can find the ready-to-use source code for this configuration file parser in my GitHub repository.","title":"Building a Configuration File Parser with C++"},{"content":"It\u0026rsquo;s really helpful to do some basic math to understand how well an imaging system works when you\u0026rsquo;re designing or improving it. Most of the time, a very basic model of the system can provide a ton of useful information. Therefore, I would like to share an example of how we can crunch some numbers to understand the signal and noise in a two-photon fluorescence microscope (2PFM). In the end, I will also provide a practical demo of how this math can help us make the system work better.\nIntroduction to Two-Photon Microscopy As shown in the cover image of this post, two-photon fluorescence microscopes work by scanning laser beams across a sample and capturing the fluorescent light emitted at each spot. Typically, they use scanning mirrors like galvo mirrors to move the laser beams around. At each scan location, laser beam reflected by the scanning mirrors is relayed to the objective lens using a tube lens and a scan lens. The objective lens then focuses the laser beam into the sample. Afterward, the fluorescence emitted from the sample is gathered by the same objective lens, redirected by a dichroic mirror (which transmits the excitation light while reflects the fluorescence), and finally gathered by some collection optics before being detected by a photodetector (usually a photomultiplier tube (PMT)).\nTwo photon fluorescence microscopes utilize two-photon excitation phenomenon to make fluorophores in samples light up. This happens when the molecules absorb two photons of light at the same time. Because of its low possibility, two photon excitation only occurs in the focal point inside the sample, where the laser beam is most intense (the density of the excitation photons reaches a maximum). To increase the efficiency of two photon excitation and avoid damaging the sample, two photon microscopes send laser pulses with very short pulse width (~femtosecond) into the sample, instead of using a high power continuous-wave laser .\nModeling of the Imaging System summary of the imaging system modeling (image credit: Jian Zhong)\nAt the heart of the two-photon microscope is the two-photon excitation phenomenon. So, when we model these microscopes, we\u0026rsquo;ll follow the sequence of this process: excitation, two-photon absorption, fluorescence generation, and finally fluorescence signal detection. Since each scan location behaves the same way, we\u0026rsquo;ll zoom in on just one spot for our analysis. We\u0026rsquo;ll start by looking at how fluorescence is generated with a single pulse and a single fluorophore, and then we\u0026rsquo;ll generalize our analysis to include multiple laser pulses and all the fluorophores within the focal point.\nTo keep our analysis focused on the most crucial factors, I've used \"\\(\\backsimeq\\)\" notation to show when things are equal by a constant factor, where the constant factor does not depend on the variable explicitly listed on the right-hand-side of the equation. So where wherever we see equation like \\( z \\backsimeq x \\cdot y \\), it means \\( z = const \\cdot (x \\cdot y) \\), where the constant \\(const\\) doesn’t depend on \\(x\\) and \\(y\\). Excitation Process Firstly, let’s model the input laser pulses incident to the sample. For most of the two photon systems we can find out the following details about the laser pulses.\nAverage power ( \\(P_{avg}\\) ): We can measure this by putting a power meter after the objective lens.\nRepetition rate ( \\(f_{PulseRR}\\) ): The laser specs usually tell us this, or we can measure it with an oscilloscope and a fast photo-diode.\nExcitation wavelength ( \\(\\lambda_{ext}\\) ): This is also in the laser specs, or we can measure it with a spectrometer.\nPulse width ( \\(\\tau_{PW}\\) ): We can measure this with an auto-correlator, or estimate it using the laser specs and the dispersion specs of the optics in the system.\nLaser pulses can have different shapes over time, but for our initial estimation, a simple rectangular shape is usually good enough. So, that\u0026rsquo;s what we\u0026rsquo;ll use in our analysis.\nWith these information, we can first calculate the instantaneous energy within a single laser pulse ( \\(E_{Pulse}\\) ) as follows:\n$$ E_{Pulse} = \\frac{P_{avg}}{f_{PulseRR}} $$\n( The trick to understand this formula is not difficult: the total energy during a certain duration \\(\\Delta t\\) is \\(P_{avg} \\cdot \\Delta t\\), and the number of pulses during this dulation is \\(f_{PulseRR} \\cdot \\Delta t\\) so the enery inside each pulse is simply the total energy divided by the number of pulses \\(E_{Pulse} = \\frac{P_{avg} \\cdot \\Delta t}{f_{PulseRR} \\cdot \\Delta t} = \\frac{P_{avg}}{f_{PulseRR}} \\) )\nConsidering that we modeled the laser pulse as a rectangular temporal profile, the instantaneous laser power within the duration of the laser pulse ( \\(P_{ins}\\) ) is as follows:\n$$ P_{ins} = \\frac{E_{Pulse}}{\\tau_{PW}} $$\nFurther analysis of the excitation process requires the knowledge of the excitation focus volume ( \\(V_{focus}\\) ). We can figure this out by measuring the Point Spread Function (PSF). Or, we can estimate it by modeling the excitation focus volume as a cylinder with a certain base area ( \\(A_{focus}\\) ) and height ( \\(d_{focus}\\) ), where the base area and height can be estimated using the lateral and axial resolution of the system respectively.\nFor conventional two photon microscopes, the lateral resolution ( \\(Res_{Lateral}\\) ) is given by:\n$$ Res_{Lateral} = 0.51 \\frac{\\lambda_{ext}}{\\sqrt{2} \\cdot NA} $$\nwhile the axial resolution ( \\(Res_{Axial}\\) ) is given by:\n$$ Res_{Axial} = 0.88 \\frac{\\lambda_{ext}}{\\sqrt{2} (\\ n - \\sqrt{n^2 - NA^2} )} $$\nWhere the \\(NA\\) is the numberical aperture of focused laser beam, \\(n\\) is the refractive index of the immesion medium.\nTypically, laser beam entering the objective lens fills its backpupil, so the \\(NA\\) of the focused laser beam is the same as the \\(NA\\) of the objective lens.\nFor water immersion objetive lenses, \\(n\\) is the refractive index of water ( \\(n = 1.33\\) ). For dry objective lenses, \\(n\\) is the refractive index of air ( \\(n = 1\\) ).\nWith the lateral and axial resolution of the two-photon microscope, the base area ( \\(A_{focus}\\) ) and height ( \\(d_{focus}\\) ) of the focus volume can be extimated as:\n$$ d_{focus} \\backsimeq Res_{Axial} $$\n$$ A_{focus} \\backsimeq \\pi \\cdot ( \\frac{Res_{Lateral}}{2} )^2 $$\nWith our simplied cylinder model for focus volume, focus volume ( \\(V_{focus}\\) ) can be calcualted as:\n$$ V_{focus} \\backsimeq A_{focus} \\cdot d_{focus} \\backsimeq \\pi \\cdot ( \\frac{Res_{Lateral}}{2} )^2 \\cdot Res_{Axial} $$\nThen, the instantaneous intensity of the excitation light during the duration of the laser pulse is as follows:\n$$ I_{ins} \\backsimeq \\frac{P_{ins}}{A_{focus}} = \\frac{E_{Pulse}}{A_{focus} \\cdot \\tau_{PW}} = \\frac{P_{avg}}{f_{PulseRR} \\cdot A_{focus} \\cdot \\tau_{PW}} $$\nTwo Photon Absorption and Fluorescence Generation Next, we\u0026rsquo;ll delve into modeling the two-photon absorption and fluorescence generation within the excitation volume. Typically, this volume contains numerous fluorophores. Initially, we\u0026rsquo;ll concentrate on describing how a single fluorophore interacts with a single laser pulse, then extend our findings to encompass all the fluorophore within the entire excitation volume interacting with a single laser pulse.\nDuring the two-photon absorption process, a fluorophore in its ground state absorbs two excitation photons and transitions to an excited state with a certain probability. This process is probabilistic, and its efficiency can be quantified using the two-photon transition rate ( \\(\\omega_{Transition}\\) ). This rate essentially tells us how many two-photon absorption events occur per unit time. In quantum mechanics, the two-photon transition rate is defined as follows:\n$$ \\omega_{Transition} = \\sigma_{2PCross} \\cdot ( I_{ins} )^2 \\backsimeq \\sigma_{2PCross} \\cdot (\\frac{P_{avg}}{f_{PulseRR} \\cdot A_{focus} \\cdot \\tau_{PW}})^2 $$\nHere, \\(\\sigma_{2PCross}\\) is the cross section of two photon absorption, which depends on the properties of the fluorophore.\nTwo photon absorption will only happen during the duration of the laser pulse, (during which the fluorophore encounters the excitation photons). With the transition rate, the following formula shows how many times this happens to one molecule during the entire laser pulse.\n$$ N_{Absorp} = \\omega_{Transition} \\cdot \\tau_{PW} $$ $$ \\backsimeq \\sigma_{2PCross} \\cdot (\\frac{P_{avg}}{f_{PulseRR} \\cdot A_{focus} \\cdot \\tau_{PW}})^2 \\cdot \\tau_{PW} $$ $$ = \\sigma_{2PCross} \\cdot P_{avg}^2 ( \\frac{1}{f_{PulseRR}} )^2 ( \\frac{1}{A_{focus}} )^2 \\frac{1}{\\tau_{PW}} $$\nOnce the fluorophore gets excited, it eventually returns to its original state and releases a fluorescence photon. This process happens with a certain probability. We use \\(\\eta_{Q}\\) to represent how efficiently this transition occurs from the excited state to emitting a fluorescence photon. It\u0026rsquo;s basically the conversion rate from absorbing two photons to emitting one fluorescence photon. The formula below tells us how many photons a single fluorophore emits during one laser pulse of excitation ( \\(N_{FP}\\) ).\n$$ N_{FP} = \\eta_{Q} \\cdot N_{Absorp} $$\n$$ \\backsimeq \\eta_{Q} \\cdot \\sigma_{2PCross} \\cdot P_{avg}^2 ( \\frac{1}{f_{PulseRR}} )^2 ( \\frac{1}{A_{focus}} )^2 \\frac{1}{\\tau_{PW}} $$\nNow, we finished the analysis of a single fluorophore interacting with a single laser pulse. Usually, the excitation volume contains more than one fluorophore. If we denote the density of the fluorophores as \\(\\rho_{fluor}\\) , then the total number of fluorophores within the excitation volume ( \\(N_{fluor}\\) ) can be expressed as:\n$$ N_{fluor} = \\rho_{fluor} \\cdot V_{focus} $$\nUsing the cylindrical model to represent the focused excitation volume, we can express the total number of fluorophores within that volume as follows:\n$$ N_{fluor} = \\rho_{fluor} \\cdot V_{focus} \\backsimeq \\rho_{fluor} \\cdot A_{focus} \\cdot d_{focus} $$\nAfter a single laser pulse hits the sample, the total number of fluorescence photons emitted from the focus volume ( \\(N_{totFP}\\) ) is just the result of multiplying the total number of fluorophores (within the focus volume) by the number of fluorescence photons emitted by each fluorophore.\n$$ N_{totFP} = N_{fluor} \\cdot N_{FP} $$ $$ \\backsimeq ( \\rho_{fluor} \\cdot A_{focus} \\cdot d_{focus} ) \\cdot ( \\eta_{Q} \\cdot \\sigma_{2PCross} \\cdot P_{avg}^2 ( \\frac{1}{f_{PulseRR}} )^2 ( \\frac{1}{A_{focus}} )^2 \\frac{1}{\\tau_{PW}} ) $$ $$ = \\rho_{fluor} \\cdot \\eta_{Q} \\cdot \\sigma_{2PCross} \\cdot P_{avg}^2 ( \\frac{1}{f_{PulseRR}} )^2 ( \\frac{1}{\\tau_{PW}} ) ( \\frac{1}{A_{focus}} ) \\cdot d_{focus} $$\nFluorescence Signal Detection The fluorescence photons will be collected by the objective lens, travel through the detection light path, and be detected by the photon detector. The photon detector converts them into a specific type of signal, usually electrical, which is easy to record and save. While the process of collecting them can be complex, we can simplify it by describing the efficiency of converting fluorescence photons to signals using a factor ( \\(\\eta_{collect}\\) ). This factor encompasses various elements like the numerical aperture of the objective lens, the transmission of the detection optics, and the conversion efficiency of the detectors. The signal is usually measured by the number of photons recorded, which we\u0026rsquo;ll call \u0026ldquo;signal photons\u0026rdquo; here. So, the total count of signal photons ( \\(N_{Signal}\\) ) generated by one laser pulse can be expressed as:\n$$ N_{Signal} = \\eta_{collect} \\cdot N_{totFP} $$\n$$ \\backsimeq \\eta_{collect} \\cdot \\rho_{fluor} \\cdot \\eta_{Q} \\cdot \\sigma_{2PCross} \\cdot P_{avg}^2 ( \\frac{1}{f_{PulseRR}} )^2 ( \\frac{1}{\\tau_{PW}} ) ( \\frac{1}{A_{focus}} ) \\cdot d_{focus} $$\nIntegration of Multiple Pulses and Signal-to-Noise Ratio Typically, in two-photon microscopes, the microscope stays at each pixel spot (or, scan location) for a specific amount of time (which is usually called dwell time). During this time, multiple laser pulses hit the sample, and all the fluorescence generated by these pulses are collected. So, the overall signal for each pixel spot (or, scan location) can be calculated like this:\n$$ N_{totSignal} = N_{pulse} \\cdot N_{Signal} $$ $$ \\backsimeq N_{pulse} \\cdot \\eta_{collect} \\cdot \\rho_{fluor} \\cdot \\eta_{Q} \\cdot \\sigma_{2PCross} \\cdot P_{avg}^2 ( \\frac{1}{f_{PulseRR}} )^2 ( \\frac{1}{\\tau_{PW}} ) ( \\frac{1}{A_{focus}} ) \\cdot d_{focus} $$\nWhere \\(N_{Pulse}\\) is the number of laser pulses that hit the sample at each scan location.\nThe distribution of signal photons usually follows a Poisson distribution. If we suppose that all other sources of noise in the system, like electrical noise, ambient light leaking in, or fluctuations in the laser pulse energy, are negligible, the signal to noise ratio (SNR) of each scan location can be written as:\n$$ SNR = \\sqrt{N_{totSignal}} $$\nSide Notes Usually, the specific characteristics of the fluorophores ( \\(\\rho_{fluor}\\) , \\(\\eta_{Q}\\) , and \\(\\sigma_{2PCross}\\) ) are difficult to be quantified. However, we can compare the signals from two fluorophores under the same conditions. By doing this, we can determine how much brighter one fluorophore is compared to another. Taking in the formula we derived above, for fluorophores A and B, if we image them with the same two photon microscope and exactly the same condition, we can have the following comparison result:\n$$ \\frac{ \\rho_{fluor_A} \\cdot \\eta_{Q_A} \\cdot \\sigma_{2PCross_A} }{ \\rho_{fluor_B} \\cdot \\eta_{Q_B} \\cdot \\sigma_{2PCross_B} } = \\frac{N_{totSignal_A}}{N_{totSignal_B}} $$\nExample Application As a practice, let’s explore how to double the signal-to-noise ratio (SNR) of a two photon microscope.\nTo improve the signal-to-noise ratio (SNR) of a two-photon imaging system by two times, we need to boost the signal by four times, according to the formula mentioned earlier. Here are some potential strategies we can consider:\nIncrease dwell time at each pixel by four times. This means letting the microscope linger longer at each pixel to gather more signal from the incident pulses. However, this might slow down the overall imaging process or reduce the number of pixels in the image, which could affect certain applications.\nCompress the pulse duration by four times. Adding a module for pulse compression or dispersion compensation optics in the system can achieve this. It helps in packing more energy into each pulse, potentially enhancing the signal.\nDouble the average power. While increasing power can boost signal strength, it must be done cautiously to avoid damaging the sample.\nEnhance detection efficiency by four times. This involves optimizing both the optical and electronic designs of the detection system to capture more of the fluorescence signal.\nThese methods aren\u0026rsquo;t mutually exclusive; we can combine several approaches to achieve the desired increase in SNR. However, it\u0026rsquo;s crucial to carefully evaluate the impact of each strategy on the overall system performance and the specific requirements of the application at hand.\nConclusion Understanding the imaging system in numbers is key for its design and optimization. The simplest math models and assumptions are a good start for our analysis, and usually reveal a lot about the system. Also, keep in mind the assumptions and simplifications we made during modeling. This way, we can easily come back to validate and refine your model later if needed.\nCitation If you found this article helpful, please cite it as:\nZhong, Jian (Apr 2024). Basic Math for Two Photon Fluorescence Microscopy. Vision Tech Insights. https://jianzhongdev.github.io/VisionTechInsights/posts/basic_math_for_two_photon_fluorescence_microscopy/.\nOr\n@article{zhong2024basicmath2PFM, title = \u0026#34;Basic Math for Two Photon Fluorescence Microscopy\u0026#34;, author = \u0026#34;Zhong, Jian\u0026#34;, journal = \u0026#34;jianzhongdev.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://jianzhongdev.github.io/VisionTechInsights/posts/basic_math_for_two_photon_fluorescence_microscopy/\u0026#34; } ","permalink":"http://localhost:1313/posts/basic_math_for_two_photon_fluorescence_microscopy/","summary":"It\u0026rsquo;s really helpful to do some basic math to understand how well an imaging system works when you\u0026rsquo;re designing or improving it. Most of the time, a very basic model of the system can provide a ton of useful information. Therefore, I would like to share an example of how we can crunch some numbers to understand the signal and noise in a two-photon fluorescence microscope (2PFM). In the end, I will also provide a practical demo of how this math can help us make the system work better.","title":"Basic Math for Two Photon Fluorescence Microscopy"}]