<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building and Training VGG with PyTorch: A Step-by-Step Guide | Vision Tech Insights</title>
<meta name=keywords content="computer vision,machine learning"><meta name=description content="A comprehensive guide on building and training VGG with PyTorch."><meta name=author content="Jian Zhong"><link rel=canonical href=http://localhost:1313/posts/implement_train_vgg_pytorch/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/implement_train_vgg_pytorch/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Building and Training VGG with PyTorch: A Step-by-Step Guide"><meta property="og:description" content="A comprehensive guide on building and training VGG with PyTorch."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/implement_train_vgg_pytorch/"><meta property="og:image" content="http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-13T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png"><meta name=twitter:title content="Building and Training VGG with PyTorch: A Step-by-Step Guide"><meta name=twitter:description content="A comprehensive guide on building and training VGG with PyTorch."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Building and Training VGG with PyTorch: A Step-by-Step Guide","item":"http://localhost:1313/posts/implement_train_vgg_pytorch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building and Training VGG with PyTorch: A Step-by-Step Guide","name":"Building and Training VGG with PyTorch: A Step-by-Step Guide","description":"A comprehensive guide on building and training VGG with PyTorch.","keywords":["computer vision","machine learning"],"articleBody":"The VGG (Visual Geometry Group) model is a type of convolutional neural network (CNN) outlined in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition. It’s known for its use of small convolution filters and deep layers, which helped it achieve top-notch performance in tasks like image classification. By stacking multiple layers with small kernel sizes, VGG can capture a wide range of features from input images. Plus, adding more rectification layers makes its decision-making process sharper and more accurate. The paper also introduced 1x1 convolutional layers to enhance nonlinearity without affecting the receptive view. For training, VGG follows the traditional supervised learning approach where input images and ground truth labels are provided.\nVGG’s architecture has significantly shaped the field of neural networks, serving as a foundation and benchmark for many subsequent models in computer vision.\nIn this blog post, we’ll guide you through implementing and training the VGG architecture using PyTorch, step by step. You can find the complete code for defining and training the VGG model on my GitHub repository (URL: https://github.com/JianZhongDev/VGGPyTorch).\nVGG architecture and implementation As you can see in the cover image of this post, the VGG model is made up of multiple layers of convolution followed by max-pooling, and it ends with a few fully connected layers. he output from these layers is then fed into a softmax layer to give a normalized confidence score for each image category.\nThe key features of the VGG network are these stacked convolutional layers and fully connected layers. We will start with these stacked layers in our implementation.\nStacked convolutional layers To start, we’ll create the stacked convolutional layer as PyTorch nn.Module, like this:\nclick to expand stacked convolutional layer code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # stacked 2D convolutional layer class VGGStacked2DConv(nn.Module): def __init__( self, layer_descriptors = [], ): assert(isinstance(layer_descriptors, list)) super().__init__() self.network = nn.Identity() # create list of stacked layers stacked_layers = [] # iterater through each descriptor for the layers and create corresponding layers prev_out_channels = 1 for i_descrip in range(len(layer_descriptors)): cur_descriptor = layer_descriptors[i_descrip] # the descriptor needs to be dict if not isinstance(cur_descriptor, dict): continue # get input or default values nof_layers = cur_descriptor.get(\"nof_layers\", 1) in_channels = cur_descriptor.get(\"in_channels\", prev_out_channels) out_channels = cur_descriptor.get(\"out_channels\", 1) kernel_size = cur_descriptor.get(\"kernel_size\", 3) stride = cur_descriptor.get(\"stride\", 1) padding = cur_descriptor.get(\"padding\", 1) bias = cur_descriptor.get(\"bias\", True) padding_mode = cur_descriptor.get(\"padding_mode\", \"zeros\") activation = cur_descriptor.get(\"activation\", nn.ReLU) # create layers cur_in_channels = in_channels for _ in range(nof_layers): stacked_layers.append( nn.Conv2d( in_channels = cur_in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding, bias = bias, padding_mode = padding_mode, ) ) stacked_layers.append( activation() ) cur_in_channels = out_channels prev_out_channels = out_channels # convert list of layers to sequential layers if len(stacked_layers) \u003e 0: self.network = nn.Sequential(*stacked_layers) def forward(self, x): y = self.network(x) return y The stacked convolutional layer takes in a list of descriptor dictionaries, each detailing the setup for a repeated convolutional layer followed by an activation. It reads these configurations and builds the stacked convolutional layers accordingly. If certain configuration parameters are not specified, the code fills in default values.\nStacked fully-connected and dropout layers VGG uses dropout regularizations in their fully connected layers. Adding the dropout regularization within PyTorch is straightforward: we just need to insert dropout layers after each hidden layer inside the stacked fully connected layer. (NOTE: Section 4.2 of the AlexNet paper provides valuable insights into dropout layers. It’s definitely worth a read.)\nWe can define the stacked fully connected layer in a similar manner as the stacked convolutional layers:\nclick to expand stacked fully-connected layer code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # stacked linear layers class VGGStackedLinear(nn.Module): def __init__( self, layer_descriptors = [], ): assert(isinstance(layer_descriptors, list)) super().__init__() self.network = nn.Identity() # create list of stacked layers stacked_layers = [] # iterater through each descriptor for the layers and create corresponding layers prev_out_features = 1 for i_descrip in range(len(layer_descriptors)): cur_descriptor = layer_descriptors[i_descrip] # the descriptor needs to be dict if not isinstance(cur_descriptor, dict): continue nof_layers = cur_descriptor.get(\"nof_layers\", 1) in_features = cur_descriptor.get(\"in_features\", prev_out_features) out_features = cur_descriptor.get(\"out_features\", 1) bias = cur_descriptor.get(\"bias\", True) activation = cur_descriptor.get(\"activation\", nn.ReLU) dropout_p = cur_descriptor.get(\"dropout_p\", None) # create layers cur_in_features = in_features for _ in range(nof_layers): stacked_layers.append( nn.Linear( in_features = cur_in_features, out_features = out_features, bias = bias, ) ) if activation is not None: stacked_layers.append( activation() ) if dropout_p is not None: stacked_layers.append( nn.Dropout(p = dropout_p) ) cur_in_features = out_features prev_out_features = out_features # convert list of layers to sequential layers if len(stacked_layers) \u003e 0: self.network = nn.Sequential(*stacked_layers) def forward(self, x): y = self.network(x) return y VGG model Now that we’ve defined the stacked convolutional and fully-connected layers, we can construct the VGG model as follows:\nclick to expand VGG model code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # VGG model definition class VGG(nn.Module): def __init__( self, stacked_conv_descriptors, stacked_linear_descriptor, ): assert(isinstance(stacked_conv_descriptors, list)) assert(isinstance(stacked_linear_descriptor, list)) super().__init__() self.network = nn.Identity() stacked_layers = [] # add stacked convolutional layers and max pooling layers for i_stackconv_descrip in range(len(stacked_conv_descriptors)): cur_stacked_conv_descriptor = stacked_conv_descriptors[i_stackconv_descrip] if not isinstance(cur_stacked_conv_descriptor, list): continue stacked_layers.append( StackedLayers.VGGStacked2DConv( cur_stacked_conv_descriptor ) ) # add max pooling layer after stacked convolutional layer stacked_layers.append( nn.MaxPool2d( kernel_size = 2, stride = 2, ) ) # flatten convolutional layers stacked_layers.append( nn.Flatten() ) # add stacked linear layers stacked_layers.append( StackedLayers.VGGStackedLinear( stacked_linear_descriptor ) ) # add softmax layer at the very end stacked_layers.append( nn.Softmax(dim = -1) ) # convert list of layers to Sequantial network if len(stacked_layers) \u003e 0: self.network = nn.Sequential(*stacked_layers) def forward(self, x): y = self.network(x) return y The VGG model takes in a stacked convolutional layer descriptor list, and a fully connected layer descriptor. First, it goes through the convolutional layer descriptors, creating stacked convolutional layers for each descriptor and adding a max pooling layer after each set of stacked convolutional layers. Then, it flattens the output from all the convolutional layers and constructs stacked fully connected layers based on the linear layer descriptor. Finally, a Softmax layer is appended at the end of the network.\nModel generation Using the model definition provided above, we can create a VGG model by specifying a few layer descriptors. For instance, we can replicate the VGG16 model described in the VGG paper as follows:\nclick to expand demo creating 16-layer VGG model\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## Demo creating 16-layer VGG model input_image_width = 224 input_image_height = 224 model_stacked_conv_list = [ [ {\"nof_layers\": 2, \"in_channels\": 3, \"out_channels\": 64,}, ], [ {\"nof_layers\": 2, \"in_ckjhannels\": 64, \"out_channels\": 128,}, ], [ {\"nof_layers\": 2, \"in_channels\": 128, \"out_channels\": 256, }, {\"nof_layers\": 1, \"out_channels\": 256, \"kernel_size\": 1, \"padding\": 0}, ], [ {\"nof_layers\": 2, \"in_ckjhannels\": 256, \"out_channels\": 512,}, {\"nof_layers\": 1, \"out_channels\": 512, \"kernel_size\": 1, \"padding\": 0}, ], [ {\"nof_layers\": 2, \"in_ckjhannels\": 512, \"out_channels\": 512,}, {\"nof_layers\": 1, \"out_channels\": 512, \"kernel_size\": 1, \"padding\": 0}, ], ] conv_image_reduce_ratio = 2**len(model_stacked_conv_list) conv_final_image_width = input_image_width//conv_image_reduce_ratio conv_final_image_height = input_image_height//conv_image_reduce_ratio model_stacked_linear = [ { \"nof_layers\": 2, \"in_features\": conv_final_image_width * conv_final_image_height * 512, \"out_features\": 4096, \"dropout_p\": 0.5 }, { \"nof_layers\": 1, \"out_features\": 1000, \"activation\": None } ] model = VGG.VGG( stacked_conv_descriptors = model_stacked_conv_list, stacked_linear_descriptor = model_stacked_linear, enable_debug = False, ) print(model) Here’s what the printout of the VGG16 model looks like:\nclick to expand 16-layer VGG model printout\rVGG( (network): Sequential( (0): VGGStacked2DConv( (network): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() ) ) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() ) ) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (4): VGGStacked2DConv( (network): Sequential( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) ) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) ) (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (8): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU() (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) ) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Flatten(start_dim=1, end_dim=-1) (11): VGGStackedLinear( (network): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU() (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU() (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) ) ) (12): Softmax(dim=-1) ) ) Data processing In the VGG paper, the only data processing done on the input data is subtracting the RGB value calculated from the training set. To apply this processing, we start by going through the entire training dataset and computing the mean value for each color channel.\nclick to expand channel mean value calculation code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## calculated the averaged channel values across the entire data set # train_dataloader is the dataloader iterating through the entire training data set input_dataloader = train_dataloader nof_batchs = len(input_dataloader) avg_ch_vals = [None for _ in range(nof_batchs)] for i_batch, data in enumerate(input_dataloader): inputs, labels = data cur_avg_ch = torch.mean(inputs, dim = (-1,-2), keepdim = True) avg_ch_vals[i_batch] = cur_avg_ch avg_ch_vals = torch.cat(avg_ch_vals, dim = 0) avg_ch_val = torch.mean(avg_ch_vals, dim = 0, keepdim = False) print(\"result size = \") print(avg_ch_val.size()) print(\"result val = \") print(repr(avg_ch_val)) Using the mean channel value, we can perform the mentioned data processing by defining a background subtraction function and using the Lambda() transform provided by torchvision like this:\nclick to expand mean channel value subtraction code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 import functools from torchvision.transforms import v2 # subtract constant value from the image def subtract_const(src_image, const_val): return src_image - const_val ## subtract global mean channel background train_data_ch_avg = torch.tensor([[[0.4914]],[[0.4822]],[[0.4465]]]) # NOTE: train_data_ch_avg is obtained from the channel mean value calculation code print(train_data_ch_avg.size()) subtract_ch_avg = functools.partial(subtract_const, const_val = train_data_ch_avg) subtract_channel_mean_transform = v2.Lambda(subtract_ch_avg) Data Augmentation The VGG paper also employed various data augmentation techniques to prevent overfitting. Here’s how we implement them:\nRandom horizontal flip torchvision already includes a built-in transformation for randomly flipping images horizontally. Therefore, we can simply utilize this built-in transformation for horizontal flips.\nclick to expand random horizontal flip code\r1 2 3 from torchvision.transforms import v2 rand_hflip_transform = v2.RandomHorizontalFlip(0.5) In the VGG paper, they utilized both the original image and its horizontally flipped counterpart to predict classification results. They then averaged these results to obtain the final classification. Consequently, we can implement the validation process as follows:\nclick to expand validation code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 # validate model in one epoch and return the top k-th result def validate_one_epoch_topk_aug( model, validate_loader, loss_func, transforms, device, top_k = 1 ): tot_loss = 0.0 avg_loss = 0.0 tot_nof_batch = len(validate_loader) correct_samples = 0 tot_samples = len(validate_loader.dataset) nof_transforms = len(transforms) model.eval() with torch.no_grad(): for i_batch, data in enumerate(validate_loader): inputs, labels = data inputs = inputs.to(device) labels = labels.to(device) group_outputs = [None for _ in range(nof_transforms)] group_loss = [None for _ in range(nof_transforms)] for i_trans in range(nof_transforms): cur_transform = transforms[i_trans] cur_input = inputs if cur_transform is not None: cur_input = cur_transform(inputs) cur_output = model(cur_input) cur_loss = loss_func(cur_output, labels) group_outputs[i_trans] = cur_output group_loss[i_trans] = cur_loss outputs = torch.mean(torch.stack(group_outputs, dim = 0), dim = 0) loss = torch.mean(torch.stack(group_loss, dim = 0), dim = 0) tot_loss += loss.item() # NOTE: we will define batch_in_top_k() later # NOTE: batch_in_top_k() return a mask array indicate if label in the top k result correct_samples += (batch_in_top_k(outputs, labels, top_k)).type(torch.float).sum().item() avg_loss = tot_loss/tot_nof_batch correct_rate = correct_samples/tot_samples print(f\"Validate: top{top_k} Accuracy: {(100*correct_rate):\u003e0.2f}%, Avg loss: {avg_loss:\u003e8f}\") return (avg_loss, correct_rate) ## demo training validation loop validate_transforms = [None, torchvision.transforms.functional.hflip] for i_epoch in range(nof_epochs): print(f\" ------ Epoch {i_epoch} ------ \") train_one_epoch(model, train_dataloader, loss_func, optimizer, device) validate_one_epoch_topk_aug(model, validate_dataloader, loss_func, validate_transforms, device, top_k) Random color shift In VGG, another augmentation technique involved adjusting the RGB values of training images by a random combination of the principal component analysis (PCA) eigenvectors derived from the RGB values across all pixels of all images in the training set. For a detailed explanation, refer to section 4.1 Data Augmentation in the AlexNet paper.\nHere’s how the random color shift is implemented:\nBefore training begins, we go through the entire training set and gather all RGB values from each image. This data is used to create an \\(m \\times n\\) data matrix, where \\(n\\) represents the number of channels (3 for RGB images) and \\(m\\) represents the total number of pixels across all images in the training set \\(m = \\text{number of images} \\times \\text{image height} \\times \\text{image width}\\). We then calculate the covariance matrix of this data matrix. Next, we conduct principal component analysis (PCA) on the covariance matrix using singular value decomposition (SVD). The resulting \\(U\\) matrix contains columns representing the PCA eigenvectors, and the \\(S\\) matrix contains the corresponding eigenvalues. click to expand dataset channel PCA code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ## PCA for covariance matrix of image channels across all the pixels # train_dataloader is the dataloader iterating through the entire training data set input_dataloader = train_dataloader nof_batchs = len(input_dataloader) ch_vecs = [None for _ in range(nof_batchs)] for i_batch, data in enumerate(input_dataloader): inputs, labels = data # swap channel and batch axis and flatten the dimension of (batch, image height, image width) ch_vecs[i_batch] = torch.flatten(torch.swapaxes(inputs, 0, 1), start_dim = 1, end_dim = -1) ch_vecs = torch.cat(ch_vecs, dim = -1) ch_cov = torch.cov(ch_vecs) ch_vecs = None U, S, Vh = torch.linalg.svd(ch_cov, full_matrices = True) ## Each column of U is a channel PCA eigenvector ## S contains the corresponding to eigenvectors print(\"U:\") print(repr(U)) print(\"S:\") print(S) print(\"Vh:\") print(Vh) Note: In this implementation, all pixels are loaded into computer memory at the same time. For larger datasets, the code for calculating the covariance matrix may need enhancements to compute it without simultaneously loading all data into memory.\nDuring training, we create a randomized linear combination of PCA eigenvectors by adding up the product of each eigenvector with a randomized amplitude. This amplitude is computed by multiplying the corresponding eigenvalue by a random value drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 0.1.\nclick to expand random color shift code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import functools from torchvision.transforms import v2 # image channel radom PCA eigenvec addition agumentation def random_ch_shift_pca(src_image, pca_eigenvecs, pca_eigenvals, random_paras = None): norm_meam = 0 norm_std = 0.1 if isinstance(random_paras, dict): norm_meam = random_paras.get(\"mean\", norm_meam) norm_std = random_paras.get(\"std\", norm_std) nof_dims = len(src_image.size()) nof_channels = src_image.size(0) assert(pca_eigenvecs.size(0) == nof_channels) assert(len(pca_eigenvals.size()) == 1) assert(pca_eigenvals.size(0) == pca_eigenvecs.size(1)) norm_means = 0 * torch.ones(pca_eigenvals.size()) norm_stds = 0.1 * torch.ones(pca_eigenvals.size()) alphas = torch.normal(norm_means, norm_stds) scale_factors = (alphas * pca_eigenvals).view((-1,1)) ch_offset = torch.matmul(pca_eigenvecs, scale_factors) ch_offset = ch_offset.view((nof_channels,) + (1,) * (nof_dims - 1)) dst_image = src_image + ch_offset return dst_image ## random channel shifts # NOTE: trainset_pca_eigenvecs and trainset_pca_eigenvals are the U and S matrix obtained from above mentioned PCA analysis trainset_pca_eigenvecs = torch.tensor([[-0.5580, 0.7063, 0.4356], [-0.5775, 0.0464, -0.8151], [-0.5960, -0.7063, 0.3820]]) print(trainset_pca_eigenvecs.size()) trainset_pca_eigenvals = torch.tensor([0.1719, 0.0139, 0.0029]) print(trainset_pca_eigenvals.size()) random_ch_shift = functools.partial(random_ch_shift_pca, pca_eigenvecs = trainset_pca_eigenvecs, pca_eigenvals = trainset_pca_eigenvals, random_paras = {\"mean\": 0, \"std\": 0.1}, ) random_ch_shift_transform = v2.Lambda(random_ch_shift) Other data augmentations The VGG paper also employed additional augmentation techniques like random translations and random crops. However, since the CIFAR dataset’s image size is much smaller (32x32) compared to the ImageNet dataset (256x256), there isn’t much flexibility to utilize these techniques effectively.\nTraining and validation Top k accuracy (or error) In the VGG paper, the main way they measured performance was using the top k error. In my version, I focused on calculating the top k accuracy instead. Top k accuracy shows how often the actual label is among the top k predictions made by the model with the highest confidence. On the other hand, top k error tells us how often the actual label is not included in the top k predictions.\nThe relationship between top k error and top k accuracy is simply connected by the following formula:\n$$ \\text{top k error} = 1 − \\text{top k accuracy} $$\nA higher top k accuracy and lower top k error indicate better model performance.\nDuring the validation (or test) process, the top k-th accuracy can be estimated by dividing the total number of valiation (or test) samples by the number of samples where the label is in the top k predictions.\n$$ \\text{top k accuracy} = \\frac{\\text{number of samples (label is in top k predictions)}}{\\text{total number of samples}} $$\nTherefore, top k accuracy can be calculated using the following code:\nclick to expand top k accuarcy calculation code\r1 2 3 4 5 6 7 # Evaluate if label is within top k prediction result for one batch of data def batch_in_top_k(outputs, labels, top_k = 1): sorted_outputs, sorted_idxs = torch.sort(outputs, dim = -1, descending = True) in_top_k = torch.full_like(labels, False) for cur_idx in range(top_k): in_top_k = torch.logical_or(sorted_idxs[:,cur_idx] == labels, in_top_k) return in_top_k In each batch, we organize the softmax layer results, which represent the confidences for each predicted category, in descending order. Then, we check if the ground truth label is among the top k predictions. This check result is stored in a boolean mask array, where ’true’ indicates the label is in the top k predictions, and ‘false’ indicates it’s not. This boolean mask array holds the results for all samples within the batch. To find the total number of samples where the label is among the top k predictions, we simply sum the mask arrays from all batches.\nLoss function, regularization, and optimizer VGG employs multinomial logistic regression as its loss function. For optimization, it utilizes mini-batch gradient descent with momentum and weight decay. In PyTorch, these can be implemented as follows:\nclick to expand loss function and optimizer code\r1 2 loss_func = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr = 1E-2, momentum = 0.9, weight_decay= 5E-4) Additionally, dropout regularization has been incorporated into the model as another form of regularization as mentioned earlier in this post.\nLearning rate adjustment In the VGG paper, the authors initially train with a learning rate of 1E-2. Then, they reduce the learning rate by a factor of 10 when the validation set accuracy plateaus. This can be implemented using the ReduceLROnPlateau() function provided by PyTorch, like this:\nclick to expand learing rate adjustment code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer = optimizer, mode = \"max\", factor = 0.1, patience = 10, threshold = 1E-3, min_lr = 0, ) # Demo training and validation loop for i_epoch in range(nof_epochs): print(f\" ------ Epoch {i_epoch} ------ \") cur_lr = optimizer.param_groups[0]['lr']; print(f\"current lr = {cur_lr}\") cur_train_loss = train_one_epoch(model, train_dataloader, loss_func, optimizer, device) cur_validate_loss, cur_validate_accuracy = validate_one_epoch_topk_aug(model, validate_dataloader, loss_func, validate_transforms, device, top_k) scheduler.step(cur_validate_accuracy) print(\"\\n\") NOTE: The description of the ReduceLROnPlateau() function in the PyTorch documentation can be confusing. I found that reading the source code of the ReduceLROnPlateau() definition provides clearer understanding.\nTraining deep models Optimizing deep models from scratch with completely random initialization can be very challenging for the optimizer. It often leads to the learning process getting stuck for long periods.\nTo tackle this issue, the VGG authors first train a shallow model. Then, they use the learned parameters from this shallow model to initialize deeper ones.\nTransferring learned parameters between models in PyTorch is straightforward. It involves copying the weights and biases from corresponding layers between the two models. If you’re using the VGG model definition from this blog post, the example code looks like this:\nclick to expand model parameter transfer code\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## demo transfer model parameters input_image_width = 32 input_image_height = 32 # create model 1 model1_stacked_conv_list = [ [ {\"nof_layers\": 1, \"in_channels\": 3, \"out_channels\": 64,}, ], [ {\"nof_layers\": 1, \"in_ckjhannels\": 64, \"out_channels\": 128,}, ], ] conv_image_reduce_ratio = 2**len(model1_stacked_conv_list) conv_final_image_width = input_image_width//conv_image_reduce_ratio conv_final_image_height = input_image_height//conv_image_reduce_ratio model1_stacked_linear = [ { \"nof_layers\": 1, \"in_features\": conv_final_image_width * conv_final_image_height * 512, \"out_features\": 512, \"dropout_p\": 0.5 }, { \"nof_layers\": 1, \"out_features\": 10, \"activation\": None } ] model1 = VGG.VGG( stacked_conv_descriptors = model1_stacked_conv_list, stacked_linear_descriptor = model1_stacked_linear, enable_debug = False, ) # create model 2 model2_stacked_conv_list = [ [ {\"nof_layers\": 1, \"in_channels\": 3, \"out_channels\": 64,}, ], [ {\"nof_layers\": 2, \"in_ckjhannels\": 64, \"out_channels\": 128,}, ], ] conv_image_reduce_ratio = 2**len(model2_stacked_conv_list) conv_final_image_width = input_image_width//conv_image_reduce_ratio conv_final_image_height = input_image_height//conv_image_reduce_ratio model2_stacked_linear = [ { \"nof_layers\": 1, \"in_features\": conv_final_image_width * conv_final_image_height * 512, \"out_features\": 512, \"dropout_p\": 0.5 }, { \"nof_layers\": 1, \"out_features\": 10, \"activation\": None } ] model2 = VGG.VGG( stacked_conv_descriptors = model2_stacked_conv_list, stacked_linear_descriptor = model2_stacked_linear, enable_debug = False, ) # transfer parameter of 1st convoluation layer from model 1 to model 2 model2.network[0].network[0].weight = model1.network[0].network[0].weight model2.network[0].network[0].bias = model1.network[0].network[0].bias NOTE: We’ve organized the sequential layers of the VGG model and stacked them within the “network” attribute of the object. This means we can access each specific layer inside the network by indexing the “network” attribute.\nResults Given the large size of the ImageNet dataset and the extensive time required for training, we’ll opt for a smaller dataset, CIFAR10, to demonstrate training and validation more quickly.\nI’ve examined several models based on the VGG architecture, and I’ve listed some of them (model I, II, and III) below:\nModel Configuration I II III conv3-128 conv3-128 conv3-128 maxpool conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 maxpool conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 maxpool FC-1024 FC-1024 FC-1024 FC-1024 FC-1024 FC-10 FC-10 FC-10 soft-max After training these model variations, I computed the top 1 to top 5 accuracies using the CIFAR10 test dataset. Here’s a summary of the results:\nModel config. top-1 accuarcy(%) top-2 accuarcy(%) top-3 accuarcy(%) top-4 accuarcy(%) top-5 accuarcy(%) I 82.45 92.74 96.23 97.82 98.87 II 84.88 93.95 96.91 98.23 98.99 III 86.93 94.39 96.83 98.15 98.90 We can observe that the accuracy tends to improve as the depth of the models increases.\nReference [1] Simonyan, K., \u0026 Zisserman, A. (2014). Very deep convolutional networks for Large-Scale image recognition. arXiv (Cornell University). https://doi.org/10.48550/arxiv.1409.1556\n[2] Krizhevsky, A., Sutskever, I., \u0026 Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems, 25, 1097–1105. http://books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf\n[3] Krizhevsky, A., Nair, V. and Hinton, G. (2014) The CIFAR-10 Dataset. https://www.cs.toronto.edu/~kriz/cifar.html\nCitation If you found this article helpful, please cite it as:\nZhong, Jian (May 2024). Building and Training VGG with PyTorch: A Step-by-Step Guide. Vision Tech Insights. https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_VGG_PyTorch/.\nOr\n@article{zhong2024buildtrainVGGPyTorch, title = \"Building and Training VGG with PyTorch: A Step-by-Step Guide\", author = \"Zhong, Jian\", journal = \"jianzhongdev.github.io\", year = \"2024\", month = \"May\", url = \"https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_VGG_PyTorch/\" } ","wordCount":"4162","inLanguage":"en","image":"http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png","datePublished":"2024-05-13T00:00:00Z","dateModified":"2024-05-13T00:00:00Z","author":{"@type":"Person","name":"Jian Zhong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/implement_train_vgg_pytorch/"},"publisher":{"@type":"Organization","name":"Vision Tech Insights","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Vision Tech Insights (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Vision Tech Insights</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building and Training VGG with PyTorch: A Step-by-Step Guide</h1><div class=post-description>A comprehensive guide on building and training VGG with PyTorch.</div><div class=post-meta><span title='2024-05-13 00:00:00 +0000 UTC'>May 13, 2024</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4162 words&nbsp;·&nbsp;Jian Zhong</div></header><figure class=entry-cover><img loading=eager srcset="http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_360x0_resize_box_3.png 360w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_480x0_resize_box_3.png 480w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_720x0_resize_box_3.png 720w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png 3200w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png alt="[cover image] Architecture of VGG Model (image credit: Jian Zhong)" width=3200 height=1800><p>[cover image] Architecture of VGG Model (image credit: Jian Zhong)</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#vgg-architecture-and-implementation>VGG architecture and implementation</a><ul><li><a href=#stacked-convolutional-layers>Stacked convolutional layers</a></li><li><a href=#stacked-fully-connected-and-dropout-layers>Stacked fully-connected and dropout layers</a></li><li><a href=#vgg-model>VGG model</a></li><li><a href=#model-generation>Model generation</a></li></ul></li><li><a href=#data-processing>Data processing</a></li><li><a href=#data-augmentation>Data Augmentation</a><ul><li><a href=#random-horizontal-flip>Random horizontal flip</a></li><li><a href=#random-color-shift>Random color shift</a></li><li><a href=#other-data-augmentations>Other data augmentations</a></li></ul></li><li><a href=#training-and-validation>Training and validation</a><ul><li><a href=#top-k-accuracy-or-error>Top k accuracy (or error)</a></li><li><a href=#loss-function-regularization-and-optimizer>Loss function, regularization, and optimizer</a></li><li><a href=#learning-rate-adjustment>Learning rate adjustment</a></li><li><a href=#training-deep-models>Training deep models</a></li></ul></li><li><a href=#results>Results</a></li><li><a href=#reference>Reference</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>The VGG (Visual Geometry Group) model is a type of convolutional neural network (CNN) outlined in the paper <a href=https://arxiv.org/abs/1409.1556v6><em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em></a>. It&rsquo;s known for its use of small convolution filters and deep layers, which helped it achieve top-notch performance in tasks like image classification. By stacking multiple layers with small kernel sizes, VGG can capture a wide range of features from input images. Plus, adding more rectification layers makes its decision-making process sharper and more accurate. The paper also introduced 1x1 convolutional layers to enhance nonlinearity without affecting the receptive view. For training, VGG follows the traditional supervised learning approach where input images and ground truth labels are provided.</p><p>VGG&rsquo;s architecture has significantly shaped the field of neural networks, serving as a foundation and benchmark for many subsequent models in computer vision.</p><p>In this blog post, we&rsquo;ll guide you through implementing and training the VGG architecture using PyTorch, step by step. You can find the complete code for defining and training the VGG model on my <a href=https://github.com/JianZhongDev/VGGPyTorch>GitHub repository</a> (URL: <a href=https://github.com/JianZhongDev/VGGPyTorch)>https://github.com/JianZhongDev/VGGPyTorch)</a>.</p><h2 id=vgg-architecture-and-implementation>VGG architecture and implementation<a hidden class=anchor aria-hidden=true href=#vgg-architecture-and-implementation>#</a></h2><p>As you can see in the <strong>cover image</strong> of this post, the VGG model is made up of multiple layers of convolution followed by max-pooling, and it ends with a few fully connected layers. he output from these layers is then fed into a softmax layer to give a normalized confidence score for each image category.</p><p>The key features of the VGG network are these stacked convolutional layers and fully connected layers. We will start with these stacked layers in our implementation.</p><h3 id=stacked-convolutional-layers>Stacked convolutional layers<a hidden class=anchor aria-hidden=true href=#stacked-convolutional-layers>#</a></h3><p>To start, we&rsquo;ll create the stacked convolutional layer as PyTorch <code>nn.Module</code>, like this:</p><p><details><summary>click to expand stacked convolutional layer code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># stacked 2D convolutional layer</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>VGGStacked2DConv</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>        <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create list of stacked layers</span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_layers</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># iterater through each descriptor for the layers and create corresponding layers</span>
</span></span><span class=line><span class=cl>        <span class=n>prev_out_channels</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_descrip</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_descriptor</span> <span class=o>=</span> <span class=n>layer_descriptors</span><span class=p>[</span><span class=n>i_descrip</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># the descriptor needs to be dict</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>cur_descriptor</span><span class=p>,</span> <span class=nb>dict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=c1># get input or default values </span>
</span></span><span class=line><span class=cl>            <span class=n>nof_layers</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>in_channels</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;in_channels&#34;</span><span class=p>,</span> <span class=n>prev_out_channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>out_channels</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;out_channels&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>kernel_size</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;kernel_size&#34;</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>stride</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;stride&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>padding</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;padding&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>bias</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;bias&#34;</span><span class=p>,</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>padding_mode</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;padding_mode&#34;</span><span class=p>,</span> <span class=s2>&#34;zeros&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>activation</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;activation&#34;</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=c1># create layers</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_in_channels</span> <span class=o>=</span> <span class=n>in_channels</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>in_channels</span> <span class=o>=</span> <span class=n>cur_in_channels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>out_channels</span> <span class=o>=</span> <span class=n>out_channels</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>kernel_size</span> <span class=o>=</span> <span class=n>kernel_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>stride</span> <span class=o>=</span> <span class=n>stride</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>padding</span> <span class=o>=</span> <span class=n>padding</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>bias</span> <span class=o>=</span> <span class=n>bias</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>padding_mode</span> <span class=o>=</span> <span class=n>padding_mode</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>activation</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_in_channels</span> <span class=o>=</span> <span class=n>out_channels</span>
</span></span><span class=line><span class=cl>            <span class=n>prev_out_channels</span> <span class=o>=</span> <span class=n>out_channels</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>        <span class=c1># convert list of layers to sequential layers</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>stacked_layers</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>stacked_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>The stacked convolutional layer takes in a list of descriptor dictionaries, each detailing the setup for a repeated convolutional layer followed by an activation. It reads these configurations and builds the stacked convolutional layers accordingly. If certain configuration parameters are not specified, the code fills in default values.</p><h3 id=stacked-fully-connected-and-dropout-layers>Stacked fully-connected and dropout layers<a hidden class=anchor aria-hidden=true href=#stacked-fully-connected-and-dropout-layers>#</a></h3><p>VGG uses dropout regularizations in their fully connected layers. Adding the dropout regularization within PyTorch is straightforward: we just need to insert dropout layers after each hidden layer inside the stacked fully connected layer. (NOTE: Section 4.2 of the <a href=https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html>AlexNet paper</a> provides valuable insights into dropout layers. It&rsquo;s definitely worth a read.)</p><p>We can define the stacked fully connected layer in a similar manner as the stacked convolutional layers:</p><p><details><summary>click to expand stacked fully-connected layer code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># stacked linear layers</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>VGGStackedLinear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create list of stacked layers</span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_layers</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># iterater through each descriptor for the layers and create corresponding layers</span>
</span></span><span class=line><span class=cl>        <span class=n>prev_out_features</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_descrip</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_descriptor</span> <span class=o>=</span> <span class=n>layer_descriptors</span><span class=p>[</span><span class=n>i_descrip</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># the descriptor needs to be dict</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>cur_descriptor</span><span class=p>,</span> <span class=nb>dict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>            
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=n>nof_layers</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>in_features</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;in_features&#34;</span><span class=p>,</span> <span class=n>prev_out_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>out_features</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;out_features&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>bias</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;bias&#34;</span><span class=p>,</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>activation</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;activation&#34;</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>dropout_p</span> <span class=o>=</span> <span class=n>cur_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;dropout_p&#34;</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># create layers</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_in_features</span> <span class=o>=</span> <span class=n>in_features</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>in_features</span> <span class=o>=</span> <span class=n>cur_in_features</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>out_features</span> <span class=o>=</span> <span class=n>out_features</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>bias</span> <span class=o>=</span> <span class=n>bias</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>activation</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>activation</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>dropout_p</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                        <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span> <span class=o>=</span> <span class=n>dropout_p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_in_features</span> <span class=o>=</span> <span class=n>out_features</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=n>prev_out_features</span> <span class=o>=</span> <span class=n>out_features</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># convert list of layers to sequential layers</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>stacked_layers</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>stacked_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><h3 id=vgg-model>VGG model<a hidden class=anchor aria-hidden=true href=#vgg-model>#</a></h3><p>Now that we&rsquo;ve defined the stacked convolutional and fully-connected layers, we can construct the VGG model as follows:</p><p><details><summary>click to expand VGG model code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># VGG model definition</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>VGG</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_conv_descriptors</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_linear_descriptor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>stacked_conv_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>stacked_linear_descriptor</span><span class=p>,</span> <span class=nb>list</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>stacked_layers</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># add stacked convolutional layers and max pooling layers</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_stackconv_descrip</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>stacked_conv_descriptors</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_stacked_conv_descriptor</span> <span class=o>=</span> <span class=n>stacked_conv_descriptors</span><span class=p>[</span><span class=n>i_stackconv_descrip</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>cur_stacked_conv_descriptor</span><span class=p>,</span> <span class=nb>list</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>
</span></span><span class=line><span class=cl>            <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>StackedLayers</span><span class=o>.</span><span class=n>VGGStacked2DConv</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>cur_stacked_conv_descriptor</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># add max pooling layer after stacked convolutional layer</span>
</span></span><span class=line><span class=cl>            <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>kernel_size</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>stride</span> <span class=o>=</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># flatten convolutional layers </span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># add stacked linear layers</span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>StackedLayers</span><span class=o>.</span><span class=n>VGGStackedLinear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>stacked_linear_descriptor</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># add softmax layer at the very end</span>
</span></span><span class=line><span class=cl>        <span class=n>stacked_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Softmax</span><span class=p>(</span><span class=n>dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># convert list of layers to Sequantial network</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>stacked_layers</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>stacked_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>The VGG model takes in a stacked convolutional layer descriptor list, and a fully connected layer descriptor. First, it goes through the convolutional layer descriptors, creating stacked convolutional layers for each descriptor and adding a max pooling layer after each set of stacked convolutional layers. Then, it flattens the output from all the convolutional layers and constructs stacked fully connected layers based on the linear layer descriptor. Finally, a Softmax layer is appended at the end of the network.</p><h3 id=model-generation>Model generation<a hidden class=anchor aria-hidden=true href=#model-generation>#</a></h3><p>Using the model definition provided above, we can create a VGG model by specifying a few layer descriptors. For instance, we can replicate the VGG16 model described in the VGG paper as follows:</p><p><details><summary>click to expand demo creating 16-layer VGG model</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1>## Demo creating 16-layer VGG model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>input_image_width</span> <span class=o>=</span> <span class=mi>224</span>
</span></span><span class=line><span class=cl><span class=n>input_image_height</span> <span class=o>=</span> <span class=mi>224</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_stacked_conv_list</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;in_channels&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;in_ckjhannels&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;in_channels&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>256</span><span class=p>,</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>256</span><span class=p>,</span> <span class=s2>&#34;kernel_size&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;padding&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;in_ckjhannels&#34;</span><span class=p>:</span> <span class=mi>256</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span> <span class=s2>&#34;kernel_size&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;padding&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;in_ckjhannels&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span> <span class=s2>&#34;kernel_size&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;padding&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>conv_image_reduce_ratio</span> <span class=o>=</span> <span class=mi>2</span><span class=o>**</span><span class=nb>len</span><span class=p>(</span><span class=n>model_stacked_conv_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>conv_final_image_width</span> <span class=o>=</span> <span class=n>input_image_width</span><span class=o>//</span><span class=n>conv_image_reduce_ratio</span>
</span></span><span class=line><span class=cl><span class=n>conv_final_image_height</span> <span class=o>=</span> <span class=n>input_image_height</span><span class=o>//</span><span class=n>conv_image_reduce_ratio</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_stacked_linear</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;in_features&#34;</span><span class=p>:</span> <span class=n>conv_final_image_width</span> <span class=o>*</span> <span class=n>conv_final_image_height</span> <span class=o>*</span> <span class=mi>512</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=mi>4096</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;dropout_p&#34;</span><span class=p>:</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=mi>1000</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;activation&#34;</span><span class=p>:</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>VGG</span><span class=o>.</span><span class=n>VGG</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>stacked_conv_descriptors</span> <span class=o>=</span> <span class=n>model_stacked_conv_list</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>stacked_linear_descriptor</span> <span class=o>=</span> <span class=n>model_stacked_linear</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>enable_debug</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>Here&rsquo;s what the printout of the VGG16 model looks like:</p><p><details><summary>click to expand 16-layer VGG model printout</summary><div><pre tabindex=0><code>VGG(
  (network): Sequential(
    (0): VGGStacked2DConv(
      (network): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
      )
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): VGGStacked2DConv(
      (network): Sequential(
        (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
      )
    )
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): VGGStacked2DConv(
      (network): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (5): ReLU()
      )
    )
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): VGGStacked2DConv(
      (network): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (5): ReLU()
      )
    )
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): VGGStacked2DConv(
      (network): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (5): ReLU()
      )
    )
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Flatten(start_dim=1, end_dim=-1)
    (11): VGGStackedLinear(
      (network): Sequential(
        (0): Linear(in_features=25088, out_features=4096, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=4096, out_features=4096, bias=True)
        (4): ReLU()
        (5): Dropout(p=0.5, inplace=False)
        (6): Linear(in_features=4096, out_features=1000, bias=True)
      )
    )
    (12): Softmax(dim=-1)
  )
)
</code></pre></div></details></p><h2 id=data-processing>Data processing<a hidden class=anchor aria-hidden=true href=#data-processing>#</a></h2><p>In the VGG paper, the only data processing done on the input data is subtracting the RGB value calculated from the training set. To apply this processing, we start by going through the entire training dataset and computing the mean value for each color channel.</p><p><details><summary>click to expand channel mean value calculation code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1>## calculated the averaged channel values across the entire data set</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># train_dataloader is the dataloader iterating through the entire training data set</span>
</span></span><span class=line><span class=cl><span class=n>input_dataloader</span> <span class=o>=</span> <span class=n>train_dataloader</span>
</span></span><span class=line><span class=cl><span class=n>nof_batchs</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>input_dataloader</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>avg_ch_vals</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_batchs</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i_batch</span><span class=p>,</span> <span class=n>data</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>input_dataloader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>data</span>
</span></span><span class=line><span class=cl>    <span class=n>cur_avg_ch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>),</span> <span class=n>keepdim</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_ch_vals</span><span class=p>[</span><span class=n>i_batch</span><span class=p>]</span> <span class=o>=</span> <span class=n>cur_avg_ch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>avg_ch_vals</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>avg_ch_vals</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>avg_ch_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>avg_ch_vals</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=n>keepdim</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;result size = &#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>avg_ch_val</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;result val = &#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>repr</span><span class=p>(</span><span class=n>avg_ch_val</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>Using the mean channel value, we can perform the mentioned data processing by defining a background subtraction function and using the <code>Lambda() </code>transform provided by <code>torchvision</code> like this:</p><p><details><summary>click to expand mean channel value subtraction code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>functools</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.transforms</span> <span class=kn>import</span> <span class=n>v2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># subtract constant value from the image</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>subtract_const</span><span class=p>(</span><span class=n>src_image</span><span class=p>,</span> <span class=n>const_val</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>src_image</span> <span class=o>-</span> <span class=n>const_val</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## subtract global mean channel background</span>
</span></span><span class=line><span class=cl><span class=n>train_data_ch_avg</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[[</span><span class=mf>0.4914</span><span class=p>]],[[</span><span class=mf>0.4822</span><span class=p>]],[[</span><span class=mf>0.4465</span><span class=p>]]])</span>
</span></span><span class=line><span class=cl><span class=c1># NOTE: train_data_ch_avg is obtained from the channel mean value calculation code</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>train_data_ch_avg</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>subtract_ch_avg</span> <span class=o>=</span> <span class=n>functools</span><span class=o>.</span><span class=n>partial</span><span class=p>(</span><span class=n>subtract_const</span><span class=p>,</span> <span class=n>const_val</span> <span class=o>=</span> <span class=n>train_data_ch_avg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>subtract_channel_mean_transform</span> <span class=o>=</span> <span class=n>v2</span><span class=o>.</span><span class=n>Lambda</span><span class=p>(</span><span class=n>subtract_ch_avg</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><h2 id=data-augmentation>Data Augmentation<a hidden class=anchor aria-hidden=true href=#data-augmentation>#</a></h2><p>The VGG paper also employed various data augmentation techniques to prevent overfitting. Here&rsquo;s how we implement them:</p><h3 id=random-horizontal-flip>Random horizontal flip<a hidden class=anchor aria-hidden=true href=#random-horizontal-flip>#</a></h3><p><code>torchvision</code> already includes a built-in transformation for randomly flipping images horizontally. Therefore, we can simply utilize this built-in transformation for horizontal flips.</p><p><details><summary>click to expand random horizontal flip code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.transforms</span> <span class=kn>import</span> <span class=n>v2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>rand_hflip_transform</span> <span class=o>=</span> <span class=n>v2</span><span class=o>.</span><span class=n>RandomHorizontalFlip</span><span class=p>(</span><span class=mf>0.5</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>In the VGG paper, they utilized both the original image and its horizontally flipped counterpart to predict classification results. They then averaged these results to obtain the final classification. Consequently, we can implement the validation process as follows:</p><p><details><summary>click to expand validation code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># validate model in one epoch and return the top k-th result </span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>validate_one_epoch_topk_aug</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>validate_loader</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>loss_func</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>transforms</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>device</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>top_k</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=p>):</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>tot_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_nof_batch</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>validate_loader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>correct_samples</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_samples</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>validate_loader</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>nof_transforms</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>transforms</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_batch</span><span class=p>,</span> <span class=n>data</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>validate_loader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>data</span> 
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>group_outputs</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_transforms</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>            <span class=n>group_loss</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_transforms</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i_trans</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_transforms</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=p>[</span><span class=n>i_trans</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_input</span> <span class=o>=</span> <span class=n>inputs</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>cur_transform</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>cur_input</span> <span class=o>=</span> <span class=n>cur_transform</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>cur_input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_loss</span> <span class=o>=</span> <span class=n>loss_func</span><span class=p>(</span><span class=n>cur_output</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>group_outputs</span><span class=p>[</span><span class=n>i_trans</span><span class=p>]</span> <span class=o>=</span> <span class=n>cur_output</span>
</span></span><span class=line><span class=cl>                <span class=n>group_loss</span><span class=p>[</span><span class=n>i_trans</span><span class=p>]</span> <span class=o>=</span> <span class=n>cur_loss</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=n>outputs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>group_outputs</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>),</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>group_loss</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>),</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=n>tot_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=c1># NOTE: we will define batch_in_top_k() later</span>
</span></span><span class=line><span class=cl>            <span class=c1># NOTE: batch_in_top_k() return a mask array indicate if label in the top k result </span>
</span></span><span class=line><span class=cl>            <span class=n>correct_samples</span> <span class=o>+=</span> <span class=p>(</span><span class=n>batch_in_top_k</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>top_k</span><span class=p>))</span><span class=o>.</span><span class=n>type</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>tot_loss</span><span class=o>/</span><span class=n>tot_nof_batch</span>
</span></span><span class=line><span class=cl>    <span class=n>correct_rate</span> <span class=o>=</span> <span class=n>correct_samples</span><span class=o>/</span><span class=n>tot_samples</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Validate: top</span><span class=si>{</span><span class=n>top_k</span><span class=si>}</span><span class=s2> Accuracy: </span><span class=si>{</span><span class=p>(</span><span class=mi>100</span><span class=o>*</span><span class=n>correct_rate</span><span class=p>)</span><span class=si>:</span><span class=s2>&gt;0.2f</span><span class=si>}</span><span class=s2>%, Avg loss: </span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>&gt;8f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>avg_loss</span><span class=p>,</span> <span class=n>correct_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## demo training validation loop</span>
</span></span><span class=line><span class=cl><span class=n>validate_transforms</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>hflip</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i_epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34; ------ Epoch </span><span class=si>{</span><span class=n>i_epoch</span><span class=si>}</span><span class=s2> ------ &#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>train_one_epoch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>loss_func</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>validate_one_epoch_topk_aug</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>validate_dataloader</span><span class=p>,</span> <span class=n>loss_func</span><span class=p>,</span> <span class=n>validate_transforms</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=n>device</span><span class=p>,</span> <span class=n>top_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span></code></pre></td></tr></table></div></div></div></details></p><h3 id=random-color-shift>Random color shift<a hidden class=anchor aria-hidden=true href=#random-color-shift>#</a></h3><p>In VGG, another augmentation technique involved adjusting the RGB values of training images by a random combination of the principal component analysis (PCA) eigenvectors derived from the RGB values across all pixels of all images in the training set. For a detailed explanation, refer to section <em>4.1 Data Augmentation</em> in the <a href=https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html>AlexNet paper</a>.</p><p>Here&rsquo;s how the random color shift is implemented:</p><p>Before training begins, we go through the entire training set and gather all RGB values from each image. This data is used to create an \(m \times n\) data matrix, where \(n\) represents the number of channels (3 for RGB images) and \(m\) represents the total number of pixels across all images in the training set \(m = \text{number of images} \times \text{image height} \times \text{image width}\). We then calculate the covariance matrix of this data matrix. Next, we conduct principal component analysis (PCA) on the covariance matrix using singular value decomposition (SVD). The resulting \(U\) matrix contains columns representing the PCA eigenvectors, and the \(S\) matrix contains the corresponding eigenvalues.</p><p><details><summary>click to expand dataset channel PCA code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1>## PCA for covariance matrix of image channels across all the pixels </span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># train_dataloader is the dataloader iterating through the entire training data set</span>
</span></span><span class=line><span class=cl><span class=n>input_dataloader</span> <span class=o>=</span> <span class=n>train_dataloader</span>
</span></span><span class=line><span class=cl><span class=n>nof_batchs</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>input_dataloader</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ch_vecs</span> <span class=o>=</span> <span class=p>[</span><span class=kc>None</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_batchs</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i_batch</span><span class=p>,</span> <span class=n>data</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>input_dataloader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>data</span>
</span></span><span class=line><span class=cl>    <span class=c1># swap channel and batch axis and flatten the dimension of (batch, image height, image width)</span>
</span></span><span class=line><span class=cl>    <span class=n>ch_vecs</span><span class=p>[</span><span class=n>i_batch</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>swapaxes</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>start_dim</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=n>end_dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ch_vecs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>ch_vecs</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ch_cov</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cov</span><span class=p>(</span><span class=n>ch_vecs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ch_vecs</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>U</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>Vh</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>svd</span><span class=p>(</span><span class=n>ch_cov</span><span class=p>,</span> <span class=n>full_matrices</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## Each column of U is a channel PCA eigenvector</span>
</span></span><span class=line><span class=cl><span class=c1>## S contains the corresponding to eigenvectors</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;U:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>repr</span><span class=p>(</span><span class=n>U</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;S:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>S</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Vh:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>Vh</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Note: In this implementation, all pixels are loaded into computer memory at the same time. For larger datasets, the code for calculating the covariance matrix may need enhancements to compute it without simultaneously loading all data into memory.</p></div></details></p><p>During training, we create a randomized linear combination of PCA eigenvectors by adding up the product of each eigenvector with a randomized amplitude. This amplitude is computed by multiplying the corresponding eigenvalue by a random value drawn from a Gaussian distribution with a mean of 0 and a standard deviation of 0.1.</p><p><details><summary>click to expand random color shift code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>functools</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.transforms</span> <span class=kn>import</span> <span class=n>v2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># image channel radom PCA eigenvec addition agumentation</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>random_ch_shift_pca</span><span class=p>(</span><span class=n>src_image</span><span class=p>,</span> <span class=n>pca_eigenvecs</span><span class=p>,</span> <span class=n>pca_eigenvals</span><span class=p>,</span> <span class=n>random_paras</span> <span class=o>=</span> <span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>norm_meam</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>norm_std</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>random_paras</span><span class=p>,</span> <span class=nb>dict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_meam</span> <span class=o>=</span> <span class=n>random_paras</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;mean&#34;</span><span class=p>,</span> <span class=n>norm_meam</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_std</span> <span class=o>=</span> <span class=n>random_paras</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;std&#34;</span><span class=p>,</span> <span class=n>norm_std</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>nof_dims</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>src_image</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>nof_channels</span> <span class=o>=</span> <span class=n>src_image</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>assert</span><span class=p>(</span><span class=n>pca_eigenvecs</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>==</span> <span class=n>nof_channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>pca_eigenvals</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span><span class=p>(</span><span class=n>pca_eigenvals</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=o>==</span> <span class=n>pca_eigenvecs</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>norm_means</span> <span class=o>=</span> <span class=mi>0</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>pca_eigenvals</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>norm_stds</span> <span class=o>=</span> <span class=mf>0.1</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>pca_eigenvals</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>alphas</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>norm_means</span><span class=p>,</span> <span class=n>norm_stds</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scale_factors</span> <span class=o>=</span> <span class=p>(</span><span class=n>alphas</span> <span class=o>*</span> <span class=n>pca_eigenvals</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>((</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ch_offset</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>pca_eigenvecs</span><span class=p>,</span> <span class=n>scale_factors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ch_offset</span> <span class=o>=</span> <span class=n>ch_offset</span><span class=o>.</span><span class=n>view</span><span class=p>((</span><span class=n>nof_channels</span><span class=p>,)</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span><span class=p>,)</span> <span class=o>*</span> <span class=p>(</span><span class=n>nof_dims</span> <span class=o>-</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dst_image</span> <span class=o>=</span> <span class=n>src_image</span> <span class=o>+</span> <span class=n>ch_offset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dst_image</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## random channel shifts</span>
</span></span><span class=line><span class=cl><span class=c1># NOTE: trainset_pca_eigenvecs and trainset_pca_eigenvals are the U and S matrix obtained from above mentioned PCA analysis</span>
</span></span><span class=line><span class=cl><span class=n>trainset_pca_eigenvecs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=o>-</span><span class=mf>0.5580</span><span class=p>,</span>  <span class=mf>0.7063</span><span class=p>,</span>  <span class=mf>0.4356</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.5775</span><span class=p>,</span>  <span class=mf>0.0464</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.8151</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.5960</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.7063</span><span class=p>,</span>  <span class=mf>0.3820</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>trainset_pca_eigenvecs</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>trainset_pca_eigenvals</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.1719</span><span class=p>,</span> <span class=mf>0.0139</span><span class=p>,</span> <span class=mf>0.0029</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>trainset_pca_eigenvals</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>random_ch_shift</span> <span class=o>=</span> <span class=n>functools</span><span class=o>.</span><span class=n>partial</span><span class=p>(</span><span class=n>random_ch_shift_pca</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                    <span class=n>pca_eigenvecs</span> <span class=o>=</span> <span class=n>trainset_pca_eigenvecs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>pca_eigenvals</span> <span class=o>=</span> <span class=n>trainset_pca_eigenvals</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                    <span class=n>random_paras</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;mean&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;std&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>},</span>
</span></span><span class=line><span class=cl>                                   <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>random_ch_shift_transform</span> <span class=o>=</span>  <span class=n>v2</span><span class=o>.</span><span class=n>Lambda</span><span class=p>(</span><span class=n>random_ch_shift</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><h3 id=other-data-augmentations>Other data augmentations<a hidden class=anchor aria-hidden=true href=#other-data-augmentations>#</a></h3><p>The VGG paper also employed additional augmentation techniques like random translations and random crops. However, since the CIFAR dataset&rsquo;s image size is much smaller (32x32) compared to the ImageNet dataset (256x256), there isn&rsquo;t much flexibility to utilize these techniques effectively.</p><h2 id=training-and-validation>Training and validation<a hidden class=anchor aria-hidden=true href=#training-and-validation>#</a></h2><h3 id=top-k-accuracy-or-error>Top k accuracy (or error)<a hidden class=anchor aria-hidden=true href=#top-k-accuracy-or-error>#</a></h3><p>In the VGG paper, the main way they measured performance was using the top k error. In my version, I focused on calculating the top k accuracy instead. Top k accuracy shows how often the actual label is among the top k predictions made by the model with the highest confidence. On the other hand, top k error tells us how often the actual label is not included in the top k predictions.</p><p>The relationship between top k error and top k accuracy is simply connected by the following formula:</p><p>$$
\text{top k error} = 1 − \text{top k accuracy}
$$</p><p>A higher top k accuracy and lower top k error indicate better model performance.</p><p>During the validation (or test) process, the top k-th accuracy can be estimated by dividing the total number of valiation (or test) samples by the number of samples where the label is in the top k predictions.</p><p>$$
\text{top k accuracy} = \frac{\text{number of samples (label is in top k predictions)}}{\text{total number of samples}}
$$</p><p>Therefore, top k accuracy can be calculated using the following code:</p><p><details><summary>click to expand top k accuarcy calculation code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># Evaluate if label is within top k prediction result for one batch of data</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>batch_in_top_k</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>top_k</span> <span class=o>=</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>sorted_outputs</span><span class=p>,</span> <span class=n>sorted_idxs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>descending</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>in_top_k</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>full_like</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>cur_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>top_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>in_top_k</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>logical_or</span><span class=p>(</span><span class=n>sorted_idxs</span><span class=p>[:,</span><span class=n>cur_idx</span><span class=p>]</span> <span class=o>==</span> <span class=n>labels</span><span class=p>,</span> <span class=n>in_top_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>in_top_k</span>   
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>In each batch, we organize the softmax layer results, which represent the confidences for each predicted category, in descending order. Then, we check if the ground truth label is among the top k predictions. This check result is stored in a boolean mask array, where &rsquo;true&rsquo; indicates the label is in the top k predictions, and &lsquo;false&rsquo; indicates it&rsquo;s not. This boolean mask array holds the results for all samples within the batch. To find the total number of samples where the label is among the top k predictions, we simply sum the mask arrays from all batches.</p><h3 id=loss-function-regularization-and-optimizer>Loss function, regularization, and optimizer<a hidden class=anchor aria-hidden=true href=#loss-function-regularization-and-optimizer>#</a></h3><p>VGG employs multinomial logistic regression as its loss function. For optimization, it utilizes mini-batch gradient descent with momentum and weight decay. In PyTorch, these can be implemented as follows:</p><p><details><summary>click to expand loss function and optimizer code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=n>loss_func</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span> <span class=o>=</span> <span class=mf>1E-2</span><span class=p>,</span> <span class=n>momentum</span> <span class=o>=</span> <span class=mf>0.9</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span> <span class=mf>5E-4</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div></details></p><p>Additionally, dropout regularization has been incorporated into the model as another form of regularization as mentioned earlier in this post.</p><h3 id=learning-rate-adjustment>Learning rate adjustment<a hidden class=anchor aria-hidden=true href=#learning-rate-adjustment>#</a></h3><p>In the VGG paper, the authors initially train with a learning rate of 1E-2. Then, they reduce the learning rate by a factor of 10 when the validation set accuracy plateaus. This can be implemented using the <code>ReduceLROnPlateau()</code> function provided by PyTorch, like this:</p><p><details><summary>click to expand learing rate adjustment code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>ReduceLROnPlateau</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>mode</span> <span class=o>=</span> <span class=s2>&#34;max&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>factor</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>patience</span> <span class=o>=</span> <span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>threshold</span> <span class=o>=</span> <span class=mf>1E-3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>min_lr</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Demo training and validation loop</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i_epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34; ------ Epoch </span><span class=si>{</span><span class=n>i_epoch</span><span class=si>}</span><span class=s2> ------ &#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>cur_lr</span> <span class=o>=</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>param_groups</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;lr&#39;</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;current lr = </span><span class=si>{</span><span class=n>cur_lr</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>cur_train_loss</span> <span class=o>=</span> <span class=n>train_one_epoch</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                     <span class=n>train_dataloader</span><span class=p>,</span>  
</span></span><span class=line><span class=cl>                                     <span class=n>loss_func</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                     <span class=n>optimizer</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                     <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>cur_validate_loss</span><span class=p>,</span> <span class=n>cur_validate_accuracy</span> <span class=o>=</span> <span class=n>validate_one_epoch_topk_aug</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                                                           <span class=n>validate_dataloader</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                                                           <span class=n>loss_func</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                                                           <span class=n>validate_transforms</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                                                           <span class=n>device</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                                                           <span class=n>top_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>cur_validate_accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>NOTE: The description of the <code>ReduceLROnPlateau()</code> function in the PyTorch documentation can be confusing. I found that reading the source code of the <code>ReduceLROnPlateau()</code> definition provides clearer understanding.</p></div></details></p><h3 id=training-deep-models>Training deep models<a hidden class=anchor aria-hidden=true href=#training-deep-models>#</a></h3><p>Optimizing deep models from scratch with completely random initialization can be very challenging for the optimizer. It often leads to the learning process getting stuck for long periods.</p><p>To tackle this issue, the VGG authors first train a shallow model. Then, they use the learned parameters from this shallow model to initialize deeper ones.</p><p>Transferring learned parameters between models in PyTorch is straightforward. It involves copying the weights and biases from corresponding layers between the two models. If you&rsquo;re using the VGG model definition from this blog post, the example code looks like this:</p><p><details><summary>click to expand model parameter transfer code</summary><div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1>## demo transfer model parameters</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>input_image_width</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>input_image_height</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create model 1</span>
</span></span><span class=line><span class=cl><span class=n>model1_stacked_conv_list</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;in_channels&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;in_ckjhannels&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>conv_image_reduce_ratio</span> <span class=o>=</span> <span class=mi>2</span><span class=o>**</span><span class=nb>len</span><span class=p>(</span><span class=n>model1_stacked_conv_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>conv_final_image_width</span> <span class=o>=</span> <span class=n>input_image_width</span><span class=o>//</span><span class=n>conv_image_reduce_ratio</span>
</span></span><span class=line><span class=cl><span class=n>conv_final_image_height</span> <span class=o>=</span> <span class=n>input_image_height</span><span class=o>//</span><span class=n>conv_image_reduce_ratio</span>
</span></span><span class=line><span class=cl><span class=n>model1_stacked_linear</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;in_features&#34;</span><span class=p>:</span> <span class=n>conv_final_image_width</span> <span class=o>*</span> <span class=n>conv_final_image_height</span> <span class=o>*</span> <span class=mi>512</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;dropout_p&#34;</span><span class=p>:</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;activation&#34;</span><span class=p>:</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>model1</span> <span class=o>=</span> <span class=n>VGG</span><span class=o>.</span><span class=n>VGG</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>stacked_conv_descriptors</span> <span class=o>=</span> <span class=n>model1_stacked_conv_list</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>stacked_linear_descriptor</span> <span class=o>=</span> <span class=n>model1_stacked_linear</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>enable_debug</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># create model 2</span>
</span></span><span class=line><span class=cl><span class=n>model2_stacked_conv_list</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;in_channels&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span> 
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;in_ckjhannels&#34;</span><span class=p>:</span> <span class=mi>64</span><span class=p>,</span> <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,},</span> 
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>conv_image_reduce_ratio</span> <span class=o>=</span> <span class=mi>2</span><span class=o>**</span><span class=nb>len</span><span class=p>(</span><span class=n>model2_stacked_conv_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>conv_final_image_width</span> <span class=o>=</span> <span class=n>input_image_width</span><span class=o>//</span><span class=n>conv_image_reduce_ratio</span>
</span></span><span class=line><span class=cl><span class=n>conv_final_image_height</span> <span class=o>=</span> <span class=n>input_image_height</span><span class=o>//</span><span class=n>conv_image_reduce_ratio</span>
</span></span><span class=line><span class=cl><span class=n>model2_stacked_linear</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;in_features&#34;</span><span class=p>:</span> <span class=n>conv_final_image_width</span> <span class=o>*</span> <span class=n>conv_final_image_height</span> <span class=o>*</span> <span class=mi>512</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=mi>512</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;dropout_p&#34;</span><span class=p>:</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>     <span class=s2>&#34;activation&#34;</span><span class=p>:</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>model2</span> <span class=o>=</span> <span class=n>VGG</span><span class=o>.</span><span class=n>VGG</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>stacked_conv_descriptors</span> <span class=o>=</span> <span class=n>model2_stacked_conv_list</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>stacked_linear_descriptor</span> <span class=o>=</span> <span class=n>model2_stacked_linear</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>enable_debug</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># transfer parameter of 1st convoluation layer from model 1 to model 2</span>
</span></span><span class=line><span class=cl><span class=n>model2</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>model1</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl><span class=n>model2</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>model1</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>network</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>bias</span>
</span></span></code></pre></td></tr></table></div></div><p>NOTE: We&rsquo;ve organized the sequential layers of the VGG model and stacked them within the &ldquo;network&rdquo; attribute of the object. This means we can access each specific layer inside the network by indexing the &ldquo;network&rdquo; attribute.</p></div></details></p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>Given the large size of the ImageNet dataset and the extensive time required for training, we&rsquo;ll opt for a smaller dataset, <a href=https://www.cs.toronto.edu/~kriz/cifar.html>CIFAR10</a>, to demonstrate training and validation more quickly.</p><p>I&rsquo;ve examined several models based on the VGG architecture, and I&rsquo;ve listed some of them (model I, II, and III) below:</p><table><style>table{font-family:arial,sans-serif;border-collapse:collapse;width:100%}td,th{border:1px solid #ddd;text-align:center;padding:8px}</style><thead><tr><td colspan=3>Model Configuration</td></tr><tr><th>I</th><th>II</th><th>III</th></tr></thead><tbody><tr><td>conv3-128</td><td>conv3-128</td><td>conv3-128</td></tr><tr><td colspan=3>maxpool</td></tr><tr><td rowspan=2>conv3-256</td><td>conv3-256</td><td>conv3-256</td></tr><tr><td>conv3-256</td><td>conv3-256</td></tr><tr><td colspan=3>maxpool</td></tr><tr><td rowspan=6>conv3-512</td><td rowspan=3>conv3-512</td><td>conv3-512</td></tr><tr><td>conv3-512</td></tr><tr><td>conv3-512</td></tr><tr><td rowspan=3>conv3-512</td><td>conv3-512</td></tr><tr><td>conv3-512</td></tr><tr><td>conv3-512</td></tr><tr><td colspan=3>maxpool</td></tr><tr><td rowspan=2>FC-1024</td><td>FC-1024</td><td>FC-1024</td></tr><tr><td>FC-1024</td><td>FC-1024</td></tr><tr><td>FC-10</td><td>FC-10</td><td>FC-10</td></tr><tr><td colspan=3>soft-max</td></tr></tbody></table><p>After training these model variations, I computed the top 1 to top 5 accuracies using the CIFAR10 test dataset. Here&rsquo;s a summary of the results:</p><table><style>table{font-family:arial,sans-serif;border-collapse:collapse;width:100%}td,th{border:1px solid #ddd;text-align:center;padding:8px}</style><thead><tr><th>Model config.</th><th>top-1 accuarcy(%)</th><th>top-2 accuarcy(%)</th><th>top-3 accuarcy(%)</th><th>top-4 accuarcy(%)</th><th>top-5 accuarcy(%)</th></tr></thead><tbody><tr><td>I</td><td>82.45</td><td>92.74</td><td>96.23</td><td>97.82</td><td>98.87</td></tr><tr><td>II</td><td>84.88</td><td>93.95</td><td>96.91</td><td>98.23</td><td>98.99</td></tr><tr><td>III</td><td>86.93</td><td>94.39</td><td>96.83</td><td>98.15</td><td>98.90</td></tr></tbody></table><p>We can observe that the accuracy tends to improve as the depth of the models increases.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>[1] Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for Large-Scale image recognition. arXiv (Cornell University). <a href=https://doi.org/10.48550/arxiv.1409.1556>https://doi.org/10.48550/arxiv.1409.1556</a></p><p>[2] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems, 25, 1097–1105. <a href=http://books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf>http://books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf</a></p><p>[3] Krizhevsky, A., Nair, V. and Hinton, G. (2014) The CIFAR-10 Dataset. <a href=https://www.cs.toronto.edu/~kriz/cifar.html>https://www.cs.toronto.edu/~kriz/cifar.html</a></p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you found this article helpful, please cite it as:</p><blockquote><p>Zhong, Jian (May 2024). Building and Training VGG with PyTorch: A Step-by-Step Guide. Vision Tech Insights. <a href=https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_VGG_PyTorch/>https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_VGG_PyTorch/</a>.</p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-html data-lang=html><span class=line><span class=cl>@article{zhong2024buildtrainVGGPyTorch,
</span></span><span class=line><span class=cl>  title   = &#34;Building and Training VGG with PyTorch: A Step-by-Step Guide&#34;,
</span></span><span class=line><span class=cl>  author  = &#34;Zhong, Jian&#34;,
</span></span><span class=line><span class=cl>  journal = &#34;jianzhongdev.github.io&#34;,
</span></span><span class=line><span class=cl>  year    = &#34;2024&#34;,
</span></span><span class=line><span class=cl>  month   = &#34;May&#34;,
</span></span><span class=line><span class=cl>  url     = &#34;https://jianzhongdev.github.io/VisionTechInsights/posts/implement_train_VGG_PyTorch/&#34;
</span></span><span class=line><span class=cl>}
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/computer-vision/>Computer Vision</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/building_a_configuration_file_parser_with_cpp/><span class=title>Next »</span><br><span>Building a Configuration File Parser with C++</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on x" href="https://x.com/intent/tweet/?text=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f&amp;hashtags=computervision%2cmachinelearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f&amp;title=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide&amp;summary=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f&title=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on whatsapp" href="https://api.whatsapp.com/send?text=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on telegram" href="https://telegram.me/share/url?text=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building and Training VGG with PyTorch: A Step-by-Step Guide on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20and%20Training%20VGG%20with%20PyTorch%3a%20A%20Step-by-Step%20Guide&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fimplement_train_vgg_pytorch%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Vision Tech Insights</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>