<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Autoencoders with PyTorch: Full Code Guide | Vision Tech Insights</title>
<meta name=keywords content="computer vision,machine learning"><meta name=description content="A comprehensive guide on building and training autoencoders with PyTorch."><meta name=author content="Jian Zhong"><link rel=canonical href=http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Autoencoders with PyTorch: Full Code Guide"><meta property="og:description" content="A comprehensive guide on building and training autoencoders with PyTorch."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/"><meta property="og:image" content="http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-23T00:00:00+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png"><meta name=twitter:title content="Autoencoders with PyTorch: Full Code Guide"><meta name=twitter:description content="A comprehensive guide on building and training autoencoders with PyTorch."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Autoencoders with PyTorch: Full Code Guide","item":"http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Autoencoders with PyTorch: Full Code Guide","name":"Autoencoders with PyTorch: Full Code Guide","description":"A comprehensive guide on building and training autoencoders with PyTorch.","keywords":["computer vision","machine learning"],"articleBody":"An autoencoder is a type of artificial neural network that learns to create efficient codings, or representations, of unlabeled data, making it useful for unsupervised learning. Autoencoders can be used for tasks like reducing the number of dimensions in data, extracting important features, and removing noise. They’re also important for building semi-supervised learning models and generative models. The concept of autoencoders has inspired many advanced models.\nIn this blog post, we’ll start with a simple introduction to autoencoders. Then, we’ll show how to build an autoencoder using a fully-connected neural network. We’ll explain what sparsity constraints are and how to add them to neural networks. After that, we’ll go over how to build autoencoders with convolutional neural networks. Finally, we’ll talk about some common uses for autoencoders.\nYou can find all the source code and tutorial scripts mentioned in this blog post in my GitHub repository (URL: https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main ).\nAutoencoder Network Redundancy of Data Representation The key idea behind autoencoders is to reduce redundancy in data representation. Often, data is represented in a way that isn’t very efficient, leading to higher dimensions than necessary. This means many parts of the data are redundant. For example, the MNIST dataset contains 28x28 pixel images of handwritten digits from 0 to 9. Ideally, we only need one variable to represent these digits, but the image representation uses 784 (28x28) grayscale values.\nAutoencoders work by compressing the features as the neural network processes the data and then reconstructing the original data from this compressed form. This process helps the network learn a more efficient way to represent the input data.\nTypical Structure of an Autoencoder Network An autoencoder network typically has two parts: an encoder and a decoder. The encoder compresses the input data into a smaller, lower-dimensional form. The decoder then takes this smaller form and reconstructs the original input data. This smaller form, created by the encoder, is often called the latent space or the “bottleneck.” The latent space usually has fewer dimensions than the original input data.\nArchitecture of autoencoder. (image credit: Jian Zhong)\nFully-Connected Autoencoder Implementing an autoencoder using a fully connected network is straightforward. For the encoder, we use a fully connected network where the number of neurons decreases with each layer. For the decoder, we do the opposite, using a fully connected network where the number of neurons increases with each layer. This creates a “bottleneck” structure in the middle of the network.\nHere is a code example demonstrating how to implement the encoder and decoder of a simple autoencoder network using fully-connected neural networks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 from .Layers import StackedLayers ## fully connected network with only fully connected layers class SimpleFCNetwork(nn.Module): def __init__( self, layer_descriptors = [], ): assert isinstance(layer_descriptors, list) super().__init__() self.network = StackedLayers.VGGStackedLinear(layer_descriptors) def forward(self, x): y = self.network(x) return y ## create models using the above Module nof_features = 28 * 28 code_dim = 64 ## create encoder model encoder_layer_descriptors = [ {\"nof_layers\": 1, \"in_features\": nof_features, \"out_features\": code_dim, \"activation\": torch.nn.LeakyReLU}, ] encoder = SimpleFCNetwork( layer_descriptors = encoder_layer_descriptors ) print(\"Encoder:\") print(encoder) print(\"\\n\") ## create decoder model decoder_layer_descriptors = [ {\"nof_layers\": 1, \"in_features\": code_dim, \"out_features\": nof_features, \"activation\": torch.nn.LeakyReLU}, ] decoder = SimpleFCNetwork( layer_descriptors = decoder_layer_descriptors ) print(\"Decoder:\") print(decoder) The VGGStackedLinear module creates several fully-connected networks based on the input layer descriptors. For a detailed explanation, please refer to my blog post on building and training VGG network with PyTorch.\nHere’s how the architecture of the encoder and decoder defined above looks:\nclick to expand simple fully-connected autoencoder printout\rEncoder: SimpleFCNetwork( (network): VGGStackedLinear( (network): Sequential( (0): Linear(in_features=784, out_features=64, bias=True) (1): LeakyReLU(negative_slope=0.01) ) ) ) Decoder: SimpleFCNetwork( (network): VGGStackedLinear( (network): Sequential( (0): Linear(in_features=64, out_features=784, bias=True) (1): LeakyReLU(negative_slope=0.01) ) ) ) After training the fully-connected network, here are the results for an example data input/output, the latent representation of data in a batch of 512 samples, and the learned feature dictionary:\nTraining results of a simple fully-connected autoencoder (encoder: 784-64, decoder 64-784). a, example data input/output. b, latent representation of data in a batch of 512 samples. c, the learned (decoder) feature dictionary. (image credit: Jian Zhong)\nWithout additional constraints, each sample typically contains numerous non-zero latent features of similar amplitudes, and the learned feature dictionary tends to be highly localized.\nFor a comprehensive understanding of how the above network was implemented and trained, please refer to the TrainSimpleFCAutoencoder Jupyter notebook in my GitHub repository.\nSparsity and Sparse Autoencoder In machine learning, sparsity suggests that in many high-dimensional datasets, only a small number of features or variables are meaningful or non-zero for each observation. In an optimal representation space, many features either have zero values or values that are negligible.\nIn the context of autoencoders, a sparse latent representation of the data is often preferred. This sparse representation can be achieved by incorporating sparse constraints into the network. Adding these constraints helps the autoencoder focus on learning more meaningful features.\nHard Sparsity in Latent Representation Implementing hard sparsity in the latent space involves adding a sparsity layer at the end of the encoder network along the feature dimension. To create a hard sparsity layer, we specify a number k of features to retain in the latent space. During the forward pass, this layer keeps only the top k largest features of the encoded representation for each sample, setting the rest to 0. During backward propagation, the hard sparsity layer only propagates gradients for these top k features.\nHere’s how the hard sparsity layer is implemented:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # hard sparsity function to select the largest k features for each sample in the batch input data # NOTE: this function works on 1d feature space class FeatureTopKFunction(torch.autograd.Function): @staticmethod def forward(ctx, x, k): assert len(x.size()) == 2 src_data_detach = x.detach() # create mask indicating the top k features for each sample within the feature space topk_mask = torch.zeros_like(x, dtype = bool, requires_grad = False) _, indices = src_data_detach.topk(k, dim = -1) for i_batch in range(x.size(0)): topk_mask[i_batch, indices[i_batch,:]] = True # save mask for backward propagation ctx.save_for_backward(topk_mask) # only propagate largest k features of each sample y = torch.zeros_like(x) y[topk_mask] = x[topk_mask] return y @staticmethod def backward(ctx, grad_output): topk_mask = ctx.saved_tensors[0] # only propagate gradient for largest k features of each sample grad_input = torch.zeros_like(grad_output, requires_grad = True) grad_input[topk_mask] = grad_output[topk_mask] return grad_input, None # hard sparsity layer class TopKSparsity(nn.Module): def __init__(self, topk = 1): super().__init__() self.topk = topk def __repr__(self): return self.__class__.__name__ + f\"(topk = {self.topk})\" def forward(self, x): y = FeatureTopKFunction.apply(x, self.topk) return y First, we created our own operation FeatureTopKFunction for hard sparsity and defined its functions for both forward and backward passes. During the forward pass, a mask is generated to identify the top k features of each input sample, which is then stored for later use in the backward pass. This mask ensures that only the top k values are kept, while the rest are set to zero for both value and gradient calculations. In the hard sparsity layer, we specify the number k and incorporate the hard sparsity operation into the forward() method.\nTo implement hard sparsity in an autoencoder, simply add a hard sparsity layer at the end of the encoder network as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # fully connected network with sparsity layer class SimpleSparseFCNetwork(nn.Module): def __init__( self, layer_descriptors = [], feature_sparsity_topk = None, ): assert isinstance(layer_descriptors, list) super().__init__() self.network = nn.Identity() network_layers = [] # add stacked fully connected layers network_layers.append(StackedLayers.VGGStackedLinear(layer_descriptors)) # add top k sparsity along the feature dimension if feature_sparsity_topk is not None : network_layers.append(SparseLayers.TopKSparsity(feature_sparsity_topk)) if len(network_layers) \u003e 0: self.network = nn.Sequential(*network_layers) def forward(self, x): y = self.network(x) return y After training the fully-connected network with these hard sparsity constraints, here are the outcomes for a sample data input/output, the latent representations of data in a batch of 512 samples, and the learned feature dictionary:\nTraining results of a simple fully-connected autoencoder with hard sparsity (encoder: 784-64-sparsity, decoder 64-784). a-c, results of autoencoder trained with top 16 sparsity. d-f, results of autoencoder trained with top 5 sparsity. a,d, example data input/output. b,e, latent representation of data in a batch of 512 samples. c,f, the learned (decoder) feature dictionary. (image credit: Jian Zhong)\nFrom the results above, we observe that increasing the required sparsity with hard constraints reduces the number of non-zero features in the latent space. This encourages the network to learn more global features.\nFor a detailed understanding of how this network was implemented and trained, please refer to the TrainSimpleSparseFCAutoencoder Jupyter notebook in my GitHub repository.\nSoft Sparsity in Latent Representation We can also encourage sparsity in the encoded features of the latent space by applying a soft constraint. This involves adding an additional penalty term to the loss function. The modified loss function with the sparsity penalty appears as follows:\n$$ H_{\\theta}(pred,gt) = J_{\\theta}(pred,gt) + \\lambda \\cdot L_{\\theta}(code) $$\nHere, \\(\\theta, pred, gt\\) represents the parameters of the autoencoder network, the output prediction of autoencoder, and the ground truth data, respectively. \\(H_{\\theta}(pred,gt)\\) ​ is the loss function with sparsity constraints, where \\(J_{\\theta}(pred,gt)\\) is the original loss function, which measures the difference between the network prediction and ground truth. \\(L_{\\theta}(pred,gt)\\) ​ denotes the penalty term for enforcing sparsity. The parameter \\(\\lambda\\) controls the strength of this penalty.\nThe L1 loss of the encoded features is commonly used as a sparsity loss. This loss function is readily available in PyTorch.\nAnother approach to implementing sparsity loss is through a penalty based on KL divergence. The penalty term for this KL divergence-based sparsity can be defined as follows:\n$$ L_{\\theta} = \\frac{1}{s} \\sum^{s}_{j=1} KL(\\rho||\\hat{\\rho_j}) $$\nHere, ​ \\(s\\) represents the number of features in the encoded representation, which corresponds to the dimension of the latent space. ​ \\(j\\) is index for the features in the latent space. \\(KL(\\rho||\\hat{\\rho_j})\\) is calculated as follows:\n$$ KL(\\rho||\\hat{\\rho_j}) = \\rho \\cdot log(\\frac{\\rho}{\\hat{\\rho}_j}) + (1 - \\rho) \\cdot log(\\frac{1-\\rho}{1-\\hat{\\rho}_j}) $$\nHere, \\(\\rho\\) is a sparsity parameter, typically a small value close to zero that is provided during training. \\(\\hat{\\rho}_j\\) ​ is computed from the j-th latent features of the samples within the mini-batch as follows:\n$$ \\hat{\\rho_{j}} = \\frac{1}{m} \\sum^{m}_{i=1} l_i $$\nHere, \\(m\\) denotes the batch size. \\(j\\) indexes the features within the latent space. \\(i\\) indexes the samples within the minibatch. \\(l\\) represents each individual feature within the latent space.\nNote that for the KL divergence expression, the values of \\(\\rho\\) and \\(\\hat{\\rho}_j\\) ​ must fall within the range \\((0,1)\\) . This range should be ensured by using suitable activation functions (such as sigmoid) for the output layer of the encoder, or by appropriately normalizing the latent space features before computing the sparsity loss.\nBelow is the PyTorch code implementation for the KL-divergence based sparsity loss:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Kullback-Leibler divergence formula def kullback_leibler_divergence( rho, rho_hat ): return rho * torch.log(rho/rho_hat) + (1 - rho)*torch.log((1 - rho)/(1 - rho_hat)) # nn.Module of sparsity loss function class KullbackLeiblerDivergenceLoss(nn.Module): def __init__(self, rho = 0.05): assert rho \u003e 0 and rho \u003c 1 super().__init__() self.rho = rho def forward(self, x): rho_hat = torch.mean(x, dim = 0) kl = torch.mean(kullback_leibler_divergence(self.rho, rho_hat)) return kl After training a basic fully-connected autoencoder model with soft sparsity constraints, the results are as follows:\nTraining results of a simple fully-connected autoencoder with soft sparsity (encoder: 784-64, decoder 64-784, KL-divergence soft sparsity loss \\(\\rho = 0.05\\) ). a-c, results of autoencoder trained with \\(\\lambda = 10^{-2}\\) . d-f, results of autoencoder trained with \\(\\lambda = 10^{-1}\\) . a,d, example data input/output. b,e, latent representation of data in a batch of 512 samples. c,f, the learned (decoder) feature dictionary. (image credit: Jian Zhong)\nIncreasing the strength of the sparsity penalty decreases the number of non-zero features in the latent space.\nFor a comprehensive understanding of how this network was implemented and trained, please refer to the TrainSimpleFCAutoencoderWithSparseLoss Jupyter notebook in my GitHub repository.\nLifetime (Winner-Takes-All) Sparsity Unlike conventional sparsity constraints that aim to increase sparsity within each individual sample, lifetime sparsity enforces sparsity across minibatch samples for each feature. Here’s how lifetime sparsity can be implemented:\nDuring training, in the forward propagation phase, for each feature in the latent space, we retain the top k largest values across all minibatch samples and set the remaining values of that feature to zero. During backward propagation, gradients are propagated only for these k non-zero values.\nDuring testing, we disable the lifetime sparsity constraints, allowing the encoder network to output the final representation of the input. The implementation of lifetime sparsity operations is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # lifetime sparsity functon to select the largest k samples for each feature # NOTE: this function works on 1d feature space class LifetimeTopKFunction(torch.autograd.Function): @staticmethod def forward(ctx, x, k): assert len(x.size()) == 2 k = min(k, x.size(0)) src_data_detach = x.detach() # create mask indicating the top k samples for each feature along the batch dimension topk_mask = torch.zeros_like(x, dtype = bool, requires_grad = False) _, indices = src_data_detach.topk(k, dim = 0) for i_feature in range(x.size(-1)): topk_mask[indices[:,i_feature],i_feature] = True # save mask indicationg the top k samples for each feature for back propagation ctx.save_for_backward(topk_mask) # only propagate largest k samples for each feature y = torch.zeros_like(x) y[topk_mask] = x[topk_mask] return y @staticmethod def backward(ctx, grad_output): topk_mask = ctx.saved_tensors[0] # only propagate gradient for largest k samples for each feature grad_input = torch.zeros_like(grad_output, requires_grad = True) grad_input[topk_mask] = grad_output[topk_mask] return grad_input, None In the forward pass, we create a mask that identifies the top k values across the minibatch dimension for each feature in the latent space. This mask is saved for use during the backward pass. During both forward and backward passes, this mask ensures that only the top k values of each feature are retained, while the rest are set to zero.\nWith these lifetime sparsity operations, we can implement a neural network layer that enforces lifetime sparsity as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # lifetime sparsity layer class LifetimeTopkSparsity(nn.Module): def __init__(self, topk = 5): super().__init__() self.topk = topk def __repr__(self): return self.__class__.__name__ + f\"(topk = {self.topk})\" def forward(self, x): y = None if self.training: # only apply lifetime sparsity during training y = LifetimeTopKFunction.apply(x, self.topk) else: y = x return y In the lifetime sparsity layer, we store the k values within the network object. During training, this layer implements lifetime sparsity operations. During testing, the layer simply passes the input directly to the output.\nTo implement lifetime sparsity in an autoencoder, we add the lifetime sparsity layer at the end of the encoder network as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # fully connected network with sparsity layer class SimpleSparseFCNetwork(nn.Module): def __init__( self, layer_descriptors = [], lifetime_sparsity_topk = None, ): assert isinstance(layer_descriptors, list) super().__init__() self.network = nn.Identity() network_layers = [] # add stacked fully connected layers network_layers.append(StackedLayers.VGGStackedLinear(layer_descriptors)) # add top k sparsity along the sample(batch) dimension if lifetime_sparsity_topk is not None: network_layers.append(SparseLayers.LifetimeTopkSparsity(lifetime_sparsity_topk)) if len(network_layers) \u003e 0: self.network = nn.Sequential(*network_layers) def forward(self, x): y = self.network(x) return y After training a simple fully-connected autoencoder model with a lifetime sparsity layer, the results are as follows:\nTraining results of a simple fully-connected autoencoder with life time sparsity (encoder: 784-64-sparsity, decoder 64-784). a-c, results of autoencoder trained with top 25% sparsity. d-f, results of autoencoder trained with top 5% sparsity. a,d, example data input/output. b,e, latent representation of data in a batch of 512 samples. c,f, the learned (decoder) feature dictionary. (image credit: Jian Zhong)\nIncreasing the strength of the lifetime sparsity constraint reduces the number of non-zero features in the latent space. This encourages the network to learn more global features.\nFor detailed insights into how this network was implemented and trained, please refer to the TrainSimpleSparseFCAutoencoder Jupyter notebook in my GitHub repository.\nConvolutional Autoencoder For image data, the encoder network can also be implemented using a convolutional network, where the feature dimensions decrease as the encoder becomes deeper. Max pooling layers can be added to further reduce feature dimensions and induce sparsity in the encoded features. Here’s an example of a convolutional encoder network:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # simple convolutional encoder = stacked convolutional network + maxpooling class SimpleCovEncoder(nn.Module): def __init__( self, convlayer_descriptors = [], maxpoollayer_descriptor = {}, ): assert(isinstance(convlayer_descriptors, list)) assert(isinstance(maxpoollayer_descriptor, dict)) super().__init__() self.network = nn.Identity() network_layers = [] # append stacked convolution layer network_layers.append(StackedLayers.VGGStacked2DConv(convlayer_descriptors)) # append maxpooling layer network_layers.append( nn.MaxPool2d( kernel_size = maxpoollayer_descriptor.get(\"kernel_size\", 2), stride = maxpoollayer_descriptor.get(\"stride\", 2), padding = maxpoollayer_descriptor.get(\"padding\", 0), dilation = maxpoollayer_descriptor.get(\"dilation\", 1), ) ) # flatten output feature space network_layers.append(nn.Flatten(start_dim = 1, end_dim = -1)) if len(network_layers) \u003e 0: self.network = nn.Sequential(*network_layers) def forward(self, x): y = self.network(x) return y ## create encoder model encoder_convlayer_descriptors = [ { \"nof_layers\": 4, \"in_channels\": 1, \"out_channels\": 8, \"kernel_size\": 6, \"stride\": 1, \"padding\": 0, \"activation\": torch.nn.LeakyReLU } ] encoder_maxpoollayer_descriptor = { \"kernel_size\": 2, \"stride\": 2, } encoder = ConvAutoencoder.SimpleCovEncoder( encoder_convlayer_descriptors, encoder_maxpoollayer_descriptor, ) print(\"Encoder:\") print(encoder) The VGGStacked2DConv module generates multiple convolutional networks based on the input layer descriptors. For a detailed explanation, please refer to my blog post on building and training VGG network with PyTorch.\nHere’s a visualization of the architecture of the encoder and decoder described above:\nclick to expand convolutional encoder printout\rEncoder: SimpleCovEncoder( (network): Sequential( (0): VGGStacked2DConv( (network): Sequential( (0): Conv2d(1, 8, kernel_size=(6, 6), stride=(1, 1)) (1): LeakyReLU(negative_slope=0.01) (2): Conv2d(8, 8, kernel_size=(6, 6), stride=(1, 1)) (3): LeakyReLU(negative_slope=0.01) (4): Conv2d(8, 8, kernel_size=(6, 6), stride=(1, 1)) (5): LeakyReLU(negative_slope=0.01) (6): Conv2d(8, 8, kernel_size=(6, 6), stride=(1, 1)) (7): LeakyReLU(negative_slope=0.01) ) ) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): Flatten(start_dim=1, end_dim=-1) ) ) After training the fully-connected network, here are the results for a sample data input/output, the latent representation of data in a batch of 512 samples, and the learned feature dictionary:\nTraining results of a simple autoencoder with convolutional encoder and fully-connected decoder (encoder: Conv6x6-Conv6x6-Conv6x6-Conv6x6-MaxPool2x2, decoder 128-784). a, example data input/output. b, latent representation of data in a batch of 512 samples. c, the learned (decoder) feature dictionary. (image credit: Jian Zhong)\nFor a detailed understanding of how this network was implemented and trained, please see the TrainSimpleConvAutoencoder Jupyter notebook in my GitHub repository.\nTraining and Validation During training, the optimal encoding of input data is generally unknown. In an autoencoder network, the encoder and decoder are trained concurrently. The encoder processes input data to generate compressed representations, while during testing, the decoder reconstructs the input from these representations. The objective of training is to minimize the discrepancy between the decoder’s output and the original input data. Typically, Mean Squared Error (MSE) loss is selected as the optimization loss function for this purpose.\nTraining Dataset When training an autoencoder with image datasets, both the input data and the ground truth are images. Depending on the application of the autoencoder, the input data and ground truth images may not necessarily be identical.\nIn this blog post, we will use the MNIST dataset for our demonstration. In PyTorch, the MNIST dataset provides handwritten digit images as input data and the corresponding digits as ground truth. To train the autoencoder with MNIST and potentially apply various transformations to both input and ground truth images, we implement the following dataset class. This class converts conventional supervised learning datasets into datasets suitable for autoencoder training.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # convert supervised data to autoencoder data def supdata_to_autoencoderdata( supdata, feature_transform = None, target_transform = None, ): src_feature = supdata[0] #extract feature # NOTE: the usuer of this function is responsible for necessary data duplication feature = src_feature if feature_transform: feature = feature_transform(feature) target = src_feature if target_transform: target = target_transform(target) return feature, target # dataset class of autoencoder using existing supervised learning dataset class AutoencoderDataset(torch.utils.data.Dataset): def __init__( self, src_supdataset, feature_transform = None, target_transform = None, ): self.dataset = src_supdataset self.feature_transform = feature_transform self.target_transform = target_transform def __len__(self): return len(self.dataset) def __getitem__(self, idx): src_data = self.dataset[idx] feature, target = supdata_to_autoencoderdata( src_data, self.feature_transform, self.target_transform, ) return feature, target Training and Validation Process The training process for one epoch is implemented as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # train encoder and decoder for one epoch def train_one_epoch( encoder_model, decoder_model, train_loader, data_loss_func, optimizer, code_loss_rate = 0, code_loss_func = None, device = None, ): tot_loss = 0.0 avg_loss = 0.0 tot_nof_batch = 0 encoder_model.train(True) decoder_model.train(True) for i_batch, data in enumerate(train_loader): inputs, targets = data if device: inputs = inputs.to(device) targets = targets.to(device) optimizer.zero_grad() cur_codes = encoder_model(inputs) # encode input data into codes in latent space cur_preds = decoder_model(cur_codes) # reconstruct input image data_loss = data_loss_func(cur_preds, targets) # loss for contraints in the latent space code_loss = 0 if code_loss_func: code_loss = code_loss_func(cur_codes) loss = data_loss + code_loss_rate * code_loss loss.backward() optimizer.step() tot_loss += loss.item() tot_nof_batch += 1 if i_batch % 100 == 0: print(f\"batch {i_batch} loss: {tot_loss/tot_nof_batch: \u003e8f}\") avg_loss = tot_loss/tot_nof_batch print(f\"Train: Avg loss: {avg_loss:\u003e8f}\") return avg_loss Mean Squared Error (MSE) loss is typically used as the loss function during training. For sparse autoencoder training, where a sparsity penalty needs to be incorporated into the loss function, the train for one epoch function accepts inputs for the sparsity penalty and its weight.\nThe validation process for one epoch can be implemented as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # validate encoder and decoder for one epoch def validate_one_epoch( encoder_model, decoder_model, validate_loader, loss_func, device = True, ): tot_loss = 0.0 avg_loss = 0.0 tot_nof_batch = len(validate_loader) tot_samples = len(validate_loader.dataset) encoder_model.eval() decoder_model.eval() with torch.no_grad(): for i_batch, data in enumerate(validate_loader): inputs, targets = data if device: inputs = inputs.to(device) targets = targets.to(device) cur_codes = encoder_model(inputs) # encode input data into codes in latent space cur_preds = decoder_model(cur_codes) # reconstruct input image loss = loss_func(cur_preds, targets) tot_loss += loss.item() avg_loss = tot_loss/tot_nof_batch print(f\"Validate: Avg loss: {avg_loss: \u003e 8f}\") return avg_loss Tying and Untying Layer Weights When training a fully-connected network with symmetrical encoder and decoder structures, it is recommended to initially share the same weight matrix between corresponding layers of the encoder and decoder. Later, for fine-tuning, the weight matrices are separated. This operation is referred to as ’tying the weights’ when they are shared, and ‘untying the weights’ when they are separated.\nIn PyTorch, we can implement the operations to tie and untie the encoder-decoder matrices as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 # create tied linear layer class WeightTiedLinear(nn.Module): def __init__( self, src_linear: nn.Linear, tie_to_linear: nn.Linear ): super().__init__() assert src_linear.weight.size() == tie_to_linear.weight.t().size() self.tie_to_linear = tie_to_linear self.bias = nn.Parameter(src_linear.bias.clone()) # use tie_to_linear layer weigth for foward propagation def forward(self, input): return F.linear(input, self.tie_to_linear.weight.t(), self.bias) # return weight of tied linear layer @property def weight(self): return self.tie_to_linear.weight.t() # tie weights for symmetrical fully-connected auto encoder network. def tie_weight_sym_fc_autoencoder( encoder_model: nn.Module, decoder_model: nn.Module, skip_no_grad_layer = False, ): # get all the fully connected layers encoder_fc_layers = [{\"indexing_str\": cur_layerstr, \"module\": cur_module} for cur_layerstr, cur_module in encoder_model.named_modules() if isinstance(cur_module, nn.Linear)] decoder_fc_layers = [{\"indexing_str\": cur_layerstr, \"module\": cur_module} for cur_layerstr, cur_module in decoder_model.named_modules() if isinstance(cur_module, nn.Linear)] # validate if the autoencoder model are symmetric assert len(encoder_fc_layers) == len(decoder_fc_layers) # tie weights for corresponding layers nof_fc_layers = len(encoder_fc_layers) for i_layer in range(nof_fc_layers): cur_encoder_layer = encoder_fc_layers[i_layer] cur_decoder_layer = decoder_fc_layers[nof_fc_layers - 1 - i_layer] # skip freezed (no grad) layers if needed if skip_no_grad_layer: if not cur_decoder_layer[\"module\"].weight.requires_grad: continue if not cur_decoder_layer[\"module\"].weight.requires_grad: continue # create tied linear module cur_tied_decoder_layermodule = WeightTiedLinear(cur_decoder_layer[\"module\"], cur_encoder_layer[\"module\"]) # update the corresponding layers cur_decoder_indexing_substrs = cur_decoder_layer[\"indexing_str\"].split('.') cur_nof_substrs = len(cur_decoder_indexing_substrs) cur_substr_slow_idx = 0 cur_substr_fast_idx = 0 # iterative access corresponding layers cur_model = decoder_model while(cur_substr_fast_idx \u003c cur_nof_substrs): if cur_decoder_indexing_substrs[cur_substr_fast_idx].isdigit(): if cur_substr_fast_idx == cur_nof_substrs - 1: cur_model.get_submodule(\".\".join(cur_decoder_indexing_substrs[cur_substr_slow_idx:cur_substr_fast_idx]))[int(cur_decoder_indexing_substrs[cur_substr_fast_idx])] = cur_tied_decoder_layermodule else: cur_model = cur_model.get_submodule(\".\".join(cur_decoder_indexing_substrs[cur_substr_slow_idx:cur_substr_fast_idx]))[int(cur_decoder_indexing_substrs[cur_substr_fast_idx])] cur_substr_slow_idx = cur_substr_fast_idx + 1 cur_substr_fast_idx += 1 return encoder_model, decoder_model # untie weights for fully-connected network def untie_weight_fc_models( model: nn.Module, ): # get all fully connected layers # fc_layers = [{\"indexing_str\": cur_layerstr, \"module\": cur_module} for cur_layerstr, cur_module in model.named_modules() if isinstance(cur_module, WeightTiedLinear)] fc_layers = [{\"indexing_str\": cur_layerstr, \"module\": cur_module} for cur_layerstr, cur_module in model.named_modules() if type(cur_module).__name__ == \"WeightTiedLinear\"] # untie weights nof_fc_layers = len(fc_layers) for i_layer in range(nof_fc_layers): cur_layer = fc_layers[i_layer] # create linear module for each tied linear layer cur_untied_module = nn.Linear( in_features = cur_layer[\"module\"].weight.size(1), out_features = cur_layer[\"module\"].weight.size(0), bias = cur_layer[\"module\"].bias is None, device = cur_layer[\"module\"].weight.device, dtype = cur_layer[\"module\"].weight.dtype, ) # update linear module weight and bias from tied linear module cur_untied_module.weight = nn.Parameter(cur_layer[\"module\"].weight.clone()) cur_untied_module.bias = nn.Parameter(cur_layer[\"module\"].bias.clone()) # update the corresponding layers cur_indexing_substrs = cur_layer[\"indexing_str\"].split('.') cur_nof_substrs = len(cur_indexing_substrs) cur_substr_slow_idx = 0 cur_substr_fast_idx = 0 # iterative access corresponding layers cur_model = model while(cur_substr_fast_idx \u003c cur_nof_substrs): if cur_indexing_substrs[cur_substr_fast_idx].isdigit(): if cur_substr_fast_idx == cur_nof_substrs - 1: cur_model.get_submodule(\".\".join(cur_indexing_substrs[cur_substr_slow_idx:cur_substr_fast_idx]))[int(cur_indexing_substrs[cur_substr_fast_idx])] = cur_untied_module else: cur_model = cur_model.get_submodule(\".\".join(cur_indexing_substrs[cur_substr_slow_idx:cur_substr_fast_idx]))[int(cur_indexing_substrs[cur_substr_fast_idx])] cur_substr_slow_idx = cur_substr_fast_idx + 1 cur_substr_fast_idx += 1 return model When tying a decoder layer to an encoder layer, we create a dummy linear layer that uses the weight of the corresponding encoder layer for forward and backward propagation. When untying the decoder layer, we create a new linear layer and update its weight and bias based on the dummy linear layer.\nUsing these tying and untying functions, we can tie and untie corresponding linear layers in the encoder and decoder as follows:\n1 2 3 4 5 6 # tie weights of encoder and decoder tie_weight_sym_fc_autoencoder(encoder, decoder) # untie weights untie_weight_fc_models(encoder) untie_weight_fc_models(decoder) Training Deep Autoencoder For deeper autoencoder networks, unsupervised training can be done in a greedy, layer-wise manner. We start by training the first layer of the encoder and the last layer of the decoder using the input and ground truth images. Once these layers are trained, we freeze them (disable their weight updates) and add the second layer of the encoder and the second-to-last layer of the decoder. We then train these new layers. This process is repeated until all the layers in the encoder and decoder have been trained. Finally, we fine-tune the entire network by training with weight updates enabled for all layers.\nThe layer state update, freezing, and unfreezing operations can be implemented using the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # get references of all the layer reference of a specific class def get_layer_refs( model: nn.Module, layer_class, ): # get all the fully connected layers # layers = [{\"indexing_str\": cur_layerstr, \"module\": cur_module} for cur_layerstr, cur_module in model.named_modules() if isinstance(cur_module, layer_class)] layers = [{\"indexing_str\": cur_layerstr, \"module\": cur_module} for cur_layerstr, cur_module in model.named_modules() if type(cur_module).__name__ == layer_class.__name__] return layers # update states of dst layers from src layers def update_corresponding_layers( src_layer_refs, dst_layer_refs, ): nof_src_layers = len(src_layer_refs) nof_dst_layers = len(dst_layer_refs) nof_itr_layers = min(nof_src_layers, nof_dst_layers) for i_layer in range(nof_itr_layers): cur_src_module = src_layer_refs[i_layer][\"module\"] cur_dst_module = dst_layer_refs[i_layer][\"module\"] cur_dst_module.load_state_dict(cur_src_module.state_dict()) return nof_src_layers, nof_dst_layers # freeze (disable grad calculation) all the layers in the input layer reference list def freeze_layers( layer_refs, ): for cur_layer in layer_refs: for param in cur_layer[\"module\"].parameters(): param.requires_grad = False return layer_refs # unfreeze (enable grad calculation) all the layers in the input layer reference list def unfreeze_layers( layer_refs, ): for cur_layer in layer_refs: for param in cur_layer[\"module\"].parameters(): param.requires_grad = True return layer_refs Using these functions, we can update a deep autoencoder network from a shallower pre trained autoencoder network and manage the freezing and unfreezing of layers as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # get layers src_encoder_fc_layer_refs = get_layer_refs(pretrain_encoder, torch.nn.Linear) dst_encoder_fc_layer_refs = get_layer_refs(encoder, torch.nn.Linear) src_decoder_fc_layer_refs = get_layer_refs(pretrain_decoder, torch.nn.Linear) dst_decoder_fc_layer_refs = get_layer_refs(decoder, torch.nn.Linear) src_decoder_fc_layer_refs = list(reversed(src_decoder_fc_layer_refs)) dst_decoder_fc_layer_refs = list(reversed(dst_decoder_fc_layer_refs)) ## update and freeze layers update_corresponding_layers(src_encoder_fc_layer_refs, dst_encoder_fc_layer_refs) freeze_layers(dst_encoder_fc_layer_refs[:len(src_encoder_fc_layer_refs)]) update_corresponding_layers(src_decoder_fc_layer_refs, dst_decoder_fc_layer_refs) freeze_layers(dst_decoder_fc_layer_refs[:len(src_decoder_fc_layer_refs)]) ## unfreeze layers unfreeze_layers(dst_encoder_fc_layer_refs) unfreeze_layers(dst_decoder_fc_layer_refs) The complete script for training the deep autoencoder can be found in the TrainDeepSimpleFCAutoencoder notebook in my GitHub repository.\nTips for Autoencoder Training Choosing the right activation function is crucial. When using the ReLU function without careful optimization, it can lead to the ‘dead ReLU’ problem, causing inactive neurons in the autoencoder models.\nAvoiding a high learning rate during training, even with a scheduler (especially for autoencoders with lifetime sparsity constraints), is important. A large learning rate can cause gradient updates to overshoot in the initial epochs, potentially leading to undesired local minima during optimization.\nFor training deep autoencoder networks, especially those with sparse constraints, it’s beneficial to adopt a layer-by-layer iterative training approach. Training the network in stacked layers all at once can result in too few meaningful features in the latent space.\nApplications Compression and Dimension Reduction The dimension reduction application of the autoencoder network is straightforward. We use the encoder network to convert high-dimensional input data into low-dimensional representations. The decoder network then reconstructs the encoded information.\nAfter dimension reduction using the encoder, we can analyze the distribution of data in the latent space.\nThe two-dimensional codes found by a 784-128-64-32-2 fully-connected autoencoder. (image credit: Jian Zhong)\nDenoise Pixel-level noise and defects cannot efficiently be represented in the much lower-dimensional latent space, so autoencoders can also be applied for noise reduction and correcting pixel defects. To train an autoencoder network for denoising, we use images with added noise as input and clean images as ground truth.\nFor denoising with autoencoders, we apply Gaussian noise and masking noise as data transformations in PyTorch.\nThe Gaussian noise transformation can be implemented as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Add gaussian noise to image pixel values class AddGaussianNoise(object): \"\"\" Add gaussian noise to image pixel values \"\"\" def __init__( self, mean = 0.0, variance = 1.0, generator = None, ): self.mean = mean self.variance = variance self.generator = generator # random number generator def __call__(self, src_image): src_image_shape = src_image.size() # generate random gaussian noise gauss_noise = torch.randn( size = src_image_shape, generator = self.generator, ) gauss_noise = self.mean + (self.variance ** 0.5) * gauss_noise # add guassian noise to image return src_image + gauss_noise def __repr__(self): return self.__class__.__name__ + f\"(mean = {self.mean}, variance = {self.variance}, generator = {self.generator})\" Here’s an example of denoising Gaussian noise using an autoencoder:\nGaussian denoise result of a simple fully-connected autoencoder (encoder: 784-64, decoder 64-784). (image credit: Jian Zhong)\nMasking noise involves randomly setting a fraction of pixels in the input image to zero.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # Rondomly choose pixels and set them to a constant value class RandomSetConstPxls(object): \"\"\" Rondomly choose pixels and set them to a constant value \"\"\" def __init__( self, rand_rate = 0.5, const_val = 0, ): self.rand_rate = rand_rate self.const_val = const_val def __call__(self, src_image): src_image_size = src_image.size() tot_nof_pxls = src_image.nelement() # calculate number of randomly choosed pixel nof_mod_pxls = tot_nof_pxls * self.rand_rate nof_mod_pxls = int(nof_mod_pxls) # generate mask for chosen pixels mod_pxl_mask = torch.full((tot_nof_pxls,), False) mod_pxl_mask[:nof_mod_pxls] = True mod_pxl_mask = mod_pxl_mask[torch.randperm(tot_nof_pxls)] # clone image and set the chosen pixels to corresponding contant value dst_image = src_image.clone() dst_image = dst_image.view(-1) dst_image[mod_pxl_mask] = self.const_val dst_image = dst_image.view(src_image_size) return dst_image def __repr__(self): return self.__class__.__name__ + f\"(rand_rate = {self.rand_rate}, const_val = {self.const_val})\" Here’s an example of using a simple fully-connected autoencoder to denoise masked noise:\nMask denoise result of a simple fully-connected autoencoder (encoder: 784-64, decoder 64-784). (image credit: Jian Zhong)\nRefer to the TrainSimpleDenoiseFCAutoencoder Jupyter notebook in my GitHub repository for more details.\nFeature extraction and semi-supervised learning When training an autoencoder to transform input data into a low-dimensional space, the encoder and decoder learn to map input data to a latent space and reconstruct it back. The encoder and decoder inherently capture essential features from the data through these transformations.\nThis feature extraction capability of autoencoders makes them highly effective for semi-supervised learning scenarios. In semi-supervised learning for classification networks, for instance, we can first train an autoencoder using the abundant unlabeled data. Subsequently, we connect a shallow fully-connected network after the encoder of the autoencoder. We then use the limited labeled data to fine-tune this shallow network.\nReference [1] Hinton, G. E. \u0026 Salakhutdinov, R. R. Reducing the Dimensionality of Data with Neural Networks. Science 313, 504–507 (2006).\n[2] Kramer, M. A. Nonlinear principal component analysis using autoassociative neural networks. AIChE Journal 37, 233–243 (1991).\n[3] Masci, J., Meier, U., Cireşan, D. \u0026 Schmidhuber, J. Stacked Convolutional Auto-Encoders for hierarchical feature extraction. in Lecture notes in computer science 52–59 (2011). doi:10.1007/978-3-642-21735-7_7.\n[4] Makhzani, A. \u0026 Frey, B. J. A Winner-Take-All method for training sparse convolutional autoencoders. arXiv (Cornell University) (2014).\n[5] A. Ng, “Sparse autoencoder,” CS294A Lecture notes, vol. 72, 2011.\nCitation If you found this article helpful, please cite it as:\nZhong, Jian (June 2024). Autoencoders with PyTorch: Full Code Guide. Vision Tech Insights. https://jianzhongdev.github.io/VisionTechInsights/posts/autoencoders_with_pytorch_full_code_guide/.\nOr\n@article{zhong2024buildtrainAutoencoderPyTorch, title = \"Autoencoders with PyTorch: Full Code Guide\", author = \"Zhong, Jian\", journal = \"jianzhongdev.github.io\", year = \"2024\", month = \"June\", url = \"https://jianzhongdev.github.io/VisionTechInsights/posts/autoencoders_with_pytorch_full_code_guide/\" } ","wordCount":"5916","inLanguage":"en","image":"http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png","datePublished":"2024-06-23T00:00:00Z","dateModified":"2024-06-23T00:00:00Z","author":{"@type":"Person","name":"Jian Zhong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/"},"publisher":{"@type":"Organization","name":"Vision Tech Insights","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Vision Tech Insights (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Vision Tech Insights</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Autoencoders with PyTorch: Full Code Guide</h1><div class=post-description>A comprehensive guide on building and training autoencoders with PyTorch.</div><div class=post-meta><span title='2024-06-23 00:00:00 +0000 UTC'>June 23, 2024</span>&nbsp;·&nbsp;28 min&nbsp;·&nbsp;5916 words&nbsp;·&nbsp;Jian Zhong</div></header><figure class=entry-cover><img loading=eager srcset="http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_360x0_resize_box_3.png 360w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_480x0_resize_box_3.png 480w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_720x0_resize_box_3.png 720w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png alt="[cover image] Architecture of Autoencoder (image credit: Jian Zhong)" width=3510 height=1600><p>[cover image] Architecture of Autoencoder (image credit: Jian Zhong)</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#autoencoder-network>Autoencoder Network</a><ul><li><a href=#redundancy-of-data-representation>Redundancy of Data Representation</a></li><li><a href=#typical-structure-of-an-autoencoder-network>Typical Structure of an Autoencoder Network</a></li></ul></li><li><a href=#fully-connected-autoencoder>Fully-Connected Autoencoder</a></li><li><a href=#sparsity-and-sparse-autoencoder>Sparsity and Sparse Autoencoder</a><ul><li><a href=#hard-sparsity-in-latent-representation>Hard Sparsity in Latent Representation</a></li><li><a href=#soft-sparsity-in-latent-representation>Soft Sparsity in Latent Representation</a></li><li><a href=#lifetime-winner-takes-all-sparsity>Lifetime (Winner-Takes-All) Sparsity</a></li></ul></li><li><a href=#convolutional-autoencoder>Convolutional Autoencoder</a></li><li><a href=#training-and-validation>Training and Validation</a><ul><li><a href=#training-dataset>Training Dataset</a></li><li><a href=#training-and-validation-process>Training and Validation Process</a></li><li><a href=#tying-and-untying-layer-weights>Tying and Untying Layer Weights</a></li><li><a href=#training-deep-autoencoder>Training Deep Autoencoder</a></li><li><a href=#tips-for-autoencoder-training>Tips for Autoencoder Training</a></li></ul></li><li><a href=#applications>Applications</a><ul><li><a href=#compression-and-dimension-reduction>Compression and Dimension Reduction</a></li><li><a href=#denoise>Denoise</a></li><li><a href=#feature-extraction-and-semi-supervised-learning>Feature extraction and semi-supervised learning</a></li></ul></li><li><a href=#reference>Reference</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>An autoencoder is a type of artificial neural network that learns to create efficient codings, or representations, of unlabeled data, making it useful for unsupervised learning. Autoencoders can be used for tasks like reducing the number of dimensions in data, extracting important features, and removing noise. They&rsquo;re also important for building semi-supervised learning models and generative models. The concept of autoencoders has inspired many advanced models.</p><p>In this blog post, we&rsquo;ll start with a simple introduction to autoencoders. Then, we&rsquo;ll show how to build an autoencoder using a fully-connected neural network. We&rsquo;ll explain what sparsity constraints are and how to add them to neural networks. After that, we&rsquo;ll go over how to build autoencoders with convolutional neural networks. Finally, we&rsquo;ll talk about some common uses for autoencoders.</p><p>You can find all the source code and tutorial scripts mentioned in this blog post in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a> (URL: <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main</a> ).</p><h2 id=autoencoder-network>Autoencoder Network<a hidden class=anchor aria-hidden=true href=#autoencoder-network>#</a></h2><h3 id=redundancy-of-data-representation>Redundancy of Data Representation<a hidden class=anchor aria-hidden=true href=#redundancy-of-data-representation>#</a></h3><p>The key idea behind autoencoders is to reduce redundancy in data representation. Often, data is represented in a way that isn&rsquo;t very efficient, leading to higher dimensions than necessary. This means many parts of the data are redundant. For example, the MNIST dataset contains 28x28 pixel images of handwritten digits from 0 to 9. Ideally, we only need one variable to represent these digits, but the image representation uses 784 (28x28) grayscale values.</p><p>Autoencoders work by compressing the features as the neural network processes the data and then reconstructing the original data from this compressed form. This process helps the network learn a more efficient way to represent the input data.</p><h3 id=typical-structure-of-an-autoencoder-network>Typical Structure of an Autoencoder Network<a hidden class=anchor aria-hidden=true href=#typical-structure-of-an-autoencoder-network>#</a></h3><p>An autoencoder network typically has two parts: an encoder and a decoder. The encoder compresses the input data into a smaller, lower-dimensional form. The decoder then takes this smaller form and reconstructs the original input data. This smaller form, created by the encoder, is often called the latent space or the &ldquo;bottleneck.&rdquo; The latent space usually has fewer dimensions than the original input data.</p><figure class=align-center><img loading=lazy src=./Images/AutoencoderCoverImage.png#center><figcaption><p>Architecture of autoencoder. (image credit: Jian Zhong)</p></figcaption></figure><h2 id=fully-connected-autoencoder>Fully-Connected Autoencoder<a hidden class=anchor aria-hidden=true href=#fully-connected-autoencoder>#</a></h2><p>Implementing an autoencoder using a fully connected network is straightforward. For the encoder, we use a fully connected network where the number of neurons decreases with each layer. For the decoder, we do the opposite, using a fully connected network where the number of neurons increases with each layer. This creates a &ldquo;bottleneck&rdquo; structure in the middle of the network.</p><p>Here is a code example demonstrating how to implement the encoder and decoder of a simple autoencoder network using fully-connected neural networks.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>.Layers</span> <span class=kn>import</span> <span class=n>StackedLayers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## fully connected network with only fully connected layers </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleFCNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>StackedLayers</span><span class=o>.</span><span class=n>VGGStackedLinear</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## create models using the above Module </span>
</span></span><span class=line><span class=cl><span class=n>nof_features</span> <span class=o>=</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl><span class=n>code_dim</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## create encoder model</span>
</span></span><span class=line><span class=cl><span class=n>encoder_layer_descriptors</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;in_features&#34;</span><span class=p>:</span> <span class=n>nof_features</span><span class=p>,</span> <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=n>code_dim</span><span class=p>,</span> <span class=s2>&#34;activation&#34;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>encoder</span> <span class=o>=</span> <span class=n>SimpleFCNetwork</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=n>encoder_layer_descriptors</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Encoder:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>encoder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## create decoder model</span>
</span></span><span class=line><span class=cl><span class=n>decoder_layer_descriptors</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;in_features&#34;</span><span class=p>:</span> <span class=n>code_dim</span><span class=p>,</span> <span class=s2>&#34;out_features&#34;</span><span class=p>:</span> <span class=n>nof_features</span><span class=p>,</span> <span class=s2>&#34;activation&#34;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>decoder</span> <span class=o>=</span> <span class=n>SimpleFCNetwork</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=n>decoder_layer_descriptors</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Decoder:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decoder</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The VGGStackedLinear module creates several fully-connected networks based on the input layer descriptors. For a detailed explanation, please refer to my blog post on <a href=/posts/implement_train_vgg_pytorch/>building and training VGG network with PyTorch</a>.</p><p>Here&rsquo;s how the architecture of the encoder and decoder defined above looks:</p><p><details><summary>click to expand simple fully-connected autoencoder printout</summary><div><pre tabindex=0><code>Encoder:
SimpleFCNetwork(
  (network): VGGStackedLinear(
    (network): Sequential(
      (0): Linear(in_features=784, out_features=64, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
  )
)


Decoder:
SimpleFCNetwork(
  (network): VGGStackedLinear(
    (network): Sequential(
      (0): Linear(in_features=64, out_features=784, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
  )
)
</code></pre></div></details></p><p>After training the fully-connected network, here are the results for an example data input/output, the latent representation of data in a batch of 512 samples, and the learned feature dictionary:</p><figure class=align-center><img loading=lazy src=./Images/SimpleFCAutoEncoderResult.png#center><figcaption><p>Training results of a simple fully-connected autoencoder (encoder: 784-64, decoder 64-784). <strong>a,</strong> example data input/output. <strong>b,</strong> latent representation of data in a batch of 512 samples. <strong>c,</strong> the learned (decoder) feature dictionary. (image credit: Jian Zhong)</p></figcaption></figure><p>Without additional constraints, each sample typically contains numerous non-zero latent features of similar amplitudes, and the learned feature dictionary tends to be highly localized.</p><p>For a comprehensive understanding of how the above network was implemented and trained, please refer to the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainSimpleFCAutoencoder.ipynb>TrainSimpleFCAutoencoder Jupyter notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a>.</p><h2 id=sparsity-and-sparse-autoencoder>Sparsity and Sparse Autoencoder<a hidden class=anchor aria-hidden=true href=#sparsity-and-sparse-autoencoder>#</a></h2><p>In machine learning, sparsity suggests that in many high-dimensional datasets, only a small number of features or variables are meaningful or non-zero for each observation. In an optimal representation space, many features either have zero values or values that are negligible.</p><p>In the context of autoencoders, a sparse latent representation of the data is often preferred. This sparse representation can be achieved by incorporating sparse constraints into the network. Adding these constraints helps the autoencoder focus on learning more meaningful features.</p><h3 id=hard-sparsity-in-latent-representation>Hard Sparsity in Latent Representation<a hidden class=anchor aria-hidden=true href=#hard-sparsity-in-latent-representation>#</a></h3><p>Implementing hard sparsity in the latent space involves adding a sparsity layer at the end of the encoder network along the feature dimension.
To create a hard sparsity layer, we specify a number k of features to retain in the latent space. During the forward pass, this layer keeps only the top k largest features of the encoded representation for each sample, setting the rest to 0. During backward propagation, the hard sparsity layer only propagates gradients for these top k features.</p><p>Here&rsquo;s how the hard sparsity layer is implemented:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># hard sparsity function to select the largest k features for each sample in the batch input data</span>
</span></span><span class=line><span class=cl><span class=c1># NOTE: this function works on 1d feature space</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeatureTopKFunction</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>Function</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=o>==</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>src_data_detach</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create mask indicating the top k features for each sample within the feature space</span>
</span></span><span class=line><span class=cl>        <span class=n>topk_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dtype</span> <span class=o>=</span> <span class=nb>bool</span><span class=p>,</span> <span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span><span class=p>,</span> <span class=n>indices</span> <span class=o>=</span> <span class=n>src_data_detach</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_batch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>topk_mask</span><span class=p>[</span><span class=n>i_batch</span><span class=p>,</span> <span class=n>indices</span><span class=p>[</span><span class=n>i_batch</span><span class=p>,:]]</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># save mask for backward propagation</span>
</span></span><span class=line><span class=cl>        <span class=n>ctx</span><span class=o>.</span><span class=n>save_for_backward</span><span class=p>(</span><span class=n>topk_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># only propagate largest k features of each sample </span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>grad_output</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>topk_mask</span> <span class=o>=</span> <span class=n>ctx</span><span class=o>.</span><span class=n>saved_tensors</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># only propagate gradient for largest k features of each sample</span>
</span></span><span class=line><span class=cl>        <span class=n>grad_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>grad_output</span><span class=p>,</span> <span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>grad_input</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span> <span class=o>=</span> <span class=n>grad_output</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>grad_input</span><span class=p>,</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># hard sparsity layer </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TopKSparsity</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>topk</span> <span class=o>=</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>topk</span> <span class=o>=</span> <span class=n>topk</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__repr__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span> <span class=o>+</span> <span class=sa>f</span><span class=s2>&#34;(topk = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>topk</span><span class=si>}</span><span class=s2>)&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>FeatureTopKFunction</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>topk</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div><p>First, we created our own operation <code>FeatureTopKFunction</code> for hard sparsity and defined its functions for both forward and backward passes. During the forward pass, a mask is generated to identify the top k features of each input sample, which is then stored for later use in the backward pass. This mask ensures that only the top k values are kept, while the rest are set to zero for both value and gradient calculations. In the hard sparsity layer, we specify the number k and incorporate the hard sparsity operation into the <code>forward()</code> method.</p><p>To implement hard sparsity in an autoencoder, simply add a hard sparsity layer at the end of the encoder network as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># fully connected network with sparsity layer</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleSparseFCNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>        <span class=n>feature_sparsity_topk</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># add stacked fully connected layers</span>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>StackedLayers</span><span class=o>.</span><span class=n>VGGStackedLinear</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># add top k sparsity along the feature dimension</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>feature_sparsity_topk</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>SparseLayers</span><span class=o>.</span><span class=n>TopKSparsity</span><span class=p>(</span><span class=n>feature_sparsity_topk</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>network_layers</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>network_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div><p>After training the fully-connected network with these hard sparsity constraints, here are the outcomes for a sample data input/output, the latent representations of data in a batch of 512 samples, and the learned feature dictionary:</p><figure class=align-center><img loading=lazy src=./Images/SimpleFCHardSparsityAutoencoderResult.png#center><figcaption><p>Training results of a simple fully-connected autoencoder with hard sparsity (encoder: 784-64-sparsity, decoder 64-784). <strong>a-c,</strong> results of autoencoder trained with top 16 sparsity. <strong>d-f,</strong> results of autoencoder trained with top 5 sparsity. <strong>a,d,</strong> example data input/output. <strong>b,e,</strong> latent representation of data in a batch of 512 samples. <strong>c,f,</strong> the learned (decoder) feature dictionary. (image credit: Jian Zhong)</p></figcaption></figure><p>From the results above, we observe that increasing the required sparsity with hard constraints reduces the number of non-zero features in the latent space. This encourages the network to learn more global features.</p><p>For a detailed understanding of how this network was implemented and trained, please refer to the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainSimpleSparseFCAutoencoder.ipynb>TrainSimpleSparseFCAutoencoder Jupyter notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a>.</p><h3 id=soft-sparsity-in-latent-representation>Soft Sparsity in Latent Representation<a hidden class=anchor aria-hidden=true href=#soft-sparsity-in-latent-representation>#</a></h3><p>We can also encourage sparsity in the encoded features of the latent space by applying a soft constraint. This involves adding an additional penalty term to the loss function. The modified loss function with the sparsity penalty appears as follows:</p><p>$$
H_{\theta}(pred,gt) = J_{\theta}(pred,gt) + \lambda \cdot L_{\theta}(code)
$$</p><p>Here, \(\theta, pred, gt\) represents the parameters of the autoencoder network, the output prediction of autoencoder, and the ground truth data, respectively. \(H_{\theta}(pred,gt)\) ​ is the loss function with sparsity constraints, where \(J_{\theta}(pred,gt)\) is the original loss function, which measures the difference between the network prediction and ground truth. \(L_{\theta}(pred,gt)\) ​ denotes the penalty term for enforcing sparsity. The parameter \(\lambda\) controls the strength of this penalty.</p><p>The L1 loss of the encoded features is commonly used as a sparsity loss. This loss function is readily available in PyTorch.</p><p>Another approach to implementing sparsity loss is through a penalty based on <a href=https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf>KL divergence</a>. The penalty term for this KL divergence-based sparsity can be defined as follows:</p><p>$$
L_{\theta} = \frac{1}{s} \sum^{s}_{j=1} KL(\rho||\hat{\rho_j})
$$</p><p>Here, ​ \(s\) represents the number of features in the encoded representation, which corresponds to the dimension of the latent space. ​ \(j\) is index for the features in the latent space. \(KL(\rho||\hat{\rho_j})\) is calculated as follows:</p><p>$$
KL(\rho||\hat{\rho_j}) = \rho \cdot log(\frac{\rho}{\hat{\rho}_j}) + (1 - \rho) \cdot log(\frac{1-\rho}{1-\hat{\rho}_j})
$$</p><p>Here, \(\rho\) is a sparsity parameter, typically a small value close to zero that is provided during training. \(\hat{\rho}_j\) ​ is computed from the j-th latent features of the samples within the mini-batch as follows:</p><p>$$
\hat{\rho_{j}} = \frac{1}{m} \sum^{m}_{i=1} l_i
$$</p><p>Here, \(m\) denotes the batch size. \(j\) indexes the features within the latent space. \(i\) indexes the samples within the minibatch. \(l\) represents each individual feature within the latent space.</p><p>Note that for the KL divergence expression, the values of \(\rho\) and \(\hat{\rho}_j\) ​ must fall within the range \((0,1)\) . This range should be ensured by using suitable activation functions (such as sigmoid) for the output layer of the encoder, or by appropriately normalizing the latent space features before computing the sparsity loss.</p><p>Below is the PyTorch code implementation for the KL-divergence based sparsity loss:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># Kullback-Leibler divergence formula</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>kullback_leibler_divergence</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>rho</span><span class=p>,</span> <span class=n>rho_hat</span>    
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>rho</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>rho</span><span class=o>/</span><span class=n>rho_hat</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>rho</span><span class=p>)</span><span class=o>*</span><span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>((</span><span class=mi>1</span> <span class=o>-</span> <span class=n>rho</span><span class=p>)</span><span class=o>/</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>rho_hat</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># nn.Module of sparsity loss function </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>KullbackLeiblerDivergenceLoss</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>rho</span> <span class=o>=</span> <span class=mf>0.05</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>rho</span> <span class=o>&gt;</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>rho</span> <span class=o>&lt;</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rho</span> <span class=o>=</span> <span class=n>rho</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>rho_hat</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>kl</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>kullback_leibler_divergence</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>rho</span><span class=p>,</span> <span class=n>rho_hat</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>kl</span>
</span></span></code></pre></td></tr></table></div></div><p>After training a basic fully-connected autoencoder model with soft sparsity constraints, the results are as follows:</p><figure class=align-center><img loading=lazy src=./Images/SimpleFCSoftSparsityAutoencoderResult.png#center><figcaption><p>Training results of a simple fully-connected autoencoder with soft sparsity (encoder: 784-64, decoder 64-784, KL-divergence soft sparsity loss \(\rho = 0.05\) ). <strong>a-c,</strong> results of autoencoder trained with \(\lambda = 10^{-2}\) . <strong>d-f,</strong> results of autoencoder trained with \(\lambda = 10^{-1}\) . <strong>a,d,</strong> example data input/output. <strong>b,e,</strong> latent representation of data in a batch of 512 samples. <strong>c,f,</strong> the learned (decoder) feature dictionary. (image credit: Jian Zhong)</p></figcaption></figure><p>Increasing the strength of the sparsity penalty decreases the number of non-zero features in the latent space.</p><p>For a comprehensive understanding of how this network was implemented and trained, please refer to the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainSimpleFCAutoencoderWithSparseLoss.ipynb>TrainSimpleFCAutoencoderWithSparseLoss Jupyter notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a>.</p><h3 id=lifetime-winner-takes-all-sparsity>Lifetime (Winner-Takes-All) Sparsity<a hidden class=anchor aria-hidden=true href=#lifetime-winner-takes-all-sparsity>#</a></h3><p>Unlike conventional sparsity constraints that aim to increase sparsity within each individual sample, lifetime sparsity enforces sparsity across minibatch samples for each feature. Here&rsquo;s how lifetime sparsity can be implemented:</p><p>During training, in the forward propagation phase, for each feature in the latent space, we retain the top k largest values across all minibatch samples and set the remaining values of that feature to zero. During backward propagation, gradients are propagated only for these k non-zero values.</p><p>During testing, we disable the lifetime sparsity constraints, allowing the encoder network to output the final representation of the input.
The implementation of lifetime sparsity operations is as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># lifetime sparsity functon to select the largest k samples for each feature</span>
</span></span><span class=line><span class=cl><span class=c1># NOTE: this function works on 1d feature space </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LifetimeTopKFunction</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>Function</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=o>==</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>src_data_detach</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create mask indicating the top k samples for each feature along the batch dimension</span>
</span></span><span class=line><span class=cl>        <span class=n>topk_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dtype</span> <span class=o>=</span> <span class=nb>bool</span><span class=p>,</span> <span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span><span class=p>,</span> <span class=n>indices</span> <span class=o>=</span> <span class=n>src_data_detach</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=n>dim</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_feature</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>topk_mask</span><span class=p>[</span><span class=n>indices</span><span class=p>[:,</span><span class=n>i_feature</span><span class=p>],</span><span class=n>i_feature</span><span class=p>]</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># save mask indicationg the top k samples for each feature for back propagation</span>
</span></span><span class=line><span class=cl>        <span class=n>ctx</span><span class=o>.</span><span class=n>save_for_backward</span><span class=p>(</span><span class=n>topk_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># only propagate largest k samples for each feature</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>grad_output</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>topk_mask</span> <span class=o>=</span> <span class=n>ctx</span><span class=o>.</span><span class=n>saved_tensors</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># only propagate gradient for largest k samples for each feature</span>
</span></span><span class=line><span class=cl>        <span class=n>grad_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>grad_output</span><span class=p>,</span> <span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>grad_input</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span> <span class=o>=</span> <span class=n>grad_output</span><span class=p>[</span><span class=n>topk_mask</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>grad_input</span><span class=p>,</span> <span class=kc>None</span>
</span></span></code></pre></td></tr></table></div></div><p>In the forward pass, we create a mask that identifies the top k values across the minibatch dimension for each feature in the latent space. This mask is saved for use during the backward pass. During both forward and backward passes, this mask ensures that only the top k values of each feature are retained, while the rest are set to zero.</p><p>With these lifetime sparsity operations, we can implement a neural network layer that enforces lifetime sparsity as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># lifetime sparsity layer</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LifetimeTopkSparsity</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>topk</span> <span class=o>=</span> <span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>topk</span> <span class=o>=</span> <span class=n>topk</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__repr__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span> <span class=o>+</span> <span class=sa>f</span><span class=s2>&#34;(topk = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>topk</span><span class=si>}</span><span class=s2>)&#34;</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># only apply lifetime sparsity during training</span>
</span></span><span class=line><span class=cl>            <span class=n>y</span> <span class=o>=</span> <span class=n>LifetimeTopKFunction</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>topk</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>y</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div><p>In the lifetime sparsity layer, we store the k values within the network object. During training, this layer implements lifetime sparsity operations. During testing, the layer simply passes the input directly to the output.</p><p>To implement lifetime sparsity in an autoencoder, we add the lifetime sparsity layer at the end of the encoder network as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># fully connected network with sparsity layer</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleSparseFCNetwork</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>layer_descriptors</span> <span class=o>=</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>        <span class=n>lifetime_sparsity_topk</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># add stacked fully connected layers</span>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>StackedLayers</span><span class=o>.</span><span class=n>VGGStackedLinear</span><span class=p>(</span><span class=n>layer_descriptors</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># add top k sparsity along the sample(batch) dimension</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>lifetime_sparsity_topk</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>SparseLayers</span><span class=o>.</span><span class=n>LifetimeTopkSparsity</span><span class=p>(</span><span class=n>lifetime_sparsity_topk</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>network_layers</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>network_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></td></tr></table></div></div><p>After training a simple fully-connected autoencoder model with a lifetime sparsity layer, the results are as follows:</p><figure class=align-center><img loading=lazy src=./Images/SimpleFCLifeTimeSparsityAutoencoderResult.png#center><figcaption><p>Training results of a simple fully-connected autoencoder with life time sparsity (encoder: 784-64-sparsity, decoder 64-784). <strong>a-c,</strong> results of autoencoder trained with top 25% sparsity. <strong>d-f,</strong> results of autoencoder trained with top 5% sparsity. <strong>a,d,</strong> example data input/output. <strong>b,e,</strong> latent representation of data in a batch of 512 samples. <strong>c,f,</strong> the learned (decoder) feature dictionary. (image credit: Jian Zhong)</p></figcaption></figure><p>Increasing the strength of the lifetime sparsity constraint reduces the number of non-zero features in the latent space. This encourages the network to learn more global features.</p><p>For detailed insights into how this network was implemented and trained, please refer to the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainSimpleSparseFCAutoencoder.ipynb>TrainSimpleSparseFCAutoencoder Jupyter notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a>.</p><h2 id=convolutional-autoencoder>Convolutional Autoencoder<a hidden class=anchor aria-hidden=true href=#convolutional-autoencoder>#</a></h2><p>For image data, the encoder network can also be implemented using a convolutional network, where the feature dimensions decrease as the encoder becomes deeper. Max pooling layers can be added to further reduce feature dimensions and induce sparsity in the encoded features.
Here&rsquo;s an example of a convolutional encoder network:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># simple convolutional encoder = stacked convolutional network + maxpooling </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleCovEncoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>convlayer_descriptors</span> <span class=o>=</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>            <span class=n>maxpoollayer_descriptor</span> <span class=o>=</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>convlayer_descriptors</span><span class=p>,</span> <span class=nb>list</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span><span class=p>(</span><span class=nb>isinstance</span><span class=p>(</span><span class=n>maxpoollayer_descriptor</span><span class=p>,</span> <span class=nb>dict</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># append stacked convolution layer</span>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>StackedLayers</span><span class=o>.</span><span class=n>VGGStacked2DConv</span><span class=p>(</span><span class=n>convlayer_descriptors</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># append maxpooling layer</span>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>kernel_size</span> <span class=o>=</span> <span class=n>maxpoollayer_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;kernel_size&#34;</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>stride</span> <span class=o>=</span> <span class=n>maxpoollayer_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;stride&#34;</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>padding</span> <span class=o>=</span> <span class=n>maxpoollayer_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;padding&#34;</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>dilation</span> <span class=o>=</span> <span class=n>maxpoollayer_descriptor</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;dilation&#34;</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># flatten output feature space</span>
</span></span><span class=line><span class=cl>        <span class=n>network_layers</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Flatten</span><span class=p>(</span><span class=n>start_dim</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span> <span class=n>end_dim</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>network_layers</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>network</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>network_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>network</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## create encoder model</span>
</span></span><span class=line><span class=cl><span class=n>encoder_convlayer_descriptors</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;nof_layers&#34;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;in_channels&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;out_channels&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;kernel_size&#34;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;stride&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;padding&#34;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;activation&#34;</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>encoder_maxpoollayer_descriptor</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;kernel_size&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;stride&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>encoder</span> <span class=o>=</span> <span class=n>ConvAutoencoder</span><span class=o>.</span><span class=n>SimpleCovEncoder</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_convlayer_descriptors</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_maxpoollayer_descriptor</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Encoder:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>encoder</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The <code>VGGStacked2DConv</code> module generates multiple convolutional networks based on the input layer descriptors. For a detailed explanation, please refer to my blog post on <a href=/posts/implement_train_vgg_pytorch/>building and training VGG network with PyTorch</a>.</p><p>Here&rsquo;s a visualization of the architecture of the encoder and decoder described above:</p><p><details><summary>click to expand convolutional encoder printout</summary><div><pre tabindex=0><code>Encoder:
SimpleCovEncoder(
  (network): Sequential(
    (0): VGGStacked2DConv(
      (network): Sequential(
        (0): Conv2d(1, 8, kernel_size=(6, 6), stride=(1, 1))
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2d(8, 8, kernel_size=(6, 6), stride=(1, 1))
        (3): LeakyReLU(negative_slope=0.01)
        (4): Conv2d(8, 8, kernel_size=(6, 6), stride=(1, 1))
        (5): LeakyReLU(negative_slope=0.01)
        (6): Conv2d(8, 8, kernel_size=(6, 6), stride=(1, 1))
        (7): LeakyReLU(negative_slope=0.01)
      )
    )
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): Flatten(start_dim=1, end_dim=-1)
  )
)
</code></pre></div></details></p><p>After training the fully-connected network, here are the results for a sample data input/output, the latent representation of data in a batch of 512 samples, and the learned feature dictionary:</p><figure class=align-center><img loading=lazy src=./Images/SimpleConvAutoEncoderResult.png#center><figcaption><p>Training results of a simple autoencoder with convolutional encoder and fully-connected decoder (encoder: Conv6x6-Conv6x6-Conv6x6-Conv6x6-MaxPool2x2, decoder 128-784). <strong>a,</strong> example data input/output. <strong>b,</strong> latent representation of data in a batch of 512 samples. <strong>c,</strong> the learned (decoder) feature dictionary. (image credit: Jian Zhong)</p></figcaption></figure><p>For a detailed understanding of how this network was implemented and trained, please see the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainSimpleConvAutoencoder.ipynb>TrainSimpleConvAutoencoder Jupyter notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a>.</p><h2 id=training-and-validation>Training and Validation<a hidden class=anchor aria-hidden=true href=#training-and-validation>#</a></h2><p>During training, the optimal encoding of input data is generally unknown. In an autoencoder network, the encoder and decoder are trained concurrently. The encoder processes input data to generate compressed representations, while during testing, the decoder reconstructs the input from these representations. The objective of training is to minimize the discrepancy between the decoder&rsquo;s output and the original input data. Typically, Mean Squared Error (MSE) loss is selected as the optimization loss function for this purpose.</p><h3 id=training-dataset>Training Dataset<a hidden class=anchor aria-hidden=true href=#training-dataset>#</a></h3><p>When training an autoencoder with image datasets, both the input data and the ground truth are images. Depending on the application of the autoencoder, the input data and ground truth images may not necessarily be identical.</p><p>In this blog post, we will use the MNIST dataset for our demonstration. In PyTorch, the MNIST dataset provides handwritten digit images as input data and the corresponding digits as ground truth. To train the autoencoder with MNIST and potentially apply various transformations to both input and ground truth images, we implement the following dataset class. This class converts conventional supervised learning datasets into datasets suitable for autoencoder training.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># convert supervised data to autoencoder data</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>supdata_to_autoencoderdata</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>supdata</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>feature_transform</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>target_transform</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>src_feature</span> <span class=o>=</span> <span class=n>supdata</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=c1>#extract feature</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># NOTE: the usuer of this function is responsible for necessary data duplication</span>
</span></span><span class=line><span class=cl>    <span class=n>feature</span> <span class=o>=</span> <span class=n>src_feature</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>feature_transform</span><span class=p>:</span> 
</span></span><span class=line><span class=cl>        <span class=n>feature</span> <span class=o>=</span> <span class=n>feature_transform</span><span class=p>(</span><span class=n>feature</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=n>src_feature</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>target_transform</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>target</span> <span class=o>=</span> <span class=n>target_transform</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>feature</span><span class=p>,</span> <span class=n>target</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># dataset class of autoencoder using existing supervised learning dataset</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AutoencoderDataset</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>src_supdataset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_transform</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>target_transform</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dataset</span> <span class=o>=</span> <span class=n>src_supdataset</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feature_transform</span> <span class=o>=</span> <span class=n>feature_transform</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>target_transform</span> <span class=o>=</span> <span class=n>target_transform</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__len__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__getitem__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dataset</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>feature</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>supdata_to_autoencoderdata</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>src_data</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>feature_transform</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>target_transform</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>feature</span><span class=p>,</span> <span class=n>target</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=training-and-validation-process>Training and Validation Process<a hidden class=anchor aria-hidden=true href=#training-and-validation-process>#</a></h3><p>The training process for one epoch is implemented as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># train encoder and decoder for one epoch</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_one_epoch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_model</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>decoder_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>train_loader</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>data_loss_func</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>code_loss_rate</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>code_loss_func</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_nof_batch</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>encoder_model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>decoder_model</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i_batch</span><span class=p>,</span> <span class=n>data</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>train_loader</span><span class=p>):</span>        
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span> <span class=o>=</span> <span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>device</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>targets</span> <span class=o>=</span> <span class=n>targets</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>cur_codes</span> <span class=o>=</span> <span class=n>encoder_model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span> <span class=c1># encode input data into codes in latent space</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_preds</span> <span class=o>=</span> <span class=n>decoder_model</span><span class=p>(</span><span class=n>cur_codes</span><span class=p>)</span> <span class=c1># reconstruct input image</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>data_loss</span> <span class=o>=</span> <span class=n>data_loss_func</span><span class=p>(</span><span class=n>cur_preds</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># loss for contraints in the latent space</span>
</span></span><span class=line><span class=cl>        <span class=n>code_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>code_loss_func</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>code_loss</span> <span class=o>=</span> <span class=n>code_loss_func</span><span class=p>(</span><span class=n>cur_codes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>data_loss</span> <span class=o>+</span> <span class=n>code_loss_rate</span> <span class=o>*</span> <span class=n>code_loss</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>tot_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>tot_nof_batch</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i_batch</span> <span class=o>%</span> <span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;batch </span><span class=si>{</span><span class=n>i_batch</span><span class=si>}</span><span class=s2> loss: </span><span class=si>{</span><span class=n>tot_loss</span><span class=o>/</span><span class=n>tot_nof_batch</span><span class=si>:</span><span class=s2> &gt;8f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>tot_loss</span><span class=o>/</span><span class=n>tot_nof_batch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Train: Avg loss: </span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2>&gt;8f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>avg_loss</span>
</span></span></code></pre></td></tr></table></div></div><p>Mean Squared Error (MSE) loss is typically used as the loss function during training. For sparse autoencoder training, where a sparsity penalty needs to be incorporated into the loss function, the train for one epoch function accepts inputs for the sparsity penalty and its weight.</p><p>The validation process for one epoch can be implemented as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># validate encoder and decoder for one epoch</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>validate_one_epoch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>decoder_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>validate_loader</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>loss_func</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>avg_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_nof_batch</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>validate_loader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tot_samples</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>validate_loader</span><span class=o>.</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>encoder_model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>decoder_model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_batch</span><span class=p>,</span> <span class=n>data</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>validate_loader</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>inputs</span><span class=p>,</span> <span class=n>targets</span> <span class=o>=</span> <span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>device</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>targets</span> <span class=o>=</span> <span class=n>targets</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>cur_codes</span> <span class=o>=</span> <span class=n>encoder_model</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span> <span class=c1># encode input data into codes in latent space</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_preds</span> <span class=o>=</span> <span class=n>decoder_model</span><span class=p>(</span><span class=n>cur_codes</span><span class=p>)</span> <span class=c1># reconstruct input image</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_func</span><span class=p>(</span><span class=n>cur_preds</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>tot_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>avg_loss</span> <span class=o>=</span> <span class=n>tot_loss</span><span class=o>/</span><span class=n>tot_nof_batch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Validate: Avg loss: </span><span class=si>{</span><span class=n>avg_loss</span><span class=si>:</span><span class=s2> &gt; 8f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>avg_loss</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=tying-and-untying-layer-weights>Tying and Untying Layer Weights<a hidden class=anchor aria-hidden=true href=#tying-and-untying-layer-weights>#</a></h3><p>When training a fully-connected network with symmetrical encoder and decoder structures, it is recommended to initially share the same weight matrix between corresponding layers of the encoder and decoder. Later, for fine-tuning, the weight matrices are separated. This operation is referred to as &rsquo;tying the weights&rsquo; when they are shared, and &lsquo;untying the weights&rsquo; when they are separated.</p><p>In PyTorch, we can implement the operations to tie and untie the encoder-decoder matrices as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># create tied linear layer</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>WeightTiedLinear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>            <span class=n>src_linear</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tie_to_linear</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span>
</span></span><span class=line><span class=cl>        <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>src_linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>tie_to_linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>t</span><span class=p>()</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tie_to_linear</span> <span class=o>=</span> <span class=n>tie_to_linear</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>src_linear</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>clone</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># use tie_to_linear layer weigth for foward propagation</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=nb>input</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>tie_to_linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>t</span><span class=p>(),</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># return weight of tied linear layer</span>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span> 
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>weight</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>tie_to_linear</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>t</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># tie weights for symmetrical fully-connected auto encoder network.</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>tie_weight_sym_fc_autoencoder</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder_model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>skip_no_grad_layer</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># get all the fully connected layers</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_fc_layers</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;indexing_str&#34;</span><span class=p>:</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=s2>&#34;module&#34;</span><span class=p>:</span> <span class=n>cur_module</span><span class=p>}</span> <span class=k>for</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=n>cur_module</span> <span class=ow>in</span> <span class=n>encoder_model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>()</span> <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>cur_module</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>decoder_fc_layers</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;indexing_str&#34;</span><span class=p>:</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=s2>&#34;module&#34;</span><span class=p>:</span> <span class=n>cur_module</span><span class=p>}</span> <span class=k>for</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=n>cur_module</span> <span class=ow>in</span> <span class=n>decoder_model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>()</span> <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>cur_module</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># validate if the autoencoder model are symmetric</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=nb>len</span><span class=p>(</span><span class=n>encoder_fc_layers</span><span class=p>)</span> <span class=o>==</span> <span class=nb>len</span><span class=p>(</span><span class=n>decoder_fc_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># tie weights for corresponding layers</span>
</span></span><span class=line><span class=cl>    <span class=n>nof_fc_layers</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>encoder_fc_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i_layer</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_fc_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_encoder_layer</span> <span class=o>=</span> <span class=n>encoder_fc_layers</span><span class=p>[</span><span class=n>i_layer</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_decoder_layer</span> <span class=o>=</span> <span class=n>decoder_fc_layers</span><span class=p>[</span><span class=n>nof_fc_layers</span> <span class=o>-</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>i_layer</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># skip freezed (no grad) layers if needed</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>skip_no_grad_layer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=n>cur_decoder_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=n>cur_decoder_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>continue</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create tied linear module</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_tied_decoder_layermodule</span> <span class=o>=</span> <span class=n>WeightTiedLinear</span><span class=p>(</span><span class=n>cur_decoder_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>],</span> <span class=n>cur_encoder_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># update the corresponding layers</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_decoder_indexing_substrs</span> <span class=o>=</span> <span class=n>cur_decoder_layer</span><span class=p>[</span><span class=s2>&#34;indexing_str&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;.&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_nof_substrs</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>cur_decoder_indexing_substrs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>cur_substr_slow_idx</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_substr_fast_idx</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># iterative access corresponding layers</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_model</span> <span class=o>=</span> <span class=n>decoder_model</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span><span class=p>(</span><span class=n>cur_substr_fast_idx</span> <span class=o>&lt;</span> <span class=n>cur_nof_substrs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>cur_decoder_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_fast_idx</span><span class=p>]</span><span class=o>.</span><span class=n>isdigit</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>cur_substr_fast_idx</span> <span class=o>==</span> <span class=n>cur_nof_substrs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>cur_model</span><span class=o>.</span><span class=n>get_submodule</span><span class=p>(</span><span class=s2>&#34;.&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>cur_decoder_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_slow_idx</span><span class=p>:</span><span class=n>cur_substr_fast_idx</span><span class=p>]))[</span><span class=nb>int</span><span class=p>(</span><span class=n>cur_decoder_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_fast_idx</span><span class=p>])]</span> <span class=o>=</span> <span class=n>cur_tied_decoder_layermodule</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>cur_model</span> <span class=o>=</span> <span class=n>cur_model</span><span class=o>.</span><span class=n>get_submodule</span><span class=p>(</span><span class=s2>&#34;.&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>cur_decoder_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_slow_idx</span><span class=p>:</span><span class=n>cur_substr_fast_idx</span><span class=p>]))[</span><span class=nb>int</span><span class=p>(</span><span class=n>cur_decoder_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_fast_idx</span><span class=p>])]</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_substr_slow_idx</span> <span class=o>=</span> <span class=n>cur_substr_fast_idx</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_substr_fast_idx</span> <span class=o>+=</span> <span class=mi>1</span>                             
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>encoder_model</span><span class=p>,</span> <span class=n>decoder_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># untie weights for fully-connected network</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>untie_weight_fc_models</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># get all fully connected layers</span>
</span></span><span class=line><span class=cl>    <span class=c1># fc_layers = [{&#34;indexing_str&#34;: cur_layerstr, &#34;module&#34;: cur_module} for cur_layerstr, cur_module in model.named_modules() if isinstance(cur_module, WeightTiedLinear)]</span>
</span></span><span class=line><span class=cl>    <span class=n>fc_layers</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;indexing_str&#34;</span><span class=p>:</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=s2>&#34;module&#34;</span><span class=p>:</span> <span class=n>cur_module</span><span class=p>}</span> <span class=k>for</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=n>cur_module</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>()</span> <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>cur_module</span><span class=p>)</span><span class=o>.</span><span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;WeightTiedLinear&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># untie weights</span>
</span></span><span class=line><span class=cl>    <span class=n>nof_fc_layers</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>fc_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i_layer</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_fc_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_layer</span> <span class=o>=</span> <span class=n>fc_layers</span><span class=p>[</span><span class=n>i_layer</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># create linear module for each tied linear layer</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_untied_module</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>in_features</span> <span class=o>=</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>out_features</span> <span class=o>=</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>bias</span> <span class=o>=</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device</span> <span class=o>=</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span> <span class=o>=</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># update linear module weight and bias from tied linear module </span>
</span></span><span class=line><span class=cl>        <span class=n>cur_untied_module</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>clone</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_untied_module</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>clone</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># update the corresponding layers</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_indexing_substrs</span> <span class=o>=</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;indexing_str&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;.&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_nof_substrs</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>cur_indexing_substrs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>cur_substr_slow_idx</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_substr_fast_idx</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># iterative access corresponding layers</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_model</span> <span class=o>=</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>        <span class=k>while</span><span class=p>(</span><span class=n>cur_substr_fast_idx</span> <span class=o>&lt;</span> <span class=n>cur_nof_substrs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>cur_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_fast_idx</span><span class=p>]</span><span class=o>.</span><span class=n>isdigit</span><span class=p>():</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>cur_substr_fast_idx</span> <span class=o>==</span> <span class=n>cur_nof_substrs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>cur_model</span><span class=o>.</span><span class=n>get_submodule</span><span class=p>(</span><span class=s2>&#34;.&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>cur_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_slow_idx</span><span class=p>:</span><span class=n>cur_substr_fast_idx</span><span class=p>]))[</span><span class=nb>int</span><span class=p>(</span><span class=n>cur_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_fast_idx</span><span class=p>])]</span> <span class=o>=</span> <span class=n>cur_untied_module</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>cur_model</span> <span class=o>=</span> <span class=n>cur_model</span><span class=o>.</span><span class=n>get_submodule</span><span class=p>(</span><span class=s2>&#34;.&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>cur_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_slow_idx</span><span class=p>:</span><span class=n>cur_substr_fast_idx</span><span class=p>]))[</span><span class=nb>int</span><span class=p>(</span><span class=n>cur_indexing_substrs</span><span class=p>[</span><span class=n>cur_substr_fast_idx</span><span class=p>])]</span>
</span></span><span class=line><span class=cl>                <span class=n>cur_substr_slow_idx</span> <span class=o>=</span> <span class=n>cur_substr_fast_idx</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=n>cur_substr_fast_idx</span> <span class=o>+=</span> <span class=mi>1</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><p>When tying a decoder layer to an encoder layer, we create a dummy linear layer that uses the weight of the corresponding encoder layer for forward and backward propagation. When untying the decoder layer, we create a new linear layer and update its weight and bias based on the dummy linear layer.</p><p>Using these tying and untying functions, we can tie and untie corresponding linear layers in the encoder and decoder as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># tie weights of encoder and decoder</span>
</span></span><span class=line><span class=cl><span class=n>tie_weight_sym_fc_autoencoder</span><span class=p>(</span><span class=n>encoder</span><span class=p>,</span> <span class=n>decoder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># untie weights</span>
</span></span><span class=line><span class=cl><span class=n>untie_weight_fc_models</span><span class=p>(</span><span class=n>encoder</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>untie_weight_fc_models</span><span class=p>(</span><span class=n>decoder</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=training-deep-autoencoder>Training Deep Autoencoder<a hidden class=anchor aria-hidden=true href=#training-deep-autoencoder>#</a></h3><p>For deeper autoencoder networks, unsupervised training can be done in a greedy, layer-wise manner. We start by training the first layer of the encoder and the last layer of the decoder using the input and ground truth images. Once these layers are trained, we freeze them (disable their weight updates) and add the second layer of the encoder and the second-to-last layer of the decoder. We then train these new layers. This process is repeated until all the layers in the encoder and decoder have been trained. Finally, we fine-tune the entire network by training with weight updates enabled for all layers.</p><p>The layer state update, freezing, and unfreezing operations can be implemented using the following:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># get references of all the layer reference of a specific class</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_layer_refs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>layer_class</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># get all the fully connected layers</span>
</span></span><span class=line><span class=cl>    <span class=c1># layers = [{&#34;indexing_str&#34;: cur_layerstr, &#34;module&#34;: cur_module} for cur_layerstr, cur_module in model.named_modules() if isinstance(cur_module, layer_class)]</span>
</span></span><span class=line><span class=cl>    <span class=n>layers</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;indexing_str&#34;</span><span class=p>:</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=s2>&#34;module&#34;</span><span class=p>:</span> <span class=n>cur_module</span><span class=p>}</span> <span class=k>for</span> <span class=n>cur_layerstr</span><span class=p>,</span> <span class=n>cur_module</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_modules</span><span class=p>()</span> <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>cur_module</span><span class=p>)</span><span class=o>.</span><span class=vm>__name__</span> <span class=o>==</span> <span class=n>layer_class</span><span class=o>.</span><span class=vm>__name__</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>layers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># update states of dst layers from src layers</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_corresponding_layers</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>src_layer_refs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>dst_layer_refs</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>nof_src_layers</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>src_layer_refs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>nof_dst_layers</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>dst_layer_refs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>nof_itr_layers</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>nof_src_layers</span><span class=p>,</span> <span class=n>nof_dst_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i_layer</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>nof_itr_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_src_module</span> <span class=o>=</span> <span class=n>src_layer_refs</span><span class=p>[</span><span class=n>i_layer</span><span class=p>][</span><span class=s2>&#34;module&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_dst_module</span> <span class=o>=</span> <span class=n>dst_layer_refs</span><span class=p>[</span><span class=n>i_layer</span><span class=p>][</span><span class=s2>&#34;module&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>cur_dst_module</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span><span class=n>cur_src_module</span><span class=o>.</span><span class=n>state_dict</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nof_src_layers</span><span class=p>,</span> <span class=n>nof_dst_layers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># freeze (disable grad calculation) all the layers in the input layer reference list</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>freeze_layers</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>layer_refs</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>cur_layer</span> <span class=ow>in</span> <span class=n>layer_refs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>layer_refs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># unfreeze (enable grad calculation) all the layers in the input layer reference list</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>unfreeze_layers</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>layer_refs</span><span class=p>,</span>    
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>cur_layer</span> <span class=ow>in</span> <span class=n>layer_refs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>cur_layer</span><span class=p>[</span><span class=s2>&#34;module&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>layer_refs</span>
</span></span></code></pre></td></tr></table></div></div><p>Using these functions, we can update a deep autoencoder network from a shallower pre trained autoencoder network and manage the freezing and unfreezing of layers as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># get layers</span>
</span></span><span class=line><span class=cl><span class=n>src_encoder_fc_layer_refs</span> <span class=o>=</span> <span class=n>get_layer_refs</span><span class=p>(</span><span class=n>pretrain_encoder</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dst_encoder_fc_layer_refs</span> <span class=o>=</span> <span class=n>get_layer_refs</span><span class=p>(</span><span class=n>encoder</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>src_decoder_fc_layer_refs</span> <span class=o>=</span> <span class=n>get_layer_refs</span><span class=p>(</span><span class=n>pretrain_decoder</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dst_decoder_fc_layer_refs</span> <span class=o>=</span> <span class=n>get_layer_refs</span><span class=p>(</span><span class=n>decoder</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>src_decoder_fc_layer_refs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>reversed</span><span class=p>(</span><span class=n>src_decoder_fc_layer_refs</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>dst_decoder_fc_layer_refs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>reversed</span><span class=p>(</span><span class=n>dst_decoder_fc_layer_refs</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## update and freeze layers</span>
</span></span><span class=line><span class=cl><span class=n>update_corresponding_layers</span><span class=p>(</span><span class=n>src_encoder_fc_layer_refs</span><span class=p>,</span> <span class=n>dst_encoder_fc_layer_refs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>freeze_layers</span><span class=p>(</span><span class=n>dst_encoder_fc_layer_refs</span><span class=p>[:</span><span class=nb>len</span><span class=p>(</span><span class=n>src_encoder_fc_layer_refs</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>update_corresponding_layers</span><span class=p>(</span><span class=n>src_decoder_fc_layer_refs</span><span class=p>,</span> <span class=n>dst_decoder_fc_layer_refs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>freeze_layers</span><span class=p>(</span><span class=n>dst_decoder_fc_layer_refs</span><span class=p>[:</span><span class=nb>len</span><span class=p>(</span><span class=n>src_decoder_fc_layer_refs</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>## unfreeze layers</span>
</span></span><span class=line><span class=cl><span class=n>unfreeze_layers</span><span class=p>(</span><span class=n>dst_encoder_fc_layer_refs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>unfreeze_layers</span><span class=p>(</span><span class=n>dst_decoder_fc_layer_refs</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The complete script for training the deep autoencoder can be found in the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainDeepSimpleFCAutoencoder.ipynb>TrainDeepSimpleFCAutoencoder notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a>.</p><h3 id=tips-for-autoencoder-training>Tips for Autoencoder Training<a hidden class=anchor aria-hidden=true href=#tips-for-autoencoder-training>#</a></h3><ul><li><p>Choosing the right activation function is crucial. When using the ReLU function without careful optimization, it can lead to the &lsquo;dead ReLU&rsquo; problem, causing inactive neurons in the autoencoder models.</p></li><li><p>Avoiding a high learning rate during training, even with a scheduler (especially for autoencoders with lifetime sparsity constraints), is important. A large learning rate can cause gradient updates to overshoot in the initial epochs, potentially leading to undesired local minima during optimization.</p></li><li><p>For training deep autoencoder networks, especially those with sparse constraints, it&rsquo;s beneficial to adopt a layer-by-layer iterative training approach. Training the network in stacked layers all at once can result in too few meaningful features in the latent space.</p></li></ul><h2 id=applications>Applications<a hidden class=anchor aria-hidden=true href=#applications>#</a></h2><h3 id=compression-and-dimension-reduction>Compression and Dimension Reduction<a hidden class=anchor aria-hidden=true href=#compression-and-dimension-reduction>#</a></h3><p>The dimension reduction application of the autoencoder network is straightforward. We use the encoder network to convert high-dimensional input data into low-dimensional representations. The decoder network then reconstructs the encoded information.</p><p>After dimension reduction using the encoder, we can analyze the distribution of data in the latent space.</p><figure class=align-center><img loading=lazy src=./Images/DeeperAutoencoder_LatentSpace.png#center><figcaption><p>The two-dimensional codes found by a 784-128-64-32-2 fully-connected autoencoder. (image credit: Jian Zhong)</p></figcaption></figure><h3 id=denoise>Denoise<a hidden class=anchor aria-hidden=true href=#denoise>#</a></h3><p>Pixel-level noise and defects cannot efficiently be represented in the much lower-dimensional latent space, so autoencoders can also be applied for noise reduction and correcting pixel defects. To train an autoencoder network for denoising, we use images with added noise as input and clean images as ground truth.</p><p>For denoising with autoencoders, we apply Gaussian noise and masking noise as data transformations in PyTorch.</p><p>The Gaussian noise transformation can be implemented as follows:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># Add gaussian noise to image pixel values</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AddGaussianNoise</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Add gaussian noise to image pixel values
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>mean</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>variance</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>generator</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>=</span> <span class=n>mean</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>variance</span> <span class=o>=</span> <span class=n>variance</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>generator</span> <span class=o>=</span> <span class=n>generator</span> <span class=c1># random number generator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src_image</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_image_shape</span> <span class=o>=</span> <span class=n>src_image</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># generate random gaussian noise</span>
</span></span><span class=line><span class=cl>        <span class=n>gauss_noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>size</span> <span class=o>=</span> <span class=n>src_image_shape</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>generator</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generator</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>gauss_noise</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mean</span> <span class=o>+</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>variance</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span> <span class=o>*</span> <span class=n>gauss_noise</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># add guassian noise to image </span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>src_image</span> <span class=o>+</span> <span class=n>gauss_noise</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__repr__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span> <span class=o>+</span> <span class=sa>f</span><span class=s2>&#34;(mean = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>mean</span><span class=si>}</span><span class=s2>, variance = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>variance</span><span class=si>}</span><span class=s2>, generator = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>generator</span><span class=si>}</span><span class=s2>)&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>Here&rsquo;s an example of denoising Gaussian noise using an autoencoder:</p><figure class=align-center><img loading=lazy src=./Images/GaussianNoise_50.png#center><figcaption><p>Gaussian denoise result of a simple fully-connected autoencoder (encoder: 784-64, decoder 64-784). (image credit: Jian Zhong)</p></figcaption></figure><p>Masking noise involves randomly setting a fraction of pixels in the input image to zero.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=c1># Rondomly choose pixels and set them to a constant value</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RandomSetConstPxls</span><span class=p>(</span><span class=nb>object</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Rondomly choose pixels and set them to a constant value
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>            <span class=n>rand_rate</span> <span class=o>=</span> <span class=mf>0.5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>const_val</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rand_rate</span> <span class=o>=</span> <span class=n>rand_rate</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>const_val</span> <span class=o>=</span> <span class=n>const_val</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src_image</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_image_size</span> <span class=o>=</span> <span class=n>src_image</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>tot_nof_pxls</span> <span class=o>=</span> <span class=n>src_image</span><span class=o>.</span><span class=n>nelement</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># calculate number of randomly choosed pixel </span>
</span></span><span class=line><span class=cl>        <span class=n>nof_mod_pxls</span> <span class=o>=</span> <span class=n>tot_nof_pxls</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>rand_rate</span>
</span></span><span class=line><span class=cl>        <span class=n>nof_mod_pxls</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>nof_mod_pxls</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># generate mask for chosen pixels</span>
</span></span><span class=line><span class=cl>        <span class=n>mod_pxl_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>full</span><span class=p>((</span><span class=n>tot_nof_pxls</span><span class=p>,),</span> <span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mod_pxl_mask</span><span class=p>[:</span><span class=n>nof_mod_pxls</span><span class=p>]</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=n>mod_pxl_mask</span> <span class=o>=</span> <span class=n>mod_pxl_mask</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>randperm</span><span class=p>(</span><span class=n>tot_nof_pxls</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># clone image and set the chosen pixels to corresponding contant value</span>
</span></span><span class=line><span class=cl>        <span class=n>dst_image</span> <span class=o>=</span> <span class=n>src_image</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>dst_image</span> <span class=o>=</span> <span class=n>dst_image</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>dst_image</span><span class=p>[</span><span class=n>mod_pxl_mask</span><span class=p>]</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>const_val</span>
</span></span><span class=line><span class=cl>        <span class=n>dst_image</span> <span class=o>=</span> <span class=n>dst_image</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>src_image_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>dst_image</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__repr__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=vm>__class__</span><span class=o>.</span><span class=vm>__name__</span> <span class=o>+</span> <span class=sa>f</span><span class=s2>&#34;(rand_rate = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>rand_rate</span><span class=si>}</span><span class=s2>, const_val = </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>const_val</span><span class=si>}</span><span class=s2>)&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>Here&rsquo;s an example of using a simple fully-connected autoencoder to denoise masked noise:</p><figure class=align-center><img loading=lazy src=./Images/MaskNoise_50.png#center><figcaption><p>Mask denoise result of a simple fully-connected autoencoder (encoder: 784-64, decoder 64-784). (image credit: Jian Zhong)</p></figcaption></figure><p>Refer to the <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/blob/main/TrainSimpleDenoiseFCAutoencoder.ipynb>TrainSimpleDenoiseFCAutoencoder Jupyter notebook</a> in my <a href=https://github.com/JianZhongDev/AutoencoderPyTorch/tree/main>GitHub repository</a> for more details.</p><h3 id=feature-extraction-and-semi-supervised-learning>Feature extraction and semi-supervised learning<a hidden class=anchor aria-hidden=true href=#feature-extraction-and-semi-supervised-learning>#</a></h3><p>When training an autoencoder to transform input data into a low-dimensional space, the encoder and decoder learn to map input data to a latent space and reconstruct it back. The encoder and decoder inherently capture essential features from the data through these transformations.</p><p>This feature extraction capability of autoencoders makes them highly effective for semi-supervised learning scenarios. In semi-supervised learning for classification networks, for instance, we can first train an autoencoder using the abundant unlabeled data. Subsequently, we connect a shallow fully-connected network after the encoder of the autoencoder. We then use the limited labeled data to fine-tune this shallow network.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>[1] Hinton, G. E. & Salakhutdinov, R. R. Reducing the Dimensionality of Data with Neural Networks. Science 313, 504–507 (2006).</p><p>[2] Kramer, M. A. Nonlinear principal component analysis using autoassociative neural networks. AIChE Journal 37, 233–243 (1991).</p><p>[3] Masci, J., Meier, U., Cireşan, D. & Schmidhuber, J. Stacked Convolutional Auto-Encoders for hierarchical feature extraction. in Lecture notes in computer science 52–59 (2011). doi:10.1007/978-3-642-21735-7_7.</p><p>[4] Makhzani, A. & Frey, B. J. A Winner-Take-All method for training sparse convolutional autoencoders. arXiv (Cornell University) (2014).</p><p>[5] A. Ng, “Sparse autoencoder,” CS294A Lecture notes, vol. 72, 2011.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you found this article helpful, please cite it as:</p><blockquote><p>Zhong, Jian (June 2024). Autoencoders with PyTorch: Full Code Guide. Vision Tech Insights. <a href=https://jianzhongdev.github.io/VisionTechInsights/posts/autoencoders_with_pytorch_full_code_guide/>https://jianzhongdev.github.io/VisionTechInsights/posts/autoencoders_with_pytorch_full_code_guide/</a>.</p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-html data-lang=html><span class=line><span class=cl>@article{zhong2024buildtrainAutoencoderPyTorch,
</span></span><span class=line><span class=cl>  title   = &#34;Autoencoders with PyTorch: Full Code Guide&#34;,
</span></span><span class=line><span class=cl>  author  = &#34;Zhong, Jian&#34;,
</span></span><span class=line><span class=cl>  journal = &#34;jianzhongdev.github.io&#34;,
</span></span><span class=line><span class=cl>  year    = &#34;2024&#34;,
</span></span><span class=line><span class=cl>  month   = &#34;June&#34;,
</span></span><span class=line><span class=cl>  url     = &#34;https://jianzhongdev.github.io/VisionTechInsights/posts/autoencoders_with_pytorch_full_code_guide/&#34;
</span></span><span class=line><span class=cl>}
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/computer-vision/>Computer Vision</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/gentle_introduction_to_variational_autoencoders/><span class=title>« Prev</span><br><span>A Gentle Introduction to Variational Autoencoders: Concept and PyTorch Implementation Guide</span>
</a><a class=next href=http://localhost:1313/posts/page_container_direct_data_io/><span class=title>Next »</span><br><span>PageContainer: Fast, Direct Data I/O Without OS Buffering</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on x" href="https://x.com/intent/tweet/?text=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f&amp;hashtags=computervision%2cmachinelearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f&amp;title=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide&amp;summary=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f&title=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on whatsapp" href="https://api.whatsapp.com/send?text=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on telegram" href="https://telegram.me/share/url?text=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Autoencoders with PyTorch: Full Code Guide on ycombinator" href="https://news.ycombinator.com/submitlink?t=Autoencoders%20with%20PyTorch%3a%20Full%20Code%20Guide&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fautoencoders_with_pytorch_full_code_guide%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Vision Tech Insights</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>