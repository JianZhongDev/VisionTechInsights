<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Modeling | Vision Tech Insights</title>
<meta name=keywords content><meta name=description content="ExampleSite description"><meta name=author content="Jian Zhong"><link rel=canonical href=http://localhost:1313/categories/modeling/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/categories/modeling/index.xml><link rel=alternate hreflang=en href=http://localhost:1313/categories/modeling/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Modeling"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/categories/modeling/"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary"><meta name=twitter:title content="Modeling"><meta name=twitter:description content="ExampleSite description"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Vision Tech Insights (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Vision Tech Insights</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/categories/>Categories</a></div><h1>Modeling
<a href=/categories/modeling/index.xml title=RSS aria-label=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_360x0_resize_box_3.png 360w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_480x0_resize_box_3.png 480w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_720x0_resize_box_3.png 720w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure.png alt="[cover image] Architecture of Variational Autoencoder (image credit: Jian Zhong)" width=3510 height=1200></figure><header class=entry-header><h2 class=entry-hint-parent>A Gentle Introduction to Variational Autoencoders: Concept and PyTorch Implementation Guide</h2></header><div class=entry-content><p>The variational autoencoder (VAE) is a type of generative model that combines principles from neural networks and probabilistic models to learn the underlying probabilistic distribution of a dataset and generate new data samples similar to the given dataset.
Due to its ability to combine probabilistic modeling and learn complex data distributions, VAEs have become a fundamental tool and have had a profound impact on the fields of machine learning and deep learning....</p></div><footer class=entry-footer><span title='2024-07-08 00:00:00 +0000 UTC'>July 8, 2024</span>&nbsp;·&nbsp;34 min&nbsp;·&nbsp;7037 words&nbsp;·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to A Gentle Introduction to Variational Autoencoders: Concept and PyTorch Implementation Guide" href=http://localhost:1313/posts/gentle_introduction_to_variational_autoencoders/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_360x0_resize_box_3.png 360w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_480x0_resize_box_3.png 480w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_720x0_resize_box_3.png 720w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png alt="[cover image] Architecture of Autoencoder (image credit: Jian Zhong)" width=3510 height=1600></figure><header class=entry-header><h2 class=entry-hint-parent>Autoencoders with PyTorch: Full Code Guide</h2></header><div class=entry-content><p>An autoencoder is a type of artificial neural network that learns to create efficient codings, or representations, of unlabeled data, making it useful for unsupervised learning. Autoencoders can be used for tasks like reducing the number of dimensions in data, extracting important features, and removing noise. They’re also important for building semi-supervised learning models and generative models. The concept of autoencoders has inspired many advanced models.
In this blog post, we’ll start with a simple introduction to autoencoders....</p></div><footer class=entry-footer><span title='2024-06-23 00:00:00 +0000 UTC'>June 23, 2024</span>&nbsp;·&nbsp;28 min&nbsp;·&nbsp;5916 words&nbsp;·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to Autoencoders with PyTorch: Full Code Guide" href=http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_360x0_resize_box_3.png 360w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_480x0_resize_box_3.png 480w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_720x0_resize_box_3.png 720w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png 3200w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png alt="[cover image] Architecture of VGG Model (image credit: Jian Zhong)" width=3200 height=1800></figure><header class=entry-header><h2 class=entry-hint-parent>Building and Training VGG with PyTorch: A Step-by-Step Guide</h2></header><div class=entry-content><p>The VGG (Visual Geometry Group) model is a type of convolutional neural network (CNN) outlined in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition. It’s known for its use of small convolution filters and deep layers, which helped it achieve top-notch performance in tasks like image classification. By stacking multiple layers with small kernel sizes, VGG can capture a wide range of features from input images. Plus, adding more rectification layers makes its decision-making process sharper and more accurate....</p></div><footer class=entry-footer><span title='2024-05-13 00:00:00 +0000 UTC'>May 13, 2024</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4250 words&nbsp;·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to Building and Training VGG with PyTorch: A Step-by-Step Guide" href=http://localhost:1313/posts/implement_train_vgg_pytorch/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles_hu56cec669adbdee3a87b260d628634efd_518952_360x0_resize_box_3.png 360w ,http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles_hu56cec669adbdee3a87b260d628634efd_518952_480x0_resize_box_3.png 480w ,http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles_hu56cec669adbdee3a87b260d628634efd_518952_720x0_resize_box_3.png 720w ,http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles_hu56cec669adbdee3a87b260d628634efd_518952_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles_hu56cec669adbdee3a87b260d628634efd_518952_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/basic_math_for_two_photon_fluorescence_microscopy/2PFM_Principles.png alt="[cover image] principles of two photon fluorescence microscopy (image credit: Jian Zhong)" width=3510 height=2478></figure><header class=entry-header><h2 class=entry-hint-parent>Basic Math for Two Photon Fluorescence Microscopy</h2></header><div class=entry-content><p>It’s really helpful to do some basic math to understand how well an imaging system works when you’re designing or improving it. Most of the time, a very basic model of the system can provide a ton of useful information. Therefore, I would like to share an example of how we can crunch some numbers to understand the signal and noise in a two-photon fluorescence microscope (2PFM). In the end, I will also provide a practical demo of how this math can help us make the system work better....</p></div><footer class=entry-footer><span title='2024-04-16 00:00:00 +0000 UTC'>April 16, 2024</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2407 words&nbsp;·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to Basic Math for Two Photon Fluorescence Microscopy" href=http://localhost:1313/posts/basic_math_for_two_photon_fluorescence_microscopy/></a></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Vision Tech Insights</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>