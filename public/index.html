<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.124.1"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vision Tech Insights</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Jian Zhong"><link rel=canonical href=http://localhost:1313/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/index.xml><link rel=alternate type=application/json href=http://localhost:1313/index.json><link rel=alternate hreflang=en href=http://localhost:1313/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Vision Tech Insights"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vision Tech Insights"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Vision Tech Insights","url":"http://localhost:1313/","description":"ExampleSite description","thumbnailUrl":"http://localhost:1313/%3Clink%20/%20abs%20url%3E","sameAs":["https://github.com/JianZhongDev/","https://www.linkedin.com/in/jian-zhong-ucb2019phy/","mailto:jian.zhong.dev@gmail.com","https://twitter.com/Jian_Zhong_Dev","https://www.instagram.com/jianzhongdev/","https://www.pinterest.com/jianzhongdev/"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Vision Tech Insights (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Vision Tech Insights</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>Vision Tech Insights</h1><h2>A blog for optical and imaging technologies</h2></header><div class=entry-content><p>Welcome to Vision Tech Insights!</p><p>I am Jian Zhong, a UC Berkeley PhD candidate specializing in ultrafast imaging technology. Join me as I share my learning notes on optical and imaging technologies, from hardware to software development. Letâ€™s explore together!</p></div><footer class=entry-footer><div class=social-icons><a href=https://github.com/JianZhongDev/ target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=https://www.linkedin.com/in/jian-zhong-ucb2019phy/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=mailto:jian.zhong.dev@gmail.com target=_blank rel="noopener noreferrer me" title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</a><a href=https://twitter.com/Jian_Zhong_Dev target=_blank rel="noopener noreferrer me" title=X><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentcolor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
</a><a href=https://www.instagram.com/jianzhongdev/ target=_blank rel="noopener noreferrer me" title=Instagram><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg>
</a><a href=https://www.pinterest.com/jianzhongdev/ target=_blank rel="noopener noreferrer me" title=Pinterest><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></a></div></footer></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_360x0_resize_box_3.png 360w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_480x0_resize_box_3.png 480w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_720x0_resize_box_3.png 720w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure_hub1bc92e1de11e59fe373d0f1b4f1b598_278888_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/gentle_introduction_to_variational_autoencoders/VariationalAutoEncoderStructure.png alt="[cover image] Architecture of Variational Autoencoder (image credit: Jian Zhong)" width=3510 height=1200></figure><header class=entry-header><h2 class=entry-hint-parent>A Gentle Introduction to Variational Autoencoders: Concept and PyTorch Implementation Guide</h2></header><div class=entry-content><p>The variational autoencoder (VAE) is a type of generative model that combines principles from neural networks and probabilistic models to learn the underlying probabilistic distribution of a dataset and generate new data samples similar to the given dataset.
Due to its ability to combine probabilistic modeling and learn complex data distributions, VAEs have become a fundamental tool and have had a profound impact on the fields of machine learning and deep learning....</p></div><footer class=entry-footer><span title='2024-07-08 00:00:00 +0000 UTC'>July 8, 2024</span>&nbsp;Â·&nbsp;16 min&nbsp;Â·&nbsp;3282 words&nbsp;Â·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to A Gentle Introduction to Variational Autoencoders: Concept and PyTorch Implementation Guide" href=http://localhost:1313/posts/gentle_introduction_to_variational_autoencoders/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_360x0_resize_box_3.png 360w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_480x0_resize_box_3.png 480w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_720x0_resize_box_3.png 720w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage_hu096f3544ecf9e22ccd8866d640cdbb31_218841_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/autoencoders_with_pytorch_full_code_guide/AutoencoderCoverImage.png alt="[cover image] Architecture of Autoencoder (image credit: Jian Zhong)" width=3510 height=1600></figure><header class=entry-header><h2 class=entry-hint-parent>Autoencoders with PyTorch: Full Code Guide</h2></header><div class=entry-content><p>An autoencoder is a type of artificial neural network that learns to create efficient codings, or representations, of unlabeled data, making it useful for unsupervised learning. Autoencoders can be used for tasks like reducing the number of dimensions in data, extracting important features, and removing noise. Theyâ€™re also important for building semi-supervised learning models and generative models. The concept of autoencoders has inspired many advanced models.
In this blog post, weâ€™ll start with a simple introduction to autoencoders....</p></div><footer class=entry-footer><span title='2024-06-23 00:00:00 +0000 UTC'>June 23, 2024</span>&nbsp;Â·&nbsp;28 min&nbsp;Â·&nbsp;5916 words&nbsp;Â·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to Autoencoders with PyTorch: Full Code Guide" href=http://localhost:1313/posts/autoencoders_with_pytorch_full_code_guide/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure_hu2239e0ea4f6ef1095e731fc236afdff5_376452_360x0_resize_box_3.png 360w ,http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure_hu2239e0ea4f6ef1095e731fc236afdff5_376452_480x0_resize_box_3.png 480w ,http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure_hu2239e0ea4f6ef1095e731fc236afdff5_376452_720x0_resize_box_3.png 720w ,http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure_hu2239e0ea4f6ef1095e731fc236afdff5_376452_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure_hu2239e0ea4f6ef1095e731fc236afdff5_376452_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/page_container_direct_data_io/PageContainerDataStructure.png alt="[cover image] data structure of PageContainer (image credit: Jian Zhong)" width=3510 height=2100></figure><header class=entry-header><h2 class=entry-hint-parent>PageContainer: Fast, Direct Data I/O Without OS Buffering</h2></header><div class=entry-content><p>When creating high-speed data streaming applications, itâ€™s important to avoid unnecessary data transfer to keep things fast and efficient. Operating systems (OS) automatically buffer file input/output (I/O) in the computerâ€™s memory. However, many data streaming applications already have their own buffering steps, making the OSâ€™s additional buffering unnecessary. Disabling this OS buffering allows direct control of data transfer, but it requires the application to access data in sizes that are multiples of the system page size (or disk sector size)....</p></div><footer class=entry-footer><span title='2024-06-13 00:00:00 +0000 UTC'>June 13, 2024</span>&nbsp;Â·&nbsp;14 min&nbsp;Â·&nbsp;2832 words&nbsp;Â·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to PageContainer: Fast, Direct Data I/O Without OS Buffering" href=http://localhost:1313/posts/page_container_direct_data_io/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_360x0_resize_box_3.png 360w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_480x0_resize_box_3.png 480w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_720x0_resize_box_3.png 720w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage_hu7178767123e027aa07b48b3dfea7116b_1234517_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png 3200w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/implement_train_VGG_PyTorch/VGGPyTorch_CoverImage.png alt="[cover image] Architecture of VGG Model (image credit: Jian Zhong)" width=3200 height=1800></figure><header class=entry-header><h2 class=entry-hint-parent>Building and Training VGG with PyTorch: A Step-by-Step Guide</h2></header><div class=entry-content><p>The VGG (Visual Geometry Group) model is a type of convolutional neural network (CNN) outlined in the paper Very Deep Convolutional Networks for Large-Scale Image Recognition. Itâ€™s known for its use of small convolution filters and deep layers, which helped it achieve top-notch performance in tasks like image classification. By stacking multiple layers with small kernel sizes, VGG can capture a wide range of features from input images. Plus, adding more rectification layers makes its decision-making process sharper and more accurate....</p></div><footer class=entry-footer><span title='2024-05-13 00:00:00 +0000 UTC'>May 13, 2024</span>&nbsp;Â·&nbsp;20 min&nbsp;Â·&nbsp;4250 words&nbsp;Â·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to Building and Training VGG with PyTorch: A Step-by-Step Guide" href=http://localhost:1313/posts/implement_train_vgg_pytorch/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy srcset="http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure_hu8d53bdb34eb1f9b827302effc8fef181_670772_360x0_resize_box_3.png 360w ,http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure_hu8d53bdb34eb1f9b827302effc8fef181_670772_480x0_resize_box_3.png 480w ,http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure_hu8d53bdb34eb1f9b827302effc8fef181_670772_720x0_resize_box_3.png 720w ,http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure_hu8d53bdb34eb1f9b827302effc8fef181_670772_1080x0_resize_box_3.png 1080w ,http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure_hu8d53bdb34eb1f9b827302effc8fef181_670772_1500x0_resize_box_3.png 1500w ,http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure.png 3510w" sizes="(min-width: 768px) 720px, 100vw" src=http://localhost:1313/images/building_a_configuration_file_parser_with_cpp/CppConfigFileModuleStructure.png alt="[cover image] diagram of the configuration file parser (image credit: Jian Zhong)" width=3510 height=2478></figure><header class=entry-header><h2 class=entry-hint-parent>Building a Configuration File Parser with C++</h2></header><div class=entry-content><p>Configuration files are commonly used to adjust settings in computer programs. Iâ€™m presently developing a configuration file parser for my high-speed data acquisition system using C++. Along the way, Iâ€™ve discovered some useful techniques involving C++ generics and inheritance that streamline coding. Therefore, I decided to document these tricks in the hope that theyâ€™ll be beneficial to others. You can find the ready-to-use source code for this configuration file parser in my GitHub repository....</p></div><footer class=entry-footer><span title='2024-04-21 00:00:00 +0000 UTC'>April 21, 2024</span>&nbsp;Â·&nbsp;21 min&nbsp;Â·&nbsp;4369 words&nbsp;Â·&nbsp;Jian Zhong</footer><a class=entry-link aria-label="post link to Building a Configuration File Parser with C++" href=http://localhost:1313/posts/building_a_configuration_file_parser_with_cpp/></a></article><footer class=page-footer><nav class=pagination><a class=next href=http://localhost:1313/page/2/>Next&nbsp;2/2&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Vision Tech Insights</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>